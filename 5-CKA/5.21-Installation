if kubernetes is corrupted or ip changed:
Please find K8S cluster settings, where master and worker IP has changed.
Edit /etc/hosts file with new host IP of master and worker nodes

Then reset cluster:
For 1.23 v: sudo kubeadm reset
For 1.26 v: sudo kubeadm reset --cri-socket unix:///var/run/crio/crio.sock
sudo rm -rf /etc/cni /etc/kubernetes /var/lib/dockershim /var/lib/etcd /var/lib/kubelet /var/run/kubernetes ~/.kube/*
sudo yum remove kubeadm kubectl kubelet kubernetes-cni kube*
kubeadm reset -f
rm -rf /etc/cni /etc/kubernetes /var/lib/dockershim /var/lib/etcd /var/lib/kubelet /var/run/kubernetes ~/.kube/*
iptables -F && iptables -X
iptables -t nat -F && iptables -t nat -X
iptables -t raw -F && iptables -t raw -X
iptables -t mangle -F && iptables -t mangle -X
[devops@rhel3 ~]$ sudo yum remove cri-o




Reinitialize the cluster
For version 1.23: sudo kubeadm init
For version 1.26: sudo kubeadm init --cri-socket unix:///var/run/crio/crio.sock
This command will generate key to join worker.
 
on WORKER node:
[devops@rhel2 ~]$ sudo vim /etc/fstab
sudo kubeadm reset --cri-socket unix://var/run/crio/crio.socket

[devops@rhel2 ~]$ sudo rm -rf /etc/kubernetes/kubelet.conf
[devops@rhel2 ~]$ sudo rm -rf /etc/kubernetes/bootstrap-kubelet.conf 
[devops@rhel2 ~]$ sudo rm -rf /etc/kubernetes/pki/ca.crt
then join command
sudo kubeadm join 192.168.100.47:6443 --token 0cqxc0.ucwjjl9yfwul6iw7         --discovery-token-ca-cert-hash sha256:4ed2ebb00bc488418f13d1c8bbdef703b8fbfe460df0bf1486fe590b71be1d3e --cri-socket unix:///var/run/crio/crio.sock 

----------------------------------------------------------------------------

 2 ways to install: kubeadm way
                    hard way - install each component like api, schedular, controller, etcd separately
 
 # Installation - kubeadm
 ~~~bash
 1  sudo hostnamectl set-hostname k8s-control
    2  sudo /etc/hosts
    3  sudo vim /etc/hosts
    4  cat << EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

    5  sudo modprobe overlay
    6  sudo modprobe br_netfilter
    7  cat << EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

    8  sudo sysctl --system
dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
    8  sudo install docker-ce
       sudo systemctl enable --now docker
    9  sudo apt-get update && sudo apt-get install -y containerd
   10  sudo mkdir -p /etc/containerd
   11  sudo containerd config default | sudo tee /etc/containerd/config.toml
   12  sudo systemctl restart containerd
   13  sudo swapoff -a
   14  sudo apt-get update && sudo apt-get install -y apt-transport-https curl
   15  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
   16  cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

   17  sudo apt-get update
   18  sudo apt-get install -y kubelet=1.27.0-00 kubeadm=1.27.0-00 kubectl=1.27.0-00
   19  sudo apt-mark hold kubelet kubeadm kubectl
   20  history

if error then give api path of kubernetes config file:
$kubectl version --short --kubeconfig=/etc/kubernetes/admin.conf

$kubectl get pods --kubeconfig=/etc/kubernetes/admin.conf

or simpler solution:

control:~$ mkdir -p $HOME/.kube
control:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[sudo] password for cloud_user: 
control:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

NOW YOU DONT NEED TO SPECIFY PATH:


kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

cloud_user@k8s-control:~$ kubeadm token create --print-join-command

kubeadm join 172.31.42.208:6443 --token m1nigj.87kt3pgbhk5410zb --discovery-token-ca-cert-hash sha256:b8cd580929d9f9f0a65fe6765dba962466a7d58cb16b4394f46b0c9e904e6161 
~~~

==========================================================================================

## Lesson Reference

> If you are using Cloud Playground, create three servers with the following settings:

- Distribution: Ubuntu 20.04 Focal Fossa LTS
- Size: medium

> If you wish, you can set an appropriate hostname for each node.

> On the control plane node:

sudo hostnamectl set-hostname k8s-control

> On the first worker node:

sudo hostnamectl set-hostname k8s-worker1

> On the second worker node:

sudo hostnamectl set-hostname k8s-worker2

> On all nodes, set up the hosts file to enable all the nodes to reach each other using these hostnames:

sudo vi /etc/hosts

> On all nodes, add the following at the end of the file. You will need to supply the actual private IP address for each node:

<control plane node private IP> k8s-control
<worker node 1 private IP> k8s-worker1
<worker node 2 private IP> k8s-worker2

> Log out of all three servers and log back in to see these changes take effect.
sudo sed -i '/ swap / s/^/#/' /etc/fstab


Step 4: Set up the IPV4 bridge on all nodes
To configure the IPV4 bridge on all nodes, execute the following commands on each node.


        cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
        overlay
        br_netfilter
        EOF

        sudo modprobe overlay
        sudo modprobe br_netfilter

        > sysctl params required by setup; params persist across reboots:

        cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
        net.bridge.bridge-nf-call-iptables  = 1
        net.bridge.bridge-nf-call-ip6tables = 1
        net.ipv4.ip_forward                 = 1
        EOF

        > Apply sysctl params without reboot:

        sudo sysctl --system

> On all nodes, set up Docker Engine and containerd. You will need to load some kernel modules and modify some system settings as part of this
process:
> Set up the Docker Engine repository:

sudo apt-get update && sudo apt-get install -y ca-certificates curl gnupg lsb-release apt-transport-https

> Add Dockerâ€™s official GPG key:

sudo mkdir -m 0755 -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

> Set up the repository:

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

> Update the apt package index:

sudo apt-get update

> Install Docker Engine, containerd, and Docker Compose:

VERSION_STRING=5:23.0.1-1~ubuntu.20.04~focal
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

> Add your 'cloud_user' to the docker group:

sudo usermod -aG docker $USER

> Log out and log back in so that your group membership is re-evaluated.

> Make sure that 'disabled_plugins' is commented out in your config.toml file:

sudo sed -i 's/disabled_plugins/#disabled_plugins/' /etc/containerd/config.toml

> Restart containerd:

sudo systemctl restart containerd

> On all nodes, disable swap:

sudo swapoff -a




snapshot
> On all nodes, install kubeadm, kubelet, and kubectl:

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

sudo apt-get update && sudo apt-get install -y kubelet=1.27.0-00 kubeadm=1.27.0-00 kubectl=1.27.0-00

sudo apt-mark hold kubelet kubeadm kubectl

> On the control plane node only, initialize the cluster and set up kubectl access:

sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.27.0

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

> Verify the cluster is working:

kubectl get nodes



kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
> Install the Calico network add-on:

kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml

Next, download the custom resources file for Calico, which contains definitions of the various resources that Calico will use.

curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml -O
sed -i 's/cidr: 192\.168\.0\.0\/16/cidr: 10.10.0.0\/16/g' custom-resources.yaml
kubectl create -f custom-resources.yaml
> Get the join command (this command is also printed during kubeadm init; feel free to simply copy it from there):

kubeadm token create --print-join-command

> Copy the join command from the control plane node. Run it on each worker node as root (i.e., with sudo ):

sudo kubeadm join ...

> On the control plane node, verify all nodes in your cluster are ready. Note that it may take a few moments for all of the nodes to enter the READY state:
~~~bash
kubectl get nodes



kubeadm join 172.31.16.108:6443 --token 7i6fq0.h46x3ux332we16v8 \
        --discovery-token-ca-cert-hash sha256:4f8ab90e1d1f0bd5edcda02fb37c7bf8a3d8911523eb6cd52d6f443be0a49346


rhel1
kubeadm join 192.168.100.155:6443 --token jrsilt.314ori2dndgag9ul \
	--discovery-token-ca-cert-hash sha256:c784ca4c7c995f90185782ef50c4b07d3e2d7232c6d6698e408c1448c43d4606 



========================================================================================
RHEL
root@rhel2 pki]# history

    7  hostnamectl set-hostname rhel2
    8  yum update -y
    9  sudo yum remove -y docker docker-client docker-client-latest docker-common \
       docker-latest docker-latest-logrotate docker-logrotate docker-engine


   10  echo "devops ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/devops


# Step 1) Disable SWAP
        sudo swapoff -a

# Step 2) Disable SELinux
        sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# Step 3) Configure networking in master and worker node
        hostnamectl set-hostname rhel2
        sudo vim /etc/hosts
        192.168.100.155 rhel1
        192.168.100.154 rhel2
        sudo dnf install -y iproute-tc

# Step 4) Allow firewall rules for k8s
        On Master node, allow following ports,

        $ sudo firewall-cmd --permanent --add-port=6443/tcp
        $ sudo firewall-cmd --permanent --add-port=2379-2380/tcp
        $ sudo firewall-cmd --permanent --add-port=10250/tcp
        $ sudo firewall-cmd --permanent --add-port=10251/tcp
        $ sudo firewall-cmd --permanent --add-port=10252/tcp
        $ sudo firewall-cmd --reload
        On Worker node, allow following ports,

        $ sudo firewall-cmd --permanent --add-port=10250/tcp
        $ sudo firewall-cmd --permanent --add-port=30000-32767/tcp                                                 
        $ sudo firewall-cmd --reload

# Step 5) Install CRI-O container runtime
                                        <!-- Kubernetes requires a container runtime for pods to run. Kubernetes 1.23 and later versions require that you install a container runtime that confirms with the Container Runtime Interface.
                                        A Container Runtime is an application that supports running containers. Kubernetes supports the following Container Runtime:
                                        Containerd
                                        CRI-O
                                        Docker Engine
                                        Mirantis Container Runtime -->
        To achieve this, we need to configure the prerequisites as follows:

        First, create a modules configuration file for Kubernetes.

        $ sudo vi /etc/modules-load.d/k8s.conf
            overlay
            br_netfilter
        Then load both modules using the modprobe command.
        $ sudo modprobe overlay
        $ sudo modprobe br_netfilter

        Next, configure the required sysctl parameters as follows
        $ sudo vi /etc/sysctl.d/k8s.conf
            net.bridge.bridge-nf-call-iptables  = 1
            net.ipv4.ip_forward                 = 1
            net.bridge.bridge-nf-call-ip6tables = 1
        $ sudo sysctl --system
        
        To install CRI-O, set the $VERSION environment variable to match your CRI-O version. For instance, to install CRI-O version 1.26 set the $VERSION as shown:

        $ export VERSION=1.26
        Next, run the following commands:

        $ sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/CentOS_8/devel:kubic:libcontainers:stable.repo
        $ sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/CentOS_8/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo
        Then use the DNF package manager to install CRI-O:

        $ sudo dnf install cri-o
 
        $ systemctl enable --now crio

# Step 6)  Install Kubernetes Packages

        $ sudo vi /etc/yum.repos.d/kubernetes.repo
            [kubernetes] 
            name=Kubernetes
            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
            enabled=1
            gpgcheck=1
            repo_gpgcheck=1
            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
            exclude=kubelet kubeadm kubectl

        $ sudo dnf install -y kubelet-1.26.1 kubeadm-1.26.1 kubectl-1.26.1 --disableexcludes=kubernetes

        $ sudo systemctl enable kubelet
        $ sudo systemctl start kubelet

# Step 7)  Create a Kubernetes cluster (ON CONTROL NODE)
        
        kubeadm init --cri-socket /var/run/crio/crio.sock
        kubeadm init --pod-network-cidr=10.1.0.0/24 --cri-socket /var/run/crio/crio.sock (optional)

        $ kubectl taint nodes --all node-role.kubernetes.io/master-
                Also, be sure to remove taints from the master node: (it will not deploy tasks to control node)


# Step 8) Install Calico Pod Network Add-on
        The next step is to install Calico CNI (Container Network Interface). It is an opensource project used to provide container networking and security. After Installing Calico CNI, nodes state will change to Ready state, DNS service inside the cluster would be functional and containers can start communicating with each other.

        Calico provides scalability, high performance, and interoperability with existing Kubernetes workloads. It can be deployed on-premises and on popular cloud technologies such as Google Cloud, AWS and Azure.

        To install Calico CNI, run the following command from the master node
curl -OL https://docs.projectcalico.org/v3.9/manifests/calico.yaml
        $ kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
        To confirm if the pods have started, run the command::

        $ kubectl get pods -n kube-system
        You should see that each pod is ‘READY’ and has the ‘RUNNING’ status as shown in the third column.
        $ kubectl get nodes -o wide
        $ kubectl get pods --all-namespaces

# Step 9) Adding worker node to the cluster
           kubeadm join 192.168.100.155:6443 --token s9c0ba.60idwc32aa0slxul --discovery-token-ca-cert-hash sha256:e7523724b39a6d24240742562cfe012e7144572be045d4ace084ec54a709fb9b --cri-socket /var/run/crio/crio.sock 

           got error 
                24  kubeadm join 192.168.100.155:6443 --token s9c0ba.60idwc32aa0slxul --discovery-token-ca-cert-hash sha256:e7523724b39a6d24240742562cfe012e7144572be045d4ace084ec54a709fb9b --cri-socket /var/run/crio/crio.sock 
                --ignore-preflight-errors all
                25  cd /etc/kubernetes/pki
                26  ls
                27  rm -rf ca.crt 
~~~

===================================================================
# MINIKUBE
## 1) Update Your System
$ sudo apt update
$ sudo apt upgrade -y
## 2) Install Docker
$ sudo apt install ca-certificates curl gnupg wget apt-transport-https -y
$ sudo install -m 0755 -d /etc/apt/keyrings
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
$ sudo chmod a+r /etc/apt/keyrings/docker.gpg
$ echo 
  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
$ sudo apt update

$ sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

Add your local user to docker group so that your local user run docker commands without sudo.

 sudo usermod -aG docker $USER
 newgrp docker

Note: To make above changes into the affect logout and login.


## 3) Download and Install Minikube Binary
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
minikube version
sudo cp minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod 755 /usr/local/bin/minikube


## 4) Install Kubectl tool
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version



## 5) Start Minikube Cluster
apt-get install conntrack -y
minikube start --driver=docker
minikube status


6) Interact with Your Minikube Cluster
$ kubectl get nodes
$ kubectl cluster-info
$ kubectl create deployment nginx-web --image=nginx
$ kubectl expose deployment nginx-web --type NodePort --port=80
$ kubectl get deployment,pod,svc

7) Managing Minikube Addons
minikube addons list
minikube dashboard
8) Managing Minikube Cluster
To stop and start the minikube cluster, run beneath commands.

$ minikube stop
$ minikube start
In order to delete the minikube cluster, run

$ minikube delete
minikube service websvc --url
