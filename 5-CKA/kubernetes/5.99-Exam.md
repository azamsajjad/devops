



## About the NEW CKA Exam (From September 2020)

> **The new CKA Certification Exam is aligned with CKAD exam pattern in terms of duration, the number of questions and passing score.**

> **Time management is more critical compare with the old version of CKA exam.**

> **Questions are weightage in three categories like 4%, 7% & 8%. Most of the 4% weightage questions are easy to answer except 1 or 2. However, if you stuck with any 4% weighted marks question then flag it & move on without investing too much time.**

> **Troubleshooting raised from 10% to 30% and logging/monitoring also included in it. There is a much greater emphasis on troubleshooting and hence, the question related to troubleshooting carries more weight marks like 7-8%.**

> **1/4 of the new curriculum is dedicated to the cluster Architecture, installation & configuration including Kubeadm to install/ version upgrade on a kubernetes cluster, etcd backup and restore, Manage RBAC. Obviously, cluster related configuration questions will carry the highest weightage %.**

> **Service & Networking increases by 9% in the weight, but there are not many changes except how to use Ingress resources and network policy. Questions coming from this section is weighted to 4% and 7%.**

> **Workload & Scheduling capture all points of the previous Application Lifecycle Management including label selectors, scale the deployment, reschedule pods and so on. Most of the questions coming from this section are easy to answer and carry 4% weightage.**

> **Storage curriculum has no significant changes. In my opinion, this is the only section which is almost similar to previous CKA version.**

> **mostly the security is separated out of the CKA exam and LF is planning to include this in their CKS(Certified Kubernetes Security Specialist) certification exam which is scheduled to release in November 2020.**


<p align="center"> <img src="https://github.com/lerndevops/educka/blob/master/static/CKA-Old-vs-New.PNG"> </p>

## Kubernetes 1.18 & above Versions Only

## `Generate Pod Yamls`

```
kubectl run nginx --image=nginx --dry-run=client -o yaml  ## generate pod yaml file
kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml  ## generate pod yaml file with restartPolicy: Never
kubectl run nginx --image=nginx -l="app=web,env=dev" --dry-run=client -o yaml  ## generate pod yaml file with labels provided
kubectl run nginx --image=nginx --env="hello=world" --env="me=naresh" --dry-run=client -o yaml ## generate pod yaml with env variables
kubectl run nginx --image=nginx --restart=OnFailure --env='hello=world' -l='app=web' --limits='cpu=100m,memory=150Mi' --dry-run=client -o yaml ## generate pod yaml with various parametes
kubectl run nginx --image=nginx --port=80 --expose --dry-run=client -o yaml ## generate pod yaml file & Service yaml file together
```

## `Create Pod` 

```
## Note: remove --dry-run -o yaml from all above commands to create pods ex as below. 
kubectl run nginx --image=nginx  # create a pod 
```

## `Generate Deployment Yamls`

```
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml  ## generate Deployment yaml file
```

## `Create Deployment` 

```
## Note: remove --dry-run -o yaml from all above commands to create Deployment ex as below. 

kubectl create deployment nginx --image=nginx  ## creates a Deployment 

```

## `scale up/down number of replicas`

```
kubectl scale deployment nginx --replicas=3  ## scale a deployment named nginx 
kubectl scale replicaset nginx --replicas=3  ## scale a replicaset named nginx
kubectl scale statefulset nginx --replicas=3  ## scale a statefulset named nginx
```

## `Generate Service yamls`

```
kubectl create service nodeport nginx --tcp=80:80 --dry-run=client -o yaml  ## generate a service yaml with type: NodePort name: 'nginx' & selector app: nginx
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml  ## generate a service yaml with type: NodePort, name: 'nginx' & selector app: nginx with nodeport given
kubectl create service clusterip nginx --tcp=80:80 --dry-run=client -o yaml ## generate a service yaml with type: ClusterIP
```

## `Generate Service yaml for to expose/access running pods/deployment`

```
## Note: to use below commands, the pod/deployment must be created first

kubectl expose pod mypod --port=80 --target-port=8080 --type=NodePort --dry-run=client -o yaml
kubectl expose deployment mydep --port=80 --target-port=8080 --type=NodePort --dry-run=client -o yaml 
kubectl expose -f pod.yaml  --port=80 --target-port=8080 --type=NodePort --dry-run=client -o yaml

kubectl expose pod mypod --port=80 --target-port=8080 --type=ClusterIP --dry-run=client -o yaml
kubectl expose deployment mydep --port=80 --target-port=8080 --type=ClusterIP --dry-run=client -o yaml 
kubectl expose -f pod.yaml  --port=80 --target-port=8080 --type=ClusterIP --dry-run=client -o yaml
```




==============================================================================
Create a new pod called admin-pod with image busybox.
Allow the pod to be able to set system_time.
The container should sleep for 3200 seconds.
        devops@T480:~/exam$ cat 1-pod.yml
        apiVersion: v1
        kind: Pod
        metadata:
        labels:
            run: admin-pod
        name: admin-pod
        spec:
        containers:
        - command:
            - sleep
            - "3200"
            image: busybox
            name: admin-pod
            securityContext:
              capabilities:
                add: ["SYS_TIME"]
===============================================================================
A kubeconfig file called test.kubeconfig has
been created in /root/TEST. There is something
wrong with the configuration.
Troubleshoot and fix it.
compare /root/TEST with 
devops@T480:~/exam$ cat 2-config.yml
devops@T480:~/exam$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://146.190.32.90:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: DATA+OMITTED
    client-key-data: DATA+OMITTED
===============================================================================
> Create a new deployment called web-proj-268, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.
        devops@T480:~/exam$ kubectl create deployment web-proj-268 --image=nginx:
        1.16 --replicas=1
        deployment.apps/web-proj-268 created
        devops@T480:~/exam$ kubectl get deploy
        NAME           READY   UP-TO-DATE   AVAILABLE   AGE
        web-proj-268   0/1     1            0           13s
        devops@T480:~/exam$ kubectl get deploy
        NAME           READY   UP-TO-DATE   AVAILABLE   AGE
        web-proj-268   0/1     1            0           22s
        devops@T480:~/exam$ kubectl describe deploy web-proj-268
        Name:                   web-proj-268
        Namespace:              default
        CreationTimestamp:      Wed, 20 Dec 2023 19:26:47 +0500
        Labels:                 app=web-proj-268
        Annotations:            deployment.kubernetes.io/revision: 1
        Selector:               app=web-proj-268
        Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
        StrategyType:           RollingUpdate
        MinReadySeconds:        0
        RollingUpdateStrategy:  25% max unavailable, 25% max surge
        Pod Template:
        Labels:  app=web-proj-268
        Containers:
        nginx:
            Image:        nginx:1.16
            Port:         <none>
            Host Port:    <none>
            Environment:  <none>
            Mounts:       <none>
        Volumes:        <none>
        Conditions:
        Type           Status  Reason
        ----           ------  ------
        Available      False   MinimumReplicasUnavailable
        Progressing    True    ReplicaSetUpdated
        OldReplicaSets:  <none>
        NewReplicaSet:   web-proj-268-68b8987b5d (1/1 replicas created)
        Events:
        Type    Reason             Age   From                   Message
        ----    ------             ----  ----                   -------
        Normal  ScalingReplicaSet  61s   deployment-controller  Scaled up replica set web-proj-268-68b8987b5d to 1
        devops@T480:~/exam$ kubectl set image deployment web-proj-268 nginx=nginx
        :1.17 --record
        Flag --record has been deprecated, --record will be removed in the future
        deployment.apps/web-proj-268 image updated
        devops@T480:~/exam$ kubectl describe deploy web-proj-268
        Name:                   web-proj-268
        Namespace:              default
        CreationTimestamp:      Wed, 20 Dec 2023 19:26:47 +0500
        Labels:                 app=web-proj-268
        Annotations:            deployment.kubernetes.io/revision: 2
                                kubernetes.io/change-cause: kubectl set image deployment web-proj-268 nginx=nginx:1.17 --record=true
        Selector:               app=web-proj-268
        Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
        StrategyType:           RollingUpdate
        MinReadySeconds:        0
        RollingUpdateStrategy:  25% max unavailable, 25% max surge
        Pod Template:
        Labels:  app=web-proj-268
        Containers:
        nginx:
            Image:        nginx:1.17

devops@T480:~/exam$ kubectl rollout history deployment web-proj-268
deployment.apps/web-proj-268
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment web-proj-268 nginx=nginx:1.17 --record=true
================================================================================
> Create a new deployment called web-003. Scale the deployment to 3 replicas.
> Make sure desired number of pod always running.
* Trick question, controller manager handles replication
if pods are not getting started , check out issues with controller manager
check /etc/kubernetes/manifest

devops@T480:~/exam$ kubectl create deployment web-003 --image=nginx --rep
licas=3
deployment.apps/web-003 created
devops@T480:~/exam$ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
web-003        0/3     0            0           17s
================================================================================

> Upgrade the Cluster (Master and worker Node) from 1.18.0 to 1.19.0.
> Make sure to first drain both Node and make it available after upgrade.

devops@T480:~$ kubectl get nodes
NAME      STATUS   ROLES           AGE   VERSION
control   Ready    control-plane   47h   v1.28.0
worker    Ready    <none>          47h   v1.28.0

devops@T480:~$ kubectl drain control --ignore-daemonsets
node/control cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-85s8l, kube-system/weave-net-zwgk7
evicting pod kube-system/coredns-5dd5756b68-58mvg
evicting pod kube-system/coredns-5dd5756b68-4hcmh




^C
devops@T480:~$ kubectl get nodes
NAME      STATUS                     ROLES           AGE   VERSION
control   Ready,SchedulingDisabled   control-plane   47h   v1.28.0
worker    Ready                      <none>          47h   v1.28.0

root@control:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.0", GitCommit:"1b4df30b3cdfeaba6024e81e559a6cd09a089d65", GitTreeState:"clean", BuildDate:"2023-04-11T17:09:06Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
root@control:~# apt install kubeadm=1.28.0-00
root@control:~# kubeadm upgrade apply v1.28.0
root@control:~# apt install kubelet=1.28.0-00
root@control:~# kubectl uncordon control
root@control:~# kubectl drain worker --ignore-daemonsets
ssh worker
apt update
apt install kubeadm=1.28.0-00
kubeadm upgrade node
apt install kubelet-1.28.0-00
systemctl restart kubelet
kubectl uncordon worker
================================================================================
> Deploy a web-load-5461 pod using the nginx:l .17 image with the labels set to tier=web.
devops@T480:~/exam$ kubectl run web-load-5461 --image=nginx:1.17 --labels tier=web --dry-run -o yaml
W1220 21:28:41.707389    6864 helpers.go:692] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    tier: web
  name: web-load-5461
spec:
  containers:
  - image: nginx:1.17
    name: web-load-5461
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
================================================================================
> expose exiting pod nginxpod as a service, service name should be nginxsvc pod port = 80
kubectl expose pod nginxpod --name=nginxsvc --port=80
nginxsvc     ClusterIP   10.110.247.77    <none>        80/TCP    17s
curl 10.110.247.77

> expose exiting pod nginxpod as a service, service name should be nginxnodeportsvc pod port = 30200

devops@T480:~/exam$ kubectl expose pod nginxpod --name=nginxnodeportsvc --type=NodePort --port=80
service/nginxnodeportsvc exposed
devops@T480:~/exam$ kubectl get svc
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes         ClusterIP   10.96.0.1        <none>        443/TCP        2d1h
mynginx            ClusterIP   10.103.201.221   <none>        80/TCP         27h
nginxnodeportsvc   NodePort    10.104.89.115    <none>        80:31629/TCP   14s
nginxsvc           ClusterIP   10.110.247.77    <none>        80/TCP         3m57s
devops@T480:~/exam$ kubectl edit svc nginxnodeportsvc
and edit port to 30200

devops@T480:~/exam$ kubectl get nodes -o wide
NAME      STATUS   ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION    CONTAINER-RUNTIME
control   Ready    control-plane   2d1h   v1.28.0   146.190.32.90    <none>        Ubuntu 23.10   6.5.0-9-generic   containerd://1.7.2
worker    Ready    <none>          2d     v1.28.0   143.198.53.104   <none>        Ubuntu 23.10   6.5.devops@T480:~/exam$
devops@T480:~/exam$ curl 146.190.32.90:30200
================================================================================
> A pod "my-nginx-pod" (image=nginx) in custom namespace is not running. Find the problem and fix it and make it running. Note: All the supported definition files has been placed at root.
devops@T480:~/exam$ kubectl -n custom get pods
No resources found in custom namespace.
devops@T480:~/exam$ kubectl -n custom describe pod mynginxpod
devops@T480:~/exam$ ls
pv.yml pvc.yml
devops@T480:~/exam$ kubectl get pv
BOUND
issue: pvc is a namespaced resource, create it in custom namespace:
devops@T480:~/exam$ kubectl get pvc
devops@T480:~/exam$ kubectl get pvc -n custom
no resource
devops@T480:~/exam$ kubectl delete pvc pv-claim-log
devops@T480:~/exam$ kubectl get pv
RELEASED
devops@T480:~/exam$ kubectl delete pv mypvlog
devops@T480:~/exam$ vim pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-claim-log
  namespace: custom   <-------------------------------->
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
devops@T480:~/exam$ kubectl apply -f pv.yml
devops@T480:~/exam$ kubectl get pv
AVAILABLE
devops@T480:~/exam$ kubectl apply -f pvc.yml
devops@T480:~/exam$ kubectl get pvc -n custom
BOUND
devops@T480:~/exam$ kubectl get pod -n custom
RUNNING
================================================================================
> Create a multi-container pod, "multi-pod" in development namespace using images:
> nginx and redis.

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
  namespace: development
spec:
  containers:
  - image: nginx
    name: nginx
  - image: redis
    name: redis
================================================================================
> A pod "nginx-pod" (image=nginx) in default namespace is not running.
> Find the problem and fix it and make it running.

devops@T480:~/exam$  kubectl describe node worker | grep -i taint
Taints:             <none>
devops@T480:~/exam$ kubectl describe node control | grep -i taint
Taints:             color=blue:NoSchedule
devops@T480:~/exam$ kubectl get pod nginx-pod -o yaml > pod.yaml
spec:
  containers:
  ..........
  tolerations:
  - key: 'color'
    operator: 'Equal'
    value: 'blue'
    effect: 'NoSchedule'
================================================================================
> Create a new deployment called nginx-deploy, with image nginx:l .16 and 8 replica. There are 5 worker node in cluster. Please make sure no pod will get deployed on 2 worker node, mentioned below:
Worker-node-I
Worker-node-2


devops@T480:~/exam$ kubectl cordon worker-node-1
node/worker cordoned
devops@T480:~/exam$ kubectl cordon worker-node-2
node/worker cordoned
devops@T480:~/exam$ kubectl get nodes
NAME      STATUS                     ROLES           AGE     VERSION
control   Ready                      control-plane   2d15h   v1.28.0
worker1    Ready,SchedulingDisabled   <none>          2d15h   v1.28.0
worker2    Ready,SchedulingDisabled   <none>          2d15h   v1.28.0
worker3   Ready                       <none>          2d15h   v1.28.0

devops@T480:~/exam$ kubectl create deployment nginx-deployment --image nginx:1.16 --replicas 8 --dry-run=client -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-deployment
  name: nginx-deployment
spec:
  replicas: 8
  selector:
    matchLabels:
      app: nginx-deployment
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-deployment
    spec:
      containers:
      - image: nginx:1.16
        name: nginx

devops@T480:~/exam$ kubectl uncordon worker-node-1
node/worker uncordoned
devops@T480:~/exam$ kubectl uncordon worker-node-2
node/worker uncordoned
devops@T480:~/exam$ kubectl get nodes
NAME      STATUS                     ROLES           AGE     VERSION
control   Ready                      control-plane   2d15h   v1.28.0
worker1   Ready                       <none>          2d15h   v1.28.0
worker2   Ready                       <none>          2d15h   v1.28.0
worker3   Ready                       <none>          2d15h   v1.28.0
================================================================================
> Create a ReplicaSet (Name: web-pod, Image: nginx:1.16, Replica: 3)
> There already a Pod running in cluster.
> Please make sure that total count of pod running into a cluster is not more than 3


devops@T480:~/exam$ kubectl get pods --show-labels
NAME                            READY   STATUS        RESTARTS   AGE   LABELS
nginxpod                        1/1     Running       0          15h   run=nginxpod


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    run: nginxpod
  name: my-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginxpod
  template:
    metadata:
      labels:
        run: nginxpod
    spec:
      containers:
        - image: nginx
          name: nginx


devops@T480:~/exam$ kubectl get pods --show-labels
NAME                            READY   STATUS        RESTARTS   AGE   LABELS
my-rs-l5nls                     1/1     Running       0          23s   run=nginxpod
my-rs-vmsqm                     1/1     Running       0          24s   run=nginxpod
nginxpod                        1/1     Running       0          15h   run=nginxpod

devops@T480:~/exam$ kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
my-rs                     3         3         3       64s
================================================================================
> There are 3 Node in the Cluster, create DaemonSet (Name: my-pod, Image: nginx)
> on each node except one node (Worker-node-3)

taint one node and deploy daemonset
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: my-pod
  name: my-pod
spec:
  selector:
    matchLabels:
      app: my-pod
  template:
    metadata:
      labels:
        app: my-pod
    spec:
      containers:
        - image: nginx
          name: nginx
================================================================================
> Generate a file (CKA007.txt) with details about the available size of all the node in a
> Kubernetes cluster using a custom column, format as mentioned below:

kubectl get node -o json
kubectl get node -o custom-columns=NAME:.metadata.name,AVAILABLE_MEMORY:.status.allocatable.memory,AVAILABLE_CPU:.status.allocatable.cpu
kubectl get node -o custom-columns=NAME:.metadata.name,AVAILABLE_MEMORY:.status.allocatable.memory,AVAILABLE_CPU:.status.allocatable.cpu > CKA007.txt
================================================================================
> There are various Pods running in all the namespaces of Kubernetes Cluster. Write a
> command into "/opt/pods_asc.sh" which lists all the
> Pods sorted by their AGE in Ascending order.

devops@T480:~/exam$ kubectl get pods -A --sort-by=.metadata.creationTimestamp
echo "kubectl get pods -A --sort-by=.metadata.creationTimestamp | tac" > pods_asc.sh
chmod +x pods_asc.sh
================================================================================
Create a new deployment called web-proj-268, with
image nginx:l .16 and 1 replica. Next upgrade the
deployment to version 1.17 using rolling update.
Make sure that the version upgrade is recorded in
the resource annotation.

devops@T480:~/exam$ kubectl set image deployment web-proj-269 nginx=nginx:1.17
  deployment.apps/web-proj-269 image updated

devops@T480:~/exam$ kubectl set image deployment web-proj-269 nginx=nginx:1.17 --rec
ord
devops@T480:~/exam$ kubectl rollout history deployment web-proj-269
deployment.apps/web-proj-269
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment web-proj-269 nginx=nginx:1.17 --record=true
================================================================================
> Create a Pod 'nginx-k8s" using image nginx and
initContainer git-k8s" with image alpine/git.
Volume mount path of the main container
"/usr/share/nginx/html"
Nginx index.html need to be override with shared
volume.
Index.html file cloned from path
> https://github.com/jhawithu/k8s-nginx.git'

apiVersion: v1
kind: Pod
metadata:
  name: nginx-k8s
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: gitvol
  initContainers:
  - image: alpine/git
    name: alpine
    args:
    - clone
    - --single-branch
    - --
    - https://github.com/jhawithu/k8s-nginx.git
    - /data
    volumeMounts:
    - mountPath: /data
      name: gitvol
  volumes:
  - name: gitvol
    emptyDir: {}

================================================================================
> Create a Deployment with below information mentioned
in the table using the provided file "deployment.yaml" at
location "/root".
There are some typo in the file, that need to fix accordingly
and create the deployment successfully in default
namespace.
Deployment Name  nginx-deployment
Image  nginx:latest
ContainerPort  80
> Replicas  3

================================================================================
> Auto scale the existing deployment frontend in production namespace at 80% of pod CPU usage, and set Minimum replicas= 3 and Maximum replicas= 5

kubectl -n production autoscale deployment frontend --min=3 --max=5 --cpu-percent=80

controlplane $ kubectl -n production autoscale deployment frontend --min=3 --max=5 --cpu-percent=80
horizontalpodautoscaler.autoscaling/frontend autoscaled
controlplane $ k get hpa -n production
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
frontend   Deployment/frontend   <unknown>/80%   3         5         0          13s
================================================================================
> Expose existing deployment in production namespace named as frontend through Nodeport and Nodeport service name should be frontendsvc
controlplane $ kubectl expose deployment frontend --name=frontendsvc --type=NodePort --port=80 -n production
service/frontendsvc exposed
controlplane $ k get svc -n production
NAME          TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
frontendsvc   NodePort   10.111.164.191   <none>        80:31905/TCP   10s

================================================================================
> deploy a pod,
> dont change anything on the nodes
> check and workaround why pod is not deploying

controlplane $ k get nodes node01 -o jsonpath='{.spec.taints}'
[{"effect':'NoSchedule','key':'node-role.kubernetes.io/node'}]

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web-pod
  name: web-pod
spec:
  tolerations:
  - key: 'node-role.kubernetes.io/node'
    operator: 'Exists'
    effect: 'NoSchedule'
  containers:
  - image: httpd
    name: web-pod
================================================================================
> Create a new PersistentVolume named web-pv. It should have a capacity of 2Gi,
accessMode ReadWriteOnce, hostPath /vol/data and no storageCIassName defined.
> Next create a new PersistentVolumeClaim in Namespace production named web-pvc . It
should request 2Gi storage, accessMode ReadWriteOnce and should not define a
storageClassName. The PVC should bound to the PV correctly.
> Finally create a new Deployment web-deploy in Namespace production which mounts that
volume at /tmp/web-data. The Pods of that ployment should be of
image nginx:1.Ä.2


apiVersion: v1
kind: PersistentVolume
metadata:
  name: web-pv
spec:
  storageClassName: manual
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  hostpath:
    path: /vol/data


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: web-pvc
  namespace: prod
spec:
  storageClassName: ""
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: web
  name: web-deploy
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      volumes:
      - name: web-deploy-vol
        persistentVolumeClaim:
          claimName: web-pvc
      containers:
      - image: nginx
        name: web-deploy-cont
        volumeMounts:
        - mountPath: /tmp/web-data
          name: web-deploy-vol


controlplane $ k get pvc -n prod
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
web-pvc   Bound    web-pv   2Gi        RWO                           16m

================================================================================
Q.9 Create a Kubernetes Pod named "my-busybox" with
the busybox:1.31.1 image. The Pod should run a sleep
command for 4800 seconds. Verify that the Pod is running in Node01.

controlplane $ kubectl run my-busybox --image=busybox:1.31.1 --command sleep 4800

controlplane $ kubectl uncordon node01
node/node01 already uncordoned
================================================================================
Q.IO You have a Kubernetes cluster that runs a three-tier web
application: a frontend tier (port 80), an applicatioa tier (port 8080),
and a backend tier (3306). The security team has mandated that the
backend tier should only be accessible from the application tier.
================================================================================

================================================================================

================================================================================
KILLER.SH

Task weight: 1%

You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts.

Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl.

Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl.
------------------------------------------------------
kubectl config get-contexts | awk {'print $2'} | tail -n+2 > context

echo "kubectl config get-contexts | grep * | awk {'print $2'}"
echo "cat .kube/config | grep current-context | awk {'print $2'}"


kubectl config get-contexts -o name
kubectl config current-context
cat ~/.kube/config | grep current | awk '{print $2}'

================================================================================



Task weight: 3%

Use context: kubectl config use-context k8s-c1-H

Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on controlplane nodes. Do not add new labels to any nodes.
---------------------------------------
k get node # find controlplane node

k describe node cluster1-controlplane1 | grep Taint -A1 
# get controlplane node taints

k get node cluster1-controlplane1 --show-labels 
# get controlplane node labels
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: httpd:2.4.41-alpine
    name: pod1-container                       # change
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:                                 # add
  - effect: NoSchedule                         # add
    key: node-role.kubernetes.io/control-plane # add
  nodeSelector:                                # add
    node-role.kubernetes.io/control-plane: ""  # add
================================================================================
Task weight: 1%

Use context: kubectl config use-context k8s-c1-H

There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources.
-------------------------------------------------
k -n project-c13 scale sts o3db --replicas 1
================================================================================
Task weight: 4%

Use context: kubectl config use-context k8s-c1-H

Do the following in Namespace default. Create a single Pod named ready-if-service-ready of image nginx:1.16.1-alpine. Configure a LivenessProbe which simply executes command true. Also configure a ReadinessProbe which does check if the url http://service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe.

Create a second Pod named am-i-ready of image nginx:1.16.1-alpine with label id: cross-server-ready. The already existing Service service-am-i-ready should now have that second Pod as endpoint.

Now the first Pod should be in ready state, confirm that.

---------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ready-if-service-ready
  name: ready-if-service-ready
spec:
  containers:
  - image: nginx:1.16.1-alpine
    name: ready-if-service-ready
    resources: {}
    livenessProbe:                                      # add from here
      exec:
        command:
        - 'true'
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - 'wget -T2 -O- http://service-am-i-ready:80'   # to here


k describe pod ready-if-service-ready
 ...
  Warning  Unhealthy  18s   kubelet, cluster1-node1  Readiness probe failed: Connecting to service-am-i-ready:80 (10.109.194.234:80)
wget: download timed out


k run am-i-ready --image=nginx:1.16.1-alpine --labels="id=cross-server-ready"


k describe svc service-am-i-ready
k get ep # also possible
Which will result in our first Pod being ready, just give it a minute for the Readiness probe to check again:

➜ k get pod ready-if-service-ready
NAME                     READY   STATUS    RESTARTS   AGE
ready-if-service-ready   1/1     Running   0          53s
================================================================================
Task weight: 1%

Use context: kubectl config use-context k8s-c1-H

There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE (metadata.creationTimestamp).

Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands.

----------------------------------------------------
# /opt/course/5/find_pods.sh
kubectl get pod -A --sort-by=.metadata.creationTimestamp

================================================================================
Task weight: 8%

Use context: kubectl config use-context k8s-c1-H

Create a new PersistentVolume named safari-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined.

Next create a new PersistentVolumeClaim in Namespace project-tiger named safari-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly.

Finally create a new Deployment safari in Namespace project-tiger which mounts that volume at /tmp/safari-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine.

-----------------------------------------------
# 6_pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
 name: safari-pv
spec:
 capacity:
  storage: 2Gi
 accessModes:
  - ReadWriteOnce
 hostPath:
  path: "/Volumes/Data"

# 6_pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: safari-pvc
  namespace: project-tiger
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
     storage: 2Gi

# 6_dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: safari
  name: safari
  namespace: project-tiger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: safari
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: safari
    spec:
      volumes:                                      # add
      - name: data                                  # add
        persistentVolumeClaim:                      # add
          claimName: safari-pvc                     # add
      containers:
      - image: httpd:2.4.41-alpine
        name: container
        volumeMounts:                               # add
        - name: data                                # add
          mountPath: /tmp/safari-data               # add


k -n project-tiger describe pod safari-5cbf46d6d-mjhsb  | grep -A2 Mounts:   
    Mounts:
      /tmp/safari-data from data (rw) # there it is
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2sjj (ro)
================================================================================
Task weight: 1%

Use context: kubectl config use-context k8s-c1-H

The metrics-server has been installed in the cluster. Your college would like to know the kubectl commands to:

show Nodes resource usage
show Pods and their containers resource usage

Please write the commands into /opt/course/7/node.sh and /opt/course/7/pod.sh.
---------------------------------------------
kubectl top node
kubectl top pods --containers=true

================================================================================
Task weight: 2%

Use context: kubectl config use-context k8s-c1-H

Ssh into the controlplane node with ssh cluster1-controlplane1. Check how the controlplane components kubelet, kube-apiserver, kube-scheduler, kube-controller-manager and etcd are started/installed on the controlplane node. Also find out the name of the DNS application and how it's started/installed on the controlplane node.

Write your findings into file /opt/course/8/controlplane-components.txt. The file should be structured like:

# /opt/course/8/controlplane-components.txt
kubelet: [TYPE]
kube-apiserver: [TYPE]
kube-scheduler: [TYPE]
kube-controller-manager: [TYPE]
etcd: [TYPE]
dns: [TYPE] [NAME]
Choices of [TYPE] are: not-installed, process, static-pod, pod


-----------------------------------------
There we see the 5 static pods, with -cluster1-controlplane1 as suffix.

# /opt/course/8/controlplane-components.txt
kubelet: process
kube-apiserver: static-pod
kube-scheduler: static-pod
kube-controller-manager: static-pod
etcd: static-pod
dns: pod coredns
================================================================================
Task weight: 5%

Use context: kubectl config use-context k8s-c2-AC

Ssh into the controlplane node with ssh cluster2-controlplane1. Temporarily stop the kube-scheduler, this means in a way that you can start it again afterwards.

Create a single Pod named manual-schedule of image httpd:2.4-alpine, confirm it's created but not scheduled on any node.

Now you're the scheduler and have all its power, manually schedule that Pod on node cluster2-controlplane1. Make sure it's running.

Start the kube-scheduler again and confirm it's running correctly by creating a second Pod named manual-schedule2 of image httpd:2.4-alpine and check if it's running on cluster2-node1.

------------------------------------------
➜ root@cluster2-controlplane1:~# mv ../kube-scheduler.yaml .

spec:
  nodeName: cluster2-controlplane1        
  # add the controlplane node name
  containers:

➜ root@cluster2-controlplane1:~# mv ../kube-scheduler.yaml .

k run manual-schedule2 --image=httpd:2.4-alpine
================================================================================
Task weight: 6%

Use context: kubectl config use-context k8s-c1-H

Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace.

-------------------------------------------------------
A ClusterRole|Role defines a set of permissions and where it is available, in the whole cluster or just a single Namespace.

A ClusterRoleBinding|RoleBinding connects a set of permissions with an account and defines where it is applied, in the whole cluster or just a single Namespace.

Because of this there are 4 different RBAC combinations and 3 valid ones:

Role + RoleBinding (available in single Namespace, applied in single Namespace)

ClusterRole + ClusterRoleBinding (available cluster-wide, applied cluster-wide)

ClusterRole + RoleBinding (available cluster-wide, applied in single Namespace)

Role + ClusterRoleBinding (NOT POSSIBLE: available in single Namespace, applied cluster-wide)

<1>
➜ k -n project-hamster create sa processor

<2>
Then for the Role:

k -n project-hamster create role -h # examples
So we execute:

k -n project-hamster create role processor \
  --verb=create \
  --resource=secret \
  --resource=configmap

<3>
Now we bind the Role to the ServiceAccount:

k -n project-hamster create rolebinding -h # examples
So we create it:

k -n project-hamster create rolebinding processor \
  --role processor \
  --serviceaccount project-hamster:processor

--------------------------------
To test our RBAC setup we can use kubectl auth can-i:

k auth can-i -h # examples
Like this:

➜ k -n project-hamster auth can-i create secret \
  --as system:serviceaccount:project-hamster:processor
yes

➜ k -n project-hamster auth can-i create configmap \
  --as system:serviceaccount:project-hamster:processor
yes

➜ k -n project-hamster auth can-i create pod \
  --as system:serviceaccount:project-hamster:processor
no

➜ k -n project-hamster auth can-i delete secret \
  --as system:serviceaccount:project-hamster:processor
no

➜ k -n project-hamster auth can-i get configmap \
  --as system:serviceaccount:project-hamster:processor
================================================================================
Task weight: 4%

Use context: kubectl config use-context k8s-c1-H

Use Namespace project-tiger for the following. Create a DaemonSet named ds-important with image httpd:2.4-alpine and labels id=ds-important and uuid=18426a0b-5f59-4e10-923f-c0e078e82462. The Pods it creates should request 10 millicore cpu and 10 mebibyte memory. The Pods of that DaemonSet should run on all nodes, also controlplanes.
----------------------------------------------
# 11.yaml
apiVersion: apps/v1
kind: DaemonSet                                     # change from Deployment to Daemonset
metadata:
  creationTimestamp: null
  labels:                                           # add
    id: ds-important                                # add
    uuid: 18426a0b-5f59-4e10-923f-c0e078e82462      # add
  name: ds-important
  namespace: project-tiger                          # important
spec:
  selector:
    matchLabels:
      id: ds-important                              # add
      uuid: 18426a0b-5f59-4e10-923f-c0e078e82462    # add
  #strategy: {}                                     # remove
  template:
    metadata:
      creationTimestamp: null
      labels:
        id: ds-important                            # add
        uuid: 18426a0b-5f59-4e10-923f-c0e078e82462  # add
    spec:
      containers:
      - image: httpd:2.4-alpine
        name: ds-important
        resources:
          requests:                                 # add
            cpu: 10m                                # add
            memory: 10Mi                            # add
      tolerations:                                  # add
      - effect: NoSchedule                          # add
        key: node-role.kubernetes.io/control-plane  # add
#status: {}                                         # remove
---------------------------------------------------
Use Namespace project-tiger for the following. Create a DaemonSet named ds-important with image httpd:2.4-alpine and labels id=ds-important and uuid=18426a0b-5f59-4e10-923f-c0e078e82462. The Pods it creates should request 10 millicore cpu and 10 mebibyte memory. The Pods of that DaemonSet should run on all nodes, also controlplanes.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    id=ds-important
    uuid=18426a0b-5f59-4e10-923f-c0e078e82462
  name: ds-important
  namespace: project-tiger
spec:
  selector:
    matchLabels:
      id: ds-important
      uuid=18426a0b-5f59-4e10-923f-c0e078e82462
  template:
    metadata:
      name: ds-important
    labels:
      id: ds-important
      uuid=18426a0b-5f59-4e10-923f-c0e078e82462
    spec:
      containers:
      - image: httpd:2.4-alpine
        name: ds-important-conf
      resources:
        requests:
          memory: 10Mb
          cpu: 10m
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: 'Exists'
        effect: 'NoSchedule'

================================================================================
Task weight: 6%

Use context: kubectl config use-context k8s-c1-H

Use Namespace project-tiger for the following. Create a Deployment named deploy-important with label id=very-important (the Pods should also have this label) and 3 replicas. It should contain two containers, the first named container1 with image nginx:1.17.6-alpine and the second one named container2 with image google/pause.

There should be only ever one Pod of that Deployment running on one worker node. We have two worker nodes: cluster1-node1 and cluster1-node2. Because the Deployment has three replicas the result should be that on both nodes one Pod is running. The third Pod won't be scheduled, unless a new worker node will be added.

In a way we kind of simulate the behaviour of a DaemonSet here, but using a Deployment and a fixed number of replicas.
---------------------------------------------------
# 12.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    id: very-important                  # change
  name: deploy-important
  namespace: project-tiger              # important
spec:
  replicas: 3                           # change
  selector:
    matchLabels:
      id: very-important                # change
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        id: very-important              # change
    spec:
      containers:
      - image: nginx:1.17.6-alpine
        name: container1                # change
        resources: {}
      - image: google/pause             # add
        name: container2                # add
      affinity:                                             # add
        podAntiAffinity:                                    # add
          requiredDuringSchedulingIgnoredDuringExecution:   # add
          - labelSelector:                                  # add
              matchExpressions:                             # add
              - key: id                                     # add
                operator: In                                # add
                values:                                     # add
                - very-important                            # add
            topologyKey: kubernetes.io/hostname             # add
status: {}

================================================================================
Task weight: 4%

Use context: kubectl config use-context k8s-c1-H

Create a Pod named multi-container-playground in Namespace default with three containers, named c1, c2 and c3. There should be a volume attached to that Pod and mounted into every container, but the volume shouldn't be persisted or shared with other Pods.

Container c1 should be of image nginx:1.17.6-alpine and have the name of the node where its Pod is running available as environment variable MY_NODE_NAME.

Container c2 should be of image busybox:1.31.1 and write the output of the date command every second in the shared volume into file date.log. You can use while true; do date >> /your/vol/path/date.log; sleep 1; done for this.

Container c3 should be of image busybox:1.31.1 and constantly send the content of file date.log from the shared volume to stdout. You can use tail -f /your/vol/path/date.log for this.

Check the logs of container c3 to confirm correct setup.
---------------------------------------------------
# 13.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-container-playground
  name: multi-container-playground
spec:
  containers:
  - image: nginx:1.17.6-alpine
    name: c1                                                                     
    resources: {}
    env:                                                                         
    - name: MY_NODE_NAME                                                         
      valueFrom:                                                                 
        fieldRef:                                                                
          fieldPath: spec.nodeName                                               
    volumeMounts:                                                                
    - name: vol                                                                  
      mountPath: /vol                                                            
  - image: busybox:1.31.1                                                        
    name: c2                                                                     
    command: ["sh", "-c", "while true; do date >> /vol/date.log; sleep 1; done"] 
    volumeMounts:                                                                
    - name: vol                                                                  
      mountPath: /vol                                                            
  - image: busybox:1.31.1                                                        
    name: c3                                                                     
    command: ["sh", "-c", "tail -f /vol/date.log"]                                    volumeMounts:                                                                
    - name: vol                                                                  
      mountPath: /vol                                                            
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:                                                                       
    - name: vol                                                                  
      emptyDir: {}                                                               
status: {}
================================================================================
Task weight: 2%

Use context: kubectl config use-context k8s-c1-H

You're ask to find out following information about the cluster k8s-c1-H :

How many controlplane nodes are available?
How many worker nodes are available?
What is the Service CIDR?
Which Networking (or CNI Plugin) is configured and where is its config file?
Which suffix will static pods have that run on cluster1-node1?
Write your answers into file /opt/course/14/cluster-info, structured like this:

# /opt/course/14/cluster-info
1: [ANSWER]
2: [ANSWER]
3: [ANSWER]
4: [ANSWER]
5: [ANSWER]


------------------------------------------------
How many controlplane and worker nodes are available?
➜ k get node
NAME                    STATUS   ROLES          AGE   VERSION
cluster1-controlplane1  Ready    control-plane  27h   v1.28.2
cluster1-node1          Ready    <none>         27h   v1.28.2
cluster1-node2          Ready    <none>         27h   v1.28.2


What is the Service CIDR?
➜ ssh cluster1-controlplane1

➜ root@cluster1-controlplane1:~# cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep range
    - --service-cluster-ip-range=10.96.0.0/12

Result
The resulting /opt/course/14/cluster-info could look like:

# /opt/course/14/cluster-info

# How many controlplane nodes are available?
1: 1

# How many worker nodes are available?
2: 2

# What is the Service CIDR?
3: 10.96.0.0/12

# Which Networking (or CNI Plugin) is configured and where is its config file?
4: Weave, /etc/cni/net.d/10-weave.conflist

# Which suffix will static pods have that run on cluster1-node1?
5: -cluster1-node1
 
================================================================================
Task weight: 3%

Use context: kubectl config use-context k8s-c2-AC

Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time (metadata.creationTimestamp). Use kubectl for it.

Now delete the kube-proxy Pod running on node cluster2-node1 and write the events this caused into /opt/course/15/pod_kill.log.

Finally kill the containerd container of the kube-proxy Pod on node cluster2-node1 and write the events into /opt/course/15/container_kill.log.

Do you notice differences in the events both actions caused?
--------------------------------------------------------------------------
# /opt/course/15/cluster_events.sh
kubectl get events -A --sort-by=.metadata.creationTimestamp
Now we delete the kube-proxy Pod:

k -n kube-system get pod -o wide | grep proxy # find pod running on cluster2-node1

k -n kube-system delete pod kube-proxy-z64cg
Now check the events:

sh /opt/course/15/cluster_events.sh
Write the events the killing caused into /opt/course/15/pod_kill.log:

ssh cluster2-node1
crictl ps | grep proxy
crictl stop b7ce743f3ffv
crictl rm b7ce743f3ffv
================================================================================
Task weight: 2%

Use context: kubectl config use-context k8s-c1-H

Write the names of all namespaced Kubernetes resources (like Pod, Secret, ConfigMap...) into /opt/course/16/resources.txt.

Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt.


------------------------------------------------------------------------------
k api-resources    # shows all

k api-resources -h # help always good

k api-resources --namespaced -o name > /opt/course/16/resources.txt
Which results in the file:

# /opt/course/16/resources.txt
bindings
configmaps
endpoints
events
limitranges
persistentvolumeclaims
pods
podtemplates
replicationcontrollers
resourcequotas
secrets
serviceaccounts
services
controllerrevisions.apps
daemonsets.apps
deployments.apps
replicasets.apps
statefulsets.apps
localsubjectaccessreviews.authorization.k8s.io
horizontalpodautoscalers.autoscaling
cronjobs.batch
jobs.batch
leases.coordination.k8s.io
events.events.k8s.io
ingresses.extensions
ingresses.networking.k8s.io
networkpolicies.networking.k8s.io
poddisruptionbudgets.policy
rolebindings.rbac.authorization.k8s.io
roles.rbac.authorization.k8s.io


➜ k -n project-c14 get role --no-headers | wc -l
300
================================================================================
Question 17
Task weight: 3%

Use context: kubectl config use-context k8s-c1-H

In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod=container and container=pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod.

Using command crictl:

Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt

Write the logs of the container into /opt/course/17/pod-container.log
---------------------------------------------------------------------------------
➜ ssh cluster1-node2

➜ root@cluster1-node2:~# crictl ps | grep tigers-reunite
b01edbe6f89ed    54b0995a63052    5 seconds ago    Running        tigers-reunite ...

➜ root@cluster1-node2:~# crictl inspect b01edbe6f89ed | grep runtimeType
    "runtimeType": "io.containerd.runc.v2",

Then we fill the requested file (on the main terminal):

# /opt/course/17/pod-container.txt
b01edbe6f89ed io.containerd.runc.v2


Finally we write the container logs in the second file:

ssh cluster1-node2 'crictl logs b01edbe6f89ed' &> /opt/course/17/pod-container.log
================================================================================
Question 18
Task weight: 8%

Use context: kubectl config use-context k8s-c3-CCC

There seems to be an issue with the kubelet not running on cluster3-node1. Fix it and confirm that cluster has node cluster3-node1 available in Ready state afterwards. You should be able to schedule a Pod on cluster3-node1 afterwards.

Write the reason of the issue into /opt/course/18/reason.txt.
--------------------------------------------------------------------------
➜ root@cluster3-node1:~# service kubelet status
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: activating (auto-restart) (Result: exit-code) since Thu 2020-04-30 22:03:10 UTC; 3s ago
     Docs: https://kubernetes.io/docs/home/
  Process: 5989 ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=203/EXEC)

  whereis kubelet 
  -> /usr/bin/kubelet

  mismatch between 2 locations

  journalctl -u kubelet

  Well, there we have it, wrong path specified. Correct the path in file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf and run:

vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # fix

systemctl daemon-reload && systemctl restart kubelet

systemctl status kubelet  # should now show running

# /opt/course/18/reason.txt
wrong path to kubelet binary specified in service config
================================================================================

Question 19
Task weight: 3%

NOTE: This task can only be solved if questions 18 or 20 have been successfully implemented and the k8s-c3-CCC cluster has a functioning worker node

Use context: kubectl config use-context k8s-c3-CCC

Do the following in a new Namespace secret. Create a Pod named secret-pod of image busybox:1.31.1 which should keep running for some time.

There is an existing Secret located at /opt/course/19/secret1.yaml, create it in the Namespace secret and mount it readonly into the Pod at /tmp/secret1.

Create a new Secret in Namespace secret called secret2 which should contain user=user1 and pass=1234. These entries should be available inside the Pod's container as environment variables APP_USER and APP_PASS.

Confirm everything is working.
-----------------------------------------------------
k create ns secret

cp /opt/course/19/secret1.yaml 19_secret1.yaml

vim 19_secret1.yaml
We need to adjust the Namespace for that Secret:

# 19_secret1.yaml
apiVersion: v1
data:
  halt: IyEgL2Jpbi9zaAo...
kind: Secret
metadata:
  creationTimestamp: null
  name: secret1
  namespace: secret           # change
k -f 19_secret1.yaml create
Next we create the second Secret:

k -n secret create secret generic secret2 --from-literal=user=user1 --from-literal=pass=1234
Now we create the Pod template:

k -n secret run secret-pod --image=busybox:1.31.1 $do -- sh -c "sleep 5d" > 19.yaml

vim 19.yaml
Then make the necessary changes:

# 19.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: secret-pod
  name: secret-pod
  namespace: secret                       # add
spec:
  containers:
  - args:
    - sh
    - -c
    - sleep 1d
    image: busybox:1.31.1
    name: secret-pod
    resources: {}
    env:                                  # add
    - name: APP_USER                      # add
      valueFrom:                          # add
        secretKeyRef:                     # add
          name: secret2                   # add
          key: user                       # add
    - name: APP_PASS                      # add
      valueFrom:                          # add
        secretKeyRef:                     # add
          name: secret2                   # add
          key: pass                       # add
    volumeMounts:                         # add
    - name: secret1                       # add
      mountPath: /tmp/secret1             # add
      readOnly: true                      # add
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:                                # add
  - name: secret1                         # add
    secret:                               # add
      secretName: secret1                 # add
status: {}
It might not be necessary in current K8s versions to specify the readOnly: true because it's the default setting anyways.

And execute:

k -f 19.yaml create
Finally we check if all is correct:

➜ k -n secret exec secret-pod -- env | grep APP
APP_PASS=1234
APP_USER=user1
➜ k -n secret exec secret-pod -- find /tmp/secret1
/tmp/secret1
/tmp/secret1/..data
/tmp/secret1/halt
/tmp/secret1/..2019_12_08_12_15_39.463036797
/tmp/secret1/..2019_12_08_12_15_39.463036797/halt
➜ k -n secret exec secret-pod -- cat /tmp/secret1/halt
#! /bin/sh
### BEGIN INIT INFO
# Provides:          halt
# Required-Start:
# Required-Stop:
# Default-Start:
# Default-Stop:      0
# Short-Description: Execute the halt command.
# Description:
...
================================================================================

Question 20
Task weight: 10%

Use context: kubectl config use-context k8s-c3-CCC

Your coworker said node cluster3-node2 is running an older Kubernetes version and is not even part of the cluster. Update Kubernetes on that node to the exact version that's running on cluster3-controlplane1. Then add this node to the cluster. Use kubeadm for this.
----------------------------------------------------
Upgrade Kubernetes to cluster3-controlplane1 version
Search in the docs for kubeadm upgrade: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade

➜ k get node
NAME                     STATUS   ROLES           AGE   VERSION
cluster3-controlplane1   Ready    control-plane   22h   v1.28.2
cluster3-node1           Ready    <none>          22h   v1.28.2
Controlplane node seems to be running Kubernetes 1.28.2. Node cluster3-node2 might not yet be part of the cluster depending on previous tasks.

➜ ssh cluster3-node2

➜ root@cluster3-node2:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:19:40Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}

➜ root@cluster3-node2:~# kubectl version --short
Flag --short has been deprecated, and will be removed in the future. The --short output will become the default.
Client Version: v1.27.4
Kustomize Version: v5.0.1
The connection to the server localhost:8080 was refused - did you specify the right host or port?

➜ root@cluster3-node2:~# kubelet --version
Kubernetes v1.27.4
Here kubeadm is already installed in the wanted version, so we don't need to install it. Hence we can run:

➜ root@cluster3-node2:~# kubeadm upgrade node
couldn't create a Kubernetes client from file "/etc/kubernetes/kubelet.conf": failed to load admin kubeconfig: open /etc/kubernetes/kubelet.conf: no such file or directory
To see the stack trace of this error execute with --v=5 or higher
This is usually the proper command to upgrade a node. But this error means that this node was never even initialised, so nothing to update here. This will be done later using kubeadm join. For now we can continue with kubelet and kubectl:

➜ root@cluster3-node2:~# apt update
➜ root@cluster3-node2:~# apt show kubectl -a | grep 1.28
...
Version: 1.28.2-00
Version: 1.28.1-00
Version: 1.28.0-00

➜ root@cluster3-node2:~# apt install kubectl=1.28.2-00 kubelet=1.28.2-00
...
➜ root@cluster3-node2:~# service kubelet restart

➜ root@cluster3-node2:~# service kubelet status
➜ ssh cluster3-controlplane1

➜ root@cluster3-controlplane1:~# kubeadm token create --print-join-command
kubeadm join 192.168.100.31:6443 --token lyl4o0.vbkmv9rdph5qd660 --discovery-token-ca-cert-hash sha256:b0c94ccf935e27306ff24bce4b8f611c621509e80075105b3f25d296a94927ce 

➜ root@cluster3-controlplane1:~# kubeadm token list
TOKEN                     TTL         EXPIRES                ...
lyl4o0.vbkmv9rdph5qd660   23h         2023-09-23T14:38:12Z   ...
n4dkqj.hu52l46jfo4he61e   <forever>   <never>                ...
s7cmex.ty1olulkuljju9am   18h         2023-09-23T09:34:20Z   ...
We see the expiration of 23h for our token, we could adjust this by passing the ttl argument.

Next we connect again to cluster3-node2 and simply execute the join command:

➜ ssh cluster3-node2

➜ root@cluster3-node2:~# kubeadm join 192.168.100.31:6443 --token lyl4o0.vbkmv9rdph5qd660 --discovery-token-ca-cert-hash sha256:b0c94ccf935e27306ff24bce4b8f611c621509e80075105b3f25d296a94927ce 


================================================================================

Question 21
Task weight: 2%

Use context: kubectl config use-context k8s-c3-CCC

Create a Static Pod named my-static-pod in Namespace default on cluster3-controlplane1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory.

Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if it's reachable through the cluster3-controlplane1 internal IP address. You can connect to the internal node IPs from your main terminal.
--------------------------------------------
➜ ssh cluster3-controlplane1

➜ root@cluster1-controlplane1:~# cd /etc/kubernetes/manifests/

➜ root@cluster1-controlplane1:~# kubectl run my-static-pod \
    --image=nginx:1.16-alpine \
    -o yaml --dry-run=client > my-static-pod.yaml
Then edit the my-static-pod.yaml to add the requested resource requests:

# /etc/kubernetes/manifests/my-static-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: my-static-pod
  name: my-static-pod
spec:
  containers:
  - image: nginx:1.16-alpine
    name: my-static-pod
    resources:
      requests:
        cpu: 10m
        memory: 20Mi
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
 

And make sure it's running:

➜ k get pod -A | grep my-static
NAMESPACE     NAME                                   READY   STATUS   ...   AGE
default       my-static-pod-cluster3-controlplane1   1/1     Running  ...   22s
Now we expose that static Pod:

k expose pod my-static-pod-cluster3-controlplane1 \
  --name static-pod-service \
  --type=NodePort \
  --port 80

================================================================================

Question 22
Task weight: 2%

Use context: kubectl config use-context k8s-c2-AC

Check how long the kube-apiserver server certificate is valid on cluster2-controlplane1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration.

Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date.

Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh.
--------------------------------------------------
Check how long the kube-apiserver server certificate is valid on cluster2-controlplane1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration.

Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date.

Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh.

 

Answer:
First let's find that certificate:

➜ ssh cluster2-controlplane1

➜ root@cluster2-controlplane1:~# find /etc/kubernetes/pki | grep apiserver
/etc/kubernetes/pki/apiserver.crt
/etc/kubernetes/pki/apiserver-etcd-client.crt
/etc/kubernetes/pki/apiserver-etcd-client.key
/etc/kubernetes/pki/apiserver-kubelet-client.crt
/etc/kubernetes/pki/apiserver.key
/etc/kubernetes/pki/apiserver-kubelet-client.key
Next we use openssl to find out the expiration date:

➜ root@cluster2-controlplane1:~# openssl x509  -noout -text -in /etc/kubernetes/pki/apiserver.crt | grep Validity -A2
        Validity
            Not Before: Dec 20 18:05:20 2022 GMT
            Not After : Dec 20 18:05:20 2023 GMT
There we have it, so we write it in the required location on our main terminal:

# /opt/course/22/expiration
Dec 20 18:05:20 2023 GMT
And we use the feature from kubeadm to get the expiration too:

➜ root@cluster2-controlplane1:~# kubeadm certs check-expiration | grep apiserver
apiserver                Jan 14, 2022 18:49 UTC   363d        ca               no      
apiserver-etcd-client    Jan 14, 2022 18:49 UTC   363d        etcd-ca          no      
apiserver-kubelet-client Jan 14, 2022 18:49 UTC   363d        ca               no 
Looking good. And finally we write the command that would renew all certificates into the requested location:

# /opt/course/22/kubeadm-renew-certs.sh
kubeadm certs renew apiserver
 

================================================================================

Question 23
Task weight: 2%

Use context: kubectl config use-context k8s-c2-AC

Node cluster2-node1 has been added to the cluster using kubeadm and TLS bootstrapping.

Find the "Issuer" and "Extended Key Usage" values of the cluster2-node1:

kubelet client certificate, the one used for outgoing connections to the kube-apiserver.
kubelet server certificate, the one used for incoming connections from the kube-apiserver.
Write the information into file /opt/course/23/certificate-info.txt.

Compare the "Issuer" and "Extended Key Usage" fields of both certificates and make sense of these.
-----------------------------
To find the correct kubelet certificate directory, we can look for the default value of the --cert-dir parameter for the kubelet. For this search for "kubelet" in the Kubernetes docs which will lead to: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet. We can check if another certificate directory has been configured using ps aux or in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.

First we check the kubelet client certificate:

➜ ssh cluster2-node1

➜ root@cluster2-node1:~# openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep Issuer
        Issuer: CN = kubernetes
        
➜ root@cluster2-node1:~# openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep "Extended Key Usage" -A1
            X509v3 Extended Key Usage: 
                TLS Web Client Authentication
Next we check the kubelet server certificate:

➜ root@cluster2-node1:~# openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep Issuer
          Issuer: CN = cluster2-node1-ca@1588186506

➜ root@cluster2-node1:~# openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep "Extended Key Usage" -A1
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication
We see that the server certificate was generated on the worker node itself and the client certificate was issued by the Kubernetes api. The "Extended Key Usage" also shows if it's for client or server authentication.

================================================================================

Question 24
Task weight: 9%

Use context: kubectl config use-context k8s-c1-H

There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod.

To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to:

connect to db1-* Pods on port 1111
connect to db2-* Pods on port 2222
Use the app label of Pods in your policy.

After implementation, connections from backend-* Pods to vault-* Pods on port 3333 should for example no longer work.
-----------------------------------------------------
➜ k -n project-snake get pod
NAME        READY   STATUS    RESTARTS   AGE
backend-0   1/1     Running   0          8s
db1-0       1/1     Running   0          8s
db2-0       1/1     Running   0          10s
vault-0     1/1     Running   0          10s

➜ k -n project-snake get pod -L app
NAME        READY   STATUS    RESTARTS   AGE     APP
backend-0   1/1     Running   0          3m15s   backend
db1-0       1/1     Running   0          3m15s   db1
db2-0       1/1     Running   0          3m17s   db2
vault-0     1/1     Running   0          3m17s   vault
We test the current connection situation and see nothing is restricted:

➜ k -n project-snake get pod -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP          ...
backend-0   1/1     Running   0          4m14s   10.44.0.24  ...
db1-0       1/1     Running   0          4m14s   10.44.0.25  ...
db2-0       1/1     Running   0          4m16s   10.44.0.23  ...
vault-0     1/1     Running   0          4m16s   10.44.0.22  ...

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.25:1111
database one

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.23:2222
database two

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.22:3333
vault secret storage
Now we create the NP by copying and chaning an example from the k8s docs:

vim 24_np.yaml
# 24_np.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-backend
  namespace: project-snake
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Egress                    # policy is only about Egress
  egress:
    -                           # first rule
      to:                           # first condition "to"
      - podSelector:
          matchLabels:
            app: db1
      ports:                        # second condition "port"
      - protocol: TCP
        port: 1111
    -                           # second rule
      to:                           # first condition "to"
      - podSelector:
          matchLabels:
            app: db2
      ports:                        # second condition "port"
      - protocol: TCP
        port: 2222
The NP above has two rules with two conditions each, it can be read as:

allow outgoing traffic if:
  (destination pod has label app=db1 AND port is 1111)
  OR
  (destination pod has label app=db2 AND port is 2222)
 

Wrong example
Now let's shortly look at a wrong example:

# WRONG
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-backend
  namespace: project-snake
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Egress
  egress:
    -                           # first rule
      to:                           # first condition "to"
      - podSelector:                    # first "to" possibility
          matchLabels:
            app: db1
      - podSelector:                    # second "to" possibility
          matchLabels:
            app: db2
      ports:                        # second condition "ports"
      - protocol: TCP                   # first "ports" possibility
        port: 1111
      - protocol: TCP                   # second "ports" possibility
        port: 2222
The NP above has one rule with two conditions and two condition-entries each, it can be read as:

allow outgoing traffic if:
  (destination pod has label app=db1 OR destination pod has label app=db2)
  AND
  (destination port is 1111 OR destination port is 2222)
Using this NP it would still be possible for backend-* Pods to connect to db2-* Pods on port 1111 for example which should be forbidden.

 

Create NetworkPolicy
We create the correct NP:

k -f 24_np.yaml create
And test again:

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.25:1111
database one

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.23:2222
database two

➜ k -n project-snake exec backend-0 -- curl -s 10.44.0.22:3333
^C
Also helpful to use kubectl describe on the NP to see how k8s has interpreted the policy.

Great, looking more secure. Task done.

 

 

================================================================================

Question 25
Task weight: 8%

Use context: kubectl config use-context k8s-c3-CCC

Make a backup of etcd running on cluster3-controlplane1 and save it on the controlplane node at /tmp/etcd-backup.db.

Then create any kind of Pod in the cluster.

Finally restore the backup, confirm the cluster is still working and that the created Pod is no longer with us.
---------------------------------------------------------
Etcd Backup
First we log into the controlplane and try to create a snapshop of etcd:

➜ ssh cluster3-controlplane1

➜ root@cluster3-controlplane1:~# ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db
Error:  rpc error: code = Unavailable desc = transport is closing
But it fails because we need to authenticate ourselves. For the necessary information we can check the etc manifest:

➜ root@cluster3-controlplane1:~# vim /etc/kubernetes/manifests/etcd.yaml
We only check the etcd.yaml for necessary information we don't change it.

# /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.100.31:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt                           # use
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://192.168.100.31:2380
    - --initial-cluster=cluster3-controlplane1=https://192.168.100.31:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key                            # use
    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.100.31:2379   # use
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.168.100.31:2380
    - --name=cluster3-controlplane1
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt                    # use
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: k8s.gcr.io/etcd:3.3.15-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: etcd
    resources: {}
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd                                                     # important
      type: DirectoryOrCreate
    name: etcd-data
status: {}
But we also know that the api-server is connecting to etcd, so we can check how its manifest is configured:

➜ root@cluster3-controlplane1:~# cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
We use the authentication information and pass it to etcdctl:

➜ root@cluster3-controlplane1:~# ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key

Snapshot saved at /tmp/etcd-backup.db
 

NOTE: Dont use snapshot status because it can alter the snapshot file and render it invalid

 

Etcd restore
Now create a Pod in the cluster and wait for it to be running:

➜ root@cluster3-controlplane1:~# kubectl run test --image=nginx
pod/test created

➜ root@cluster3-controlplane1:~# kubectl get pod -l run=test -w
NAME   READY   STATUS    RESTARTS   AGE
test   1/1     Running   0          60s
 

NOTE: If you didn't solve questions 18 or 20 and cluster3 doesn't have a ready worker node then the created pod might stay in a Pending state. This is still ok for this task.

 

Next we stop all controlplane components:

root@cluster3-controlplane1:~# cd /etc/kubernetes/manifests/

root@cluster3-controlplane1:/etc/kubernetes/manifests# mv * ..

root@cluster3-controlplane1:/etc/kubernetes/manifests# watch crictl ps
Now we restore the snapshot into a specific directory:

➜ root@cluster3-controlplane1:~# ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \
--data-dir /var/lib/etcd-backup \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key

2020-09-04 16:50:19.650804 I | mvcc: restore compact to 9935
2020-09-04 16:50:19.659095 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
We could specify another host to make the backup from by using etcdctl --endpoints http://IP, but here we just use the default value which is: http://127.0.0.1:2379,http://127.0.0.1:4001.

The restored files are located at the new folder /var/lib/etcd-backup, now we have to tell etcd to use that directory:

➜ root@cluster3-controlplane1:~# vim /etc/kubernetes/etcd.yaml
# /etc/kubernetes/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
...
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-backup                # change
      type: DirectoryOrCreate
    name: etcd-data
status: {}
Now we move all controlplane yaml again into the manifest directory. Give it some time (up to several minutes) for etcd to restart and for the api-server to be reachable again:

root@cluster3-controlplane1:/etc/kubernetes/manifests# mv ../*.yaml .

root@cluster3-controlplane1:/etc/kubernetes/manifests# watch crictl ps
Then we check again for the Pod:

➜ root@cluster3-controlplane1:~# kubectl get pod -l run=test
No resources found in default namespace.
Awesome, backup and restore worked as our pod is gone.

================================================================================

Extra Question 1
Extra Question 1
Use context: kubectl config use-context k8s-c1-H

Check all available Pods in the Namespace project-c13 and find the names of those that would probably be terminated first if the Nodes run out of resources (cpu or memory) to schedule all Pods. Write the Pod names into /opt/course/e1/pods-not-stable.txt.
--------------------------------------
When available cpu or memory resources on the nodes reach their limit, Kubernetes will look for Pods that are using more resources than they requested. These will be the first candidates for termination. If some Pods containers have no resource requests/limits set, then by default those are considered to use more than requested.

Kubernetes assigns Quality of Service classes to Pods based on the defined resources and limits, read more here: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod

Hence we should look for Pods without resource requests defined, we can do this with a manual approach:

k -n project-c13 describe pod | less -p Requests # describe all pods and highlight Requests
Or we do:

k -n project-c13 describe pod | egrep "^(Name:|    Requests:)" -A1
We see that the Pods of Deployment c13-3cc-runner-heavy don't have any resources requests specified. Hence our answer would be:

# /opt/course/e1/pods-not-stable.txt
c13-3cc-runner-heavy-65588d7d6-djtv9map
c13-3cc-runner-heavy-65588d7d6-v8kf5map
c13-3cc-runner-heavy-65588d7d6-wwpb4map
o3db-0
o3db-1 # maybe not existing if already removed via previous scenario 
To automate this process you could use jsonpath like this:

➜ k -n project-c13 get pod \
  -o jsonpath="{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\n'}"
  Or we look for the Quality of Service classes:

➜ k get pods -n project-c13 \
  -o jsonpath="{range .items[*]}{.metadata.name} {.status.qosClass}{'\n'}"
Here we see three with BestEffort, which Pods get that don't have any memory or cpu limits or requests defined.

A good practice is to always set resource requests and limits. If you don't know the values your containers should have you can find this out using metric tools like Prometheus. You can also use kubectl top pod or even kubectl exec into the container and use top and similar tools.

 

================================================================================

Extra Question 2
Extra Question 2
Use context: kubectl config use-context k8s-c1-H

There is an existing ServiceAccount secret-reader in Namespace project-hamster. Create a Pod of image curlimages/curl:7.65.3 named tmp-api-contact which uses this ServiceAccount. Make sure the container keeps running.

Exec into the Pod and use curl to access the Kubernetes Api of that cluster manually, listing all available secrets. You can ignore insecure https connection. Write the command(s) for this into file /opt/course/e4/list-secrets.sh.

------------------------------------------------------------
k run tmp-api-contact \
  --image=curlimages/curl:7.65.3 $do \
  --command > e2.yaml -- sh -c 'sleep 1d'

vim e2.yaml
Add the service account name and Namespace:

# e2.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: tmp-api-contact
  name: tmp-api-contact
  namespace: project-hamster          # add
spec:
  serviceAccountName: secret-reader   # add
  containers:
  - command:
    - sh
    - -c
    - sleep 1d
    image: curlimages/curl:7.65.3
    name: tmp-api-contact
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
Then run and exec into:

k -f 6.yaml create

k -n project-hamster exec tmp-api-contact -it -- sh
Once on the container we can try to connect to the api using curl, the api is usually available via the Service named kubernetes in Namespace default (You should know how dns resolution works across Namespaces.). Else we can find the endpoint IP via environment variables running env.

So now we can do:

curl https://kubernetes.default
curl -k https://kubernetes.default # ignore insecure as allowed in ticket description
curl -k https://kubernetes.default/api/v1/secrets # should show Forbidden 403
The last command shows 403 forbidden, this is because we are not passing any authorisation information with us. The Kubernetes Api Server thinks we are connecting as system:anonymous. We want to change this and connect using the Pods ServiceAccount named secret-reader.

We find the the token in the mounted folder at /var/run/secrets/kubernetes.io/serviceaccount, so we do:

➜ TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
➜ curl -k https://kubernetes.default/api/v1/secrets -H "Authorization: Bearer ${TOKEN}"

Now we're able to list all Secrets, registering as the ServiceAccount secret-reader under which our Pod is running.

To use encrypted https connection we can run:

CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
curl --cacert ${CACERT} https://kubernetes.default/api/v1/secrets -H "Authorization: Bearer ${TOKEN}"
For troubleshooting we could also check if the ServiceAccount is actually able to list Secrets using:

➜ k auth can-i get secret --as system:serviceaccount:project-hamster:secret-reader
yes
Finally write the commands into the requested location:

# /opt/course/e4/list-secrets.sh
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -k https://kubernetes.default/api/v1/secrets -H "Authorization: Bearer ${TOKEN}"
================================================================================
Preview Question 1
Preview Question 1
Use context: kubectl config use-context k8s-c2-AC

The cluster admin asked you to find out the following information about etcd running on cluster2-controlplane1:

Server private key location
Server certificate expiration date
Is client certificate authentication enabled
Write these information into /opt/course/p1/etcd-info.txt

Finally you're asked to save an etcd snapshot at /etc/etcd-snapshot.db on cluster2-controlplane1 and display its status.


================================================================================
Preview Question 2
Preview Question 2
Use context: kubectl config use-context k8s-c1-H

You're asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster:

Create a new Pod named p2-pod with two containers, one of image nginx:1.21.3-alpine and one of image busybox:1.31. Make sure the busybox container keeps running for some time.

Create a new Service named p2-service which exposes that Pod internally in the cluster on port 3000->80.

Find the kube-proxy container on all nodes cluster1-controlplane1, cluster1-node1 and cluster1-node2 and make sure that it's using iptables. Use command crictl for this.

Write the iptables rules of all nodes belonging the created Service p2-service into file /opt/course/p2/iptables.txt.

Finally delete the Service and confirm that the iptables rules are gone from all nodes.
--------------------------------------------------------
Find out etcd information
Let's check the nodes:

➜ k get node
NAME                     STATUS   ROLES           AGE    VERSION
cluster2-controlplane1   Ready    control-plane   89m   v1.28.2
cluster2-node1           Ready    <none>          87m   v1.28.2

➜ ssh cluster2-controlplane1
First we check how etcd is setup in this cluster:

➜ root@cluster2-controlplane1:~# kubectl -n kube-system get pod

➜ root@cluster2-controlplane1:~# vim /etc/kubernetes/manifests/etcd.yaml
So we look at the yaml and the parameters with which etcd is started:

# /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.102.11:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt              # server certificate
    - --client-cert-auth=true                                      # enabled
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://192.168.102.11:2380
    - --initial-cluster=cluster2-controlplane1=https://192.168.102.11:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key               # server private key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.102.11:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.168.102.11:2380
    - --name=cluster2-controlplane1
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
...
We see that client authentication is enabled and also the requested path to the server private key, now let's find out the expiration of the server certificate:

➜ root@cluster2-controlplane1:~# openssl x509  -noout -text -in /etc/kubernetes/pki/etcd/server.crt | grep Validity -A2
        Validity
            Not Before: Sep 13 13:01:31 2021 GMT
            Not After : Sep 13 13:01:31 2022 GMT
There we have it. Let's write the information into the requested file:

# /opt/course/p1/etcd-info.txt
Server private key location: /etc/kubernetes/pki/etcd/server.key
Server certificate expiration date: Sep 13 13:01:31 2022 GMT
Is client certificate authentication enabled: yes
 

Create etcd snapshot
First we try:

ETCDCTL_API=3 etcdctl snapshot save /etc/etcd-snapshot.db
We get the endpoint also from the yaml. But we need to specify more parameters, all of which we can find the yaml declaration above:

ETCDCTL_API=3 etcdctl snapshot save /etc/etcd-snapshot.db \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key
This worked. Now we can output the status of the backup file:

➜ root@cluster2-controlplane1:~# ETCDCTL_API=3 etcdctl snapshot status /etc/etcd-snapshot.db
4d4e953, 7213, 1291, 2.7 MB
The status shows:

Hash: 4d4e953
Revision: 7213
Total Keys: 1291
Total Size: 2.7 MB
 

 


================================================================================
Preview Question 3
Preview Question 3
Use context: kubectl config use-context k8s-c2-AC

Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember/output the IP of that Service.

Change the Service CIDR to 11.96.0.0/12 for the cluster.

Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed.

Let's create the Pod and expose it:

k run check-ip --image=httpd:2.4.41-alpine

k expose pod check-ip --name check-ip-service --port 80
And check the Pod and Service ips:

➜ k get svc,ep -l run=check-ip
NAME                       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/check-ip-service   ClusterIP   10.104.3.45   <none>        80/TCP    8s

NAME                         ENDPOINTS      AGE
endpoints/check-ip-service   10.44.0.3:80   7s
Now we change the Service CIDR on the kube-apiserver:

➜ ssh cluster2-controlplane1

➜ root@cluster2-controlplane1:~# vim /etc/kubernetes/manifests/kube-apiserver.yaml
# /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.100.21
...
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-cluster-ip-range=11.96.0.0/12             # change
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
...
Give it a bit for the kube-apiserver and controller-manager to restart

Wait for the api to be up again:

➜ root@cluster2-controlplane1:~# kubectl -n kube-system get pod | grep api
kube-apiserver-cluster2-controlplane1            1/1     Running   0              49s
 

 

Now we do the same for the controller manager:

➜ root@cluster2-controlplane1:~# vim /etc/kubernetes/manifests/kube-controller-manager.yaml
# /etc/kubernetes/manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --node-cidr-mask-size=24
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=11.96.0.0/12         # change
    - --use-service-account-credentials=true
Give it a bit for the scheduler to restart.

We can check if it was restarted using crictl:

➜ root@cluster2-controlplane1:~# crictl ps | grep scheduler
3d258934b9fd6    aca5ededae9c8    About a minute ago   Running    kube-scheduler ...
 

 

Checking our existing Pod and Service again:

➜ k get pod,svc -l run=check-ip
NAME           READY   STATUS    RESTARTS   AGE
pod/check-ip   1/1     Running   0          21m

NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/check-ip-service   ClusterIP   10.99.32.177   <none>        80/TCP    21m
Nothing changed so far. Now we create another Service like before:

k expose pod check-ip --name check-ip-service2 --port 80
And check again:

➜ k get svc,ep -l run=check-ip
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/check-ip-service    ClusterIP   10.109.222.111   <none>        80/TCP    8m
service/check-ip-service2   ClusterIP   11.111.108.194   <none>        80/TCP    6m32s

NAME                          ENDPOINTS      AGE
endpoints/check-ip-service    10.44.0.1:80   8m
endpoints/check-ip-service2   10.44.0.1:80   6m13s
There we go, the new Service got an ip of the new specified range assigned. We also see that both Services have our Pod as endpoint.

 =================================================================================







 CKA Tips Kubernetes 1.28
In this section we'll provide some tips on how to handle the CKA exam and browser terminal.

 

Knowledge
Study all topics as proposed in the curriculum till you feel comfortable with all.

 

General

Study all topics as proposed in the curriculum till you feel comfortable with all
Do 1 or 2 test session with this CKA Simulator. Understand the solutions and maybe try out other ways to achieve the same thing.
Setup your aliases, be fast and breath kubectl
The majority of tasks in the CKA will also be around creating Kubernetes resources, like it's tested in the CKAD. So preparing a bit for the CKAD can't hurt.
Learn and Study the in-browser scenarios on https://killercoda.com/killer-shell-cka (and maybe for CKAD https://killercoda.com/killer-shell-ckad)
Imagine and create your own scenarios to solve
 

Components

Understanding Kubernetes components and being able to fix and investigate clusters: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster
Know advanced scheduling: https://kubernetes.io/docs/concepts/scheduling/kube-scheduler
When you have to fix a component (like kubelet) in one cluster, just check how it's setup on another node in the same or even another cluster. You can copy config files over etc
If you like you can look at Kubernetes The Hard Way once. But it's NOT necessary to do, the CKA is not that complex. But KTHW helps understanding the concepts
You should install your own cluster using kubeadm (one controlplane, one worker) in a VM or using a cloud provider and investigate the components
Know how to use Kubeadm to for example add nodes to a cluster
Know how to create an Ingress resources
Know how to snapshot/restore ETCD from another machine
 

 

CKA Preparation
Read the Curriculum

https://github.com/cncf/curriculum

Read the Handbook

https://docs.linuxfoundation.org/tc-docs/certification/lf-handbook2

Read the important tips

https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad

Read the FAQ

https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad

 

Kubernetes documentation
Get familiar with the Kubernetes documentation and be able to use the search. Allowed links are:

https://kubernetes.io/docs
https://kubernetes.io/blog
https://helm.sh/docs
NOTE: Verify the list here

 

The Test Environment / Browser Terminal
You'll be provided with a browser terminal which uses Ubuntu 20. The standard shells included with a minimal install of Ubuntu 20 will be available, including bash.

Laggin

There could be some lagging, definitely make sure you are using a good internet connection because your webcam and screen are uploading all the time.

Kubectl autocompletion and commands

Autocompletion is configured by default, as well as the k alias source and others:

kubectl with k alias and Bash autocompletion

yq and jqfor YAML/JSON processing

tmux for terminal multiplexing

curl and wget for testing web services

man and man pages for further documentation

Copy & Paste

There could be issues copying text (like pod names) from the left task information into the terminal. Some suggested to "hard" hit or long hold Cmd/Ctrl+C a few times to take action. Apart from that copy and paste should just work like in normal terminals.

Percentages and Score

There are 15-20 questions in the exam and 100% of total percentage to reach. Each questions shows the % it gives if you solve it. Your results will be automatically checked according to the handbook. If you don't agree with the results you can request a review by contacting the Linux Foundation support.

Notepad & Skipping Questions

You have access to a simple notepad in the browser which can be used for storing any kind of plain text. It makes sense to use this for saving skipped question numbers and their percentages. This way it's possible to move some questions to the end. It might make sense to skip 2% or 3% questions and go directly to higher ones.

Contexts

You'll receive access to various different clusters and resources in each. They provide you the exact command you need to run to connect to another cluster/context. But you should be comfortable working in different namespaces with kubectl.

 

PSI Bridge
Starting with PSI Bridge:

The exam will now be taken using the PSI Secure Browser, which can be downloaded using the newest versions of Microsoft Edge, Safari, Chrome, or Firefox
Multiple monitors will no longer be permitted
Use of personal bookmarks will no longer be permitted
The new ExamUI includes improved features such as:

A remote desktop configured with the tools and software needed to complete the tasks
A timer that displays the actual time remaining (in minutes) and provides an alert with 30, 15, or 5 minute remaining
The content panel remains the same (presented on the Left Hand Side of the ExamUI)
Read more here.

 

Browser Terminal Setup
It should be considered to spend ~1 minute in the beginning to setup your terminal. In the real exam the vast majority of questions will be done from the main terminal. For few you might need to ssh into another machine. Just be aware that configurations to your shell will not be transferred in this case.

Minimal Setup
Alias

The alias k for kubectl will already be configured together with autocompletion. In case not you can configure it using this link.

Vim

The following settings will already be configured in your real exam environment in ~/.vimrc. But it can never hurt to be able to type these down:

set tabstop=2
set expandtab
set shiftwidth=2
The expandtab make sure to use spaces for tabs. Memorize these and just type them down. You can't have any written notes with commands on your desktop etc.

Optional Setup
Fast dry-run output

export do="--dry-run=client -o yaml"
This way you can just run k run pod1 --image=nginx $do. Short for "dry output", but use whatever name you like.

Fast pod delete

export now="--force --grace-period 0"
This way you can run k delete pod1 $now and don't have to wait for ~30 seconds termination time.

Persist bash settings

You can store aliases and other setup in ~/.bashrc if you're planning on using different shells or tmux.

Alias Namespace

In addition you could define an alias like:

alias kn='kubectl config set-context --current --namespace '
Which allows you to define the default namespace of the current context. Then once you switch a context or namespace you can just run:

kn default        # set default to default
kn my-namespace   # set default to my-namespace
But only do this if you used it before and are comfortable doing so. Else you need to specify the namespace for every call, which is also fine:

k -n my-namespace get all
k -n my-namespace get pod
...
 

Be fast
Use the history command to reuse already entered commands or use even faster history search through Ctrl r .

If a command takes some time to execute, like sometimes kubectl delete pod x. You can put a task in the background using Ctrl z and pull it back into foreground running command fg.

You can delete pods fast with:

k delete pod x --grace-period 0 --force

k delete pod x $now # if export from above is configured
 

Vim
Be great with vim.

toggle vim line numbers

When in vim you can press Esc and type :set number or :set nonumber followed by Enter to toggle line numbers. This can be useful when finding syntax errors based on line - but can be bad when wanting to mark&copy by mouse. You can also just jump to a line number with Esc :22 + Enter.

copy&paste

Get used to copy/paste/cut with vim:

Mark lines: Esc+V (then arrow keys)
Copy marked lines: y
Cut marked lines: d
Past lines: p or P
Indent multiple lines

To indent multiple lines press Esc and type :set shiftwidth=2. First mark multiple lines using Shift v and the up/down keys. Then to indent the marked lines press > or <. You can then press . to repeat the action.

 

Split terminal screen
By default tmux is installed and can be used to split your one terminal into multiple. But just do this if you know your shit, because scrolling is different and copy&pasting might be weird.