

# VOLUMES
# SECRETS
# CONFIGMAPS




## emptyDir: {}
----------------------------------------------pod
|   container1      container2      container3
|       |               |               |
|       |               |               |
|       |               |               |
|       --------------demo-volume--------    
|
-----------------------------------------------
~~~yml
apiVersion: v1
kind: Pod
metadata: 
  name: volume-pod
spec:
  containers:
  - image: centos
    imagePullPolicy: IfNotPresent
    name: volume-container-1
    command: ['sh', '-c', 'echo The Bench Container 1 is Running ; sleep 3600']
    volumeMounts:
    - mountPath: /demo1
      name: demo-volume

  - image: ubuntu
    imagePullPolicy: IfNotPresent
    name: volume-container-2
    command: ['/bin/bash', '-c', 'echo The Bench Container 2 is Running ; sleep 3600']
    volumeMounts:
    - mountPath: /demo2
      name: demo-volume

  - image: alpine
    imagePullPolicy: IfNotPresent
    name: volume-container-3
    command: ['sh', '-c', 'echo The Bench Container 3 is Running ; sleep 3600']
    volumeMounts:
    - mountPath: /demo3
      name: demo-volume
  volumes:
  - name: demo-volume
    emptyDir: {}
~~~
~~~bash 
  

devops@T480:~/k8/pods$ kubectl exec -it volume-pod -c volume-container-1 -- bash
[root@volume-pod /]# ll
bash: ll: command not found
[root@volume-pod /]# ls
bin    dev  home  lib64       media  opt   root  sbin  sys  usr
demo1  etc  lib   lost+found  mnt    proc  run   srv   tmp  var
[root@volume-pod /]# df -h
Filesystem      Size  Used Avail Use% Mounted on
overlay        1007G  3.9G  952G   1% /
tmpfs            64M     0   64M   0% /dev
tmpfs           7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/sde       1007G  3.9G  952G   1% /demo1
shm              64M     0   64M   0% /dev/shm
tmpfs            16G   12K   16G   1% /run/secrets/kubernetes.io/serviceaccount        
tmpfs           7.8G     0  7.8G   0% /proc/acpi
tmpfs           7.8G     0  7.8G   0% /sys/firmware
[root@volume-pod /]# cd demo1
[root@volume-pod demo1]# touch c1file.txt
[root@volume-pod demo1]# ls
c1file.txt
[root@volume-pod demo1]# exit
devops@T480:~/k8/pods$ kubectl exec -it volume-pod -c volume-container-2 -- bash       
root@volume-pod:/# ls
bin   demo2  etc   lib    lib64   media  opt   root  sbin  sys  usr
boot  dev    home  lib32  libx32  mnt    proc  run   srv   tmp  var
root@volume-pod:/# cd demo2/
root@volume-pod:/demo2# ls
c1file.txt
root@volume-pod:/demo2# touch c2file.txt
root@volume-pod:/demo2# ls
c1file.txt  c2file.txt
root@volume-pod:/demo2# ls -alh
total 8.0K
drwxrwxrwx 2 root root 4.0K Dec 12 16:45 .
drwxr-xr-x 1 root root 4.0K Dec 12 16:38 ..
-rw-r--r-- 1 root root    0 Dec 12 16:44 c1file.txt
-rw-r--r-- 1 root root    0 Dec 12 16:45 c2file.txt
root@volume-pod:/demo2#
exit
devops@T480:~/k8/pods$ kubectl exec -it volume-pod -c volume-container-3 -- bash       
OCI runtime exec failed: exec failed: unable to start container process: exec: "bash": executable file not found in $PATH: unknown
command terminated with exit code 126
devops@T480:~/k8/pods$ kubectl exec -it volume-pod -c volume-container-3 -- sh
/ # cd demo3/
/demo3 # ls
c1file.txt  c2file.txt
/demo3 # touch c3file.txt
/demo3 #

~~~
-----------------------------------------------------------------------
if you want to store your logs on your host where container is running
you use "hostPath" . it is persistant

## hostPath
  <!-- Warning:
  HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly.

  If restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts MUST be required to use readOnly mounts for the policy to be effective. -->
----------------------------------------------pod
|   container1      container2      container3
|       |               |               |
|       |               |               |
-----------------------------------------------
        |               |               |
        --------------demo-volume-------- persistant   

apiVersion: v1
kind: Pod
metadata:
  name: volume-pod-hp
spec:
  containers:
  - image: tomcat
    name: volume-container-1
    volumeMounts:
    - mountPath: /usr/local/tomcat/logs
      name: hostpath-volume
  volumes:
  - name: hostpath-volume
    hostPath:
   >   path: /run/desktop/mnt/host/e/logs   #because of wsl 
      type: Directory



but there is a problem, if there are 1000 nodes , you cannot go and create 1000 /tmp/logs folder on each node for persistance. moreover, if a node fails and pod get recreated on another node, what will happen to persistant storage. 

SOLUTION:
------------------------------------------------------------------------------
## PV,PVC
1 pod = 1 pvc
1 deployment = 1 pvc

even if you dont have path defined on a node, kubernetes will create it on your behalf, because PV,PVC are kubernetes Objects. it takes care of them

> persistentVolumeClaim 
A persistentVolumeClaim volume is used to mount a PersistentVolume into a Pod. PersistentVolumeClaims are a way for users to "claim" durable storage (such as an iSCSI volume) without knowing the details of the particular cloud environment.

first you create PV, then PVC, then Pod
Pod ->Persistant Volume Claim -> Storage Class -> Persistant Volume
e.g. your request of booking->receptionist->your hotel room
                                                1GB
                                                2GB
                                                5GB
if your request is 2GB, and its not available, then SC will assign you 5GB until 2GB becomes available

> STEP 1:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  storageClassName: standard
  persistentVolumeReclaimPolicy: Recycle ## Retain / Delete
  capacity:
    storage: 1000Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/pvdata





> STEP 2:

~~~yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi

devops@T480:~/k8/pods$ kubectl create -f volumePVC.yml 
persistentvolumeclaim/my-pvc created
devops@T480:~/k8/pods$ kubectl get pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pvc   Bound    my-pv    100Mi      RWO            standard       8s
devops@T480:~/k8/pods$ kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
my-pv   100Mi      RWO            Recycle          Bound    default/my-pvc   standard     


> PersistentVolumeClaims 
Each PVC contains a spec and status, which is the specification and status of the claim. The name of a PersistentVolumeClaim object must be a valid DNS subdomain name.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}

> Claims As Volumes 
Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.
# Volume Snapshot and Restore Volume from Snapshot Support
# FEATURE STATE: Kubernetes v1.20 [stable]
# Volume snapshots only support the out-of-tree CSI volume plugins. For details, see Volume Snapshots. In-tree volume plugins are deprecated. You can read about the deprecated volume plugins in the Volume Plugin FAQ.

# Create a PersistentVolumeClaim from a Volume Snapshot
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: restore-pvc
# spec:
#   storageClassName: csi-hostpath-sc
#   dataSource:
#     name: new-snapshot-test
#     kind: VolumeSnapshot
#     apiGroup: snapshot.storage.k8s.io
#   accessModes:
#     - ReadWriteOnce
#   resources:
#     requests:
#       storage: 10Gi
# Volume Cloning
# Volume Cloning only available for CSI volume plugins.

# Create PersistentVolumeClaim from an existing PVC
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: cloned-pvc
# spec:
#   storageClassName: my-csi-plugin
#   dataSource:
#     name: existing-src-pvc-name
#     kind: PersistentVolumeClaim
#   accessModes:
#     - ReadWriteOnce
#   resources:
#     requests:
#       storage: 10Gi


> STEP 3:
claim PVC by creating a POD

apiVersion: v1
kind: Pod
metadata:
  name: my-pvc-pod
spec:
  containers:
  - image: tomcat
    name: my-pvc-pod-container
    volumeMounts:
    - name: pv
      mountPath: "/usr/local/tomcat/logs"
  
  volumes:
  - name: pv
    persistentVolumeClaim:
      claimName: my-pvc
~~~


# CONCEPTs:
> Class: A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class.
> A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class


> Access modes
   # ReadWriteOnce – the volume can be mounted as read-write by a single node ... RWO
   # ReadOnlyMany – the volume can be mounted read-only by many nodes ... ROX
   # ReadWriteMany – the volume can be mounted as read-write by many nodes ... RWX
   # ReadWriteOncePod
    FEATURE STATE: Kubernetes v1.29 [stable]
    the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it.


> The Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim bec ause the previous claimant's data remains on the volume


> Class 
A PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class.

In the past, the annotation volume.beta.kubernetes.io/storage-class was used instead of the storageClassName attribute. This annotation is still working; however, it will become fully deprecated in a future Kubernetes release.

> Reclaim Policy
Current reclaim policies are:

Retain -- manual reclamation
Recycle -- basic scrub (rm -rf /thevolume/*)
Delete -- delete the volume
For Kubernetes 1.29, only nfs and hostPath volume types support recycling.


> Node Affinity 
Note: For most volume types, you do not need to set this field. You need to explicitly set this for local volumes.
A PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity. To specify node affinity, set nodeAffinity in the .spec of a PV.

## PV<PVC LIFECYCLE>
    
    Provisioning 
    There are two ways PVs may be provisioned: statically or dynamically.

Static
    A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.

Dynamic
    When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class "" effectively disable dynamic provisioning for themselves.
Binding 
    A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.
Using 
    Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

    Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block.
Storage Object in Use Protection 
    The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.

    Note: PVC is in active use by a Pod when a Pod object exists that is using the PVC.
> Reclaiming 
    When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.

1) Retain
    The Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.

    1- Delete the PersistentVolume. The associated storage asset in external infrastructure still exists after the PV is deleted.
    2- Manually clean up the data on the associated storage asset accordingly.
    3- Manually delete the associated storage asset.
    4- If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.
2) Delete
    For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure. Volumes that were dynamically provisioned inherit the reclaim policy of their StorageClass, which defaults to Delete. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. 
3) Recycle 
    Warning: The Recycle reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.
    If supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.

    However, an administrator can configure a custom recycler Pod template using the Kubernetes controller manager command line arguments as described in the reference. The custom recycler Pod template must contain a volumes specification, as shown in the example below:

<!-- apiVersion: v1
kind: Pod
metadata:
  name: pv-recycler
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    hostPath:
      path: /any/path/it/will/be/replaced
  containers:
  - name: pv-recycler
    image: "registry.k8s.io/busybox"
    command: ["/bin/sh", "-c", "test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \"$(ls -A /scrub)\" || exit 1"]
    volumeMounts:
    - name: vol
      mountPath: /scrub -->

# Types of Persistent Volumes 
PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:

csi - Container Storage Interface (CSI)
fc - Fibre Channel (FC) storage
hostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)
iscsi - iSCSI (SCSI over IP) storage
local - local storage devices mounted on nodes.
nfs - Network File System (NFS) storage
# Volume Mode
FEATURE STATE: Kubernetes v1.18 [stable]
Kubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block.

volumeMode is an optional API parameter. Filesystem is the default mode used when volumeMode parameter is omitted.

A volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volume is backed by a block device and the device is empty, Kubernetes creates a filesystem on the device before mounting it for the first time.

You can set the value of volumeMode to Block to use a volume as a raw block device. Such volume is presented into a Pod as a block device, without any filesystem on it. This mode is useful to provide a Pod the fastest possible way to access a volume, without any filesystem layer between the Pod and the volume. On the other hand, the application running in the Pod must know how to handle a raw block device.
---------------------------------------------------------------
# SECRETS
> Warning: Any containers that run with privileged: true on a node can access all Secrets used on that node.
> Using Secrets with static Pods
You cannot use ConfigMaps or Secrets with static Pods.
> 2 ways to use secrets
Using Secrets as environment variables
    For each container in your Pod specification, add an environment variable for each Secret key that you want to use to the env[].valueFrom.secretKeyRef field.
Using Secrets as files from a Pod
    If you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes make the value of that Secret be available as a file inside the filesystem of one or more of the Pod's containers.
> Optiional is possible
When you reference a Secret in a Pod, you can mark the Secret as optional, such as in the following example. If an optional Secret doesn't exist, Kubernetes ignores it.
 volumes:
  - name: foo
    secret:
      secretName: mysecret
      optional: true
> Editing a Secret
You can edit an existing Secret unless it is immutable. To edit a Secret, use one of the following methods:
Use kubectl
Use a configuration file
You can also edit the data in a Secret using the Kustomize tool. However, this method creates a new Secret object with the edited data.
> Size limit 
Individual Secrets are limited to 1MiB in size. This is to discourage creation of very large Secrets that could exhaust the API server and kubelet memory.
> Types of Secret
Kubernetes provides several built-in types for some common usage scenarios. These types vary in terms of the validations performed and the constraints Kubernetes imposes on them.

Built-in Type	Usage
Opaque	                           arbitrary user-defined data
kubernet.io/service-account-token	 ServiceAccount token
kubernetes.io/dockercfg	           serialized ~/.dockercfg file
kubernetes.io/dockerconfigjson	   serialized ~/.docker/config.json 
kubernetes.io/basic-auth	      credentials for basic authentication
kubernetes.io/ssh-auth	        credentials for SSH authentication
kubernetes.io/tls	              data for a TLS client or server
bootstrap.kubernetes.io/token	  bootstrap token data

1) Opaque:
You can define and use your own Secret type by assigning a non-empty string as the type value for a Secret object (an empty string is treated as an Opaque type).

2) ServiceAccount token Secrets:
A kubernetes.io/service-account-token type of Secret is used to store a token credential that identifies a ServiceAccount. This is a legacy mechanism that provides long-lived ServiceAccount credentials to Pods.

  <!-- In Kubernetes v1.22 and later, the recommended approach is to obtain a short-lived, automatically rotating ServiceAccount token by using the TokenRequest API instead. You can get these short-lived tokens using the following methods:

  Call the TokenRequest API either directly or by using an API client like kubectl. For example, you can use the kubectl create token command.
  Request a mounted token in a projected volume in your Pod manifest. Kubernetes creates the token and mounts it in the Pod. The token is automatically invalidated when the Pod that it's mounted in is deleted. For details, see Launch a Pod using service account token projection.
  Note: You should only create a ServiceAccount token Secret if you can't use the TokenRequest API to obtain a token, and the security exposure of persisting a non-expiring token credential in a readable API object is acceptable to you." -->

3) 4) Docker config Secrets
If you are creating a Secret to store credentials for accessing a container image registry
  <!-- kubernetes.io/dockercfg: store a serialized ~/.dockercfg which is the legacy format for configuring Docker command line. The Secret data field contains a .dockercfg key whose value is the content of a base64 encoded ~/.dockercfg file.
  
  kubernetes.io/dockerconfigjson: store a serialized JSON that follows the same format rules as the ~/.docker/config.json file, which is a new format for ~/.dockercfg. The Secret data field must contain a .dockerconfigjson key for which the value is the content of a base64 encoded ~/.docker/config.json file. -->

5) Basic authentication Secret 
The kubernetes.io/basic-auth type is provided for storing credentials needed for basic authentication. When using this Secret type, the data field of the Secret must contain one of the following two keys:

  username: the user name for authentication
  password: the password or token for authentication
  <!-- Both values for the above two keys are base64 encoded strings. You can alternatively provide the clear text content using the stringData field in the Secret manifest. -->
  <!-- The basic authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for basic authentication. However, using the defined and public Secret type (kubernetes.io/basic-auth) helps other people to understand the purpose of your Secret, and sets a convention for what key names to expect. -->

The following manifest is an example of a basic authentication Secret:
~~~yml
apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth
stringData:
  username: admin # required field for kubernetes.io/basic-auth
  password: t0p-Secret # required field for kubernetes.io/basic-auth
~~~

6) ssh-auth:
The builtin type kubernetes.io/ssh-auth is provided for storing data used in SSH authentication. When using this Secret type, you will have to specify a ssh-privatekey key-value pair in the data (or stringData) field as the SSH credential to use.

The following manifest is an example of a Secret used for SSH public/private key authentication:

~~~yml
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # the data is abbreviated in this example
  ssh-privatekey: |
    UG91cmluZzYlRW1vdGljb24lU2N1YmE= 
~~~
7) tls:
The kubernetes.io/tls Secret type is for storing a certificate and its associated key that are typically used for TLS.

One common use for TLS Secrets is to configure encryption in transit for an Ingress, but you can also use it with other resources or directly in your workload. When using this type of Secret, the tls.key and the tls.crt key must be provided in the data (or stringData) field of the Secret configuration, although the API server doesn't actually validate the values for each key.

As an alternative to using stringData, you can use the data field to provide the base64 encoded certificate and private key. For details, see Constraints on Secret names and data.

The following YAML contains an example config for a TLS Secret:


<!-- apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # values are base64 encoded, which obscures them but does NOT provide
  # any useful level of confidentiality
  tls.crt: |
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNVakNDQWJzQ0FnMytNQTBHQ1NxR1NJYjNE
    UUVCQlFVQU1JR2JNUXN3Q1FZRFZRUUdFd0pLVURFT01Bd0cKQTFVRUNCTUZWRzlyZVc4eEVEQU9C
    Z05WQkFjVEIwTm9kVzh0YTNVeEVUQVBCZ05WQkFvVENFWnlZVzVyTkVSRQpNUmd3RmdZRFZRUUxF
    dzlYWldKRFpYSjBJRk4xY0hCdmNuUXhHREFXQmdOVkJBTVREMFp5WVc1ck5FUkVJRmRsCllpQkRR
    VEVqTUNFR0NTcUdTSWIzRFFFSkFSWVVjM1Z3Y0c5eWRFQm1jbUZ1YXpSa1pDNWpiMjB3SGhjTk1U
    TXcKTVRFeE1EUTFNVE01V2hjTk1UZ3dNVEV3TURRMU1UTTVXakJMTVFzd0NRWURWUVFHREFKS1VE
    RVBNQTBHQTFVRQpDQXdHWEZSdmEzbHZNUkV3RHdZRFZRUUtEQWhHY21GdWF6UkVSREVZTUJZR0Ex
    VUVBd3dQZDNkM0xtVjRZVzF3CmJHVXVZMjl0TUlHYU1BMEdDU3FHU0liM0RRRUJBUVVBQTRHSUFE
    Q0JoQUo5WThFaUhmeHhNL25PbjJTbkkxWHgKRHdPdEJEVDFKRjBReTliMVlKanV2YjdjaTEwZjVN
    Vm1UQllqMUZTVWZNOU1vejJDVVFZdW4yRFljV29IcFA4ZQpqSG1BUFVrNVd5cDJRN1ArMjh1bklI
    QkphVGZlQ09PekZSUFY2MEdTWWUzNmFScG04L3dVVm16eGFLOGtCOWVaCmhPN3F1TjdtSWQxL2pW
    cTNKODhDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUVVGQUFPQmdRQU1meTQzeE15OHh3QTUKVjF2T2NS
    OEtyNWNaSXdtbFhCUU8xeFEzazlxSGtyNFlUY1JxTVQ5WjVKTm1rWHYxK2VSaGcwTi9WMW5NUTRZ
    RgpnWXcxbnlESnBnOTduZUV4VzQyeXVlMFlHSDYyV1hYUUhyOVNVREgrRlowVnQvRGZsdklVTWRj
    UUFEZjM4aU9zCjlQbG1kb3YrcE0vNCs5a1h5aDhSUEkzZXZ6OS9NQT09Ci0tLS0tRU5EIENFUlRJ
    RklDQVRFLS0tLS0K    
  # In this example, the key data is not a real PEM-encoded private key
  tls.key: |
    RXhhbXBsZSBkYXRhIGZvciB0aGUgVExTIGNydCBmaWVsZA==  -->
or

kubectl create secret tls my-tls-secret \
  --cert=path/to/cert/file \
  --key=path/to/key/file
- - - - - - - - - - - - - - - - - - - -  - -- - - -  - - - - - - - - - - - - -
BASE64:  
echo -n 'my-app' | base64
echo -n '39528$vdg7Jb' | base64

CREATE ON CMD
kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'
> when you store secret from command line, you give plaintext secret, not base64


> Note: If you do not want to perform the base64 encoding, you can choose to use the stringData: field instead of Data: in yml file
~~~yml
apiVersion: v1
kind: Secret 
metadata:
  name: userpass
data:
  username: bXktYXBw
  password: cGFzc3dvcmQ=

--- 
apiVersion: v1
kind: Pod 
metadata: 
  name: secret-pod
spec: 
  containers:
    - image: nginx
      name: secret-pod-cont
      volumeMounts:
        - name: secret-vol
          mountPath: /etc/secret
          readOnly: true
  volumes:
  - name: secret-vol
    secret:
      secretName: userpass

~~~

kubectl get secret test-secret
Output:

NAME          TYPE      DATA      AGE
test-secret   Opaque    2         1m


# Run this in the shell inside the container
echo "$( cat /etc/secret-volume/username )"
echo "$( cat /etc/secret-volume/password )"

>Use case: dotfiles in a secret volume
You can make your data "hidden" by defining a key that begins with a dot. This key represents a dotfile or "hidden" file. For example, when the following Secret is mounted into a volume, secret-volume, the volume will contain a single file, called .secret-file, and the dotfile-test-container will have this file present at the path /etc/secret-volume/.secret-file.

> Alternatives to Secrets
Rather than using a Secret to protect confidential data, you can pick from alternatives.

Here are some of your options:

If your cloud-native component needs to authenticate to another application that you know is running within the same Kubernetes cluster, you can use a ServiceAccount and its tokens to identify your client.
There are third-party tools that you can run, either within or outside your cluster, that manage sensitive data. For example, a service that Pods access over HTTPS, that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount token).
For authentication, you can implement a custom signer for X.509 certificates, and use CertificateSigningRequests to let that custom signer issue certificates to Pods that need them.
You can use a device plugin to expose node-local encryption hardware to a specific Pod. For example, you can schedule trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.
--------------------------------------------------------------
# CONFIGMAPS
> configMaps can be immutable
apiVersion: v1
kind: ConfigMap
metadata:
  ...
data:
  ...
immutable: true
# Application Configuration



**Application Configuration**
When you are running applications in
Kubernetes, you may want to pass dynamic
values to your applications at runtime to
control how they behave. This is known as
application configuration.


# 2 ways to pass configMaps: 
## Volumes
## ENV Variables

**ConfigMaps** 
You can store configuration data in
Kubernetes using ConfigMaps.
ConfigMaps store data in the form of a
key-value map. ConfigMap data can
be passed to your container
applications.
## ENV Variables
~~~yaml
apiVersion: vl
kind: ConfigMap
metadata:
  name: my-configmap
data:
  key 1: valuel
  key 2: value2
  key 3:
    subkey: some more data
    morekeys: data
key4: |
    You can also do
    multi-line
    data.
~~~


**Secrets**
Secrets are similar to ConfigMaps but are designed to store sensitive data, such as
passwords or API keys, more securely. They are created and used similarly to ConfigMaps.
~~~yaml
apiVersion: vl
kind: Secret
metadata:
    name: my-secret
    type: Opaque
data :
    username: user
    password: mypass
~~~

You can pass ConfigMap and Secret data to your containers as environment variables. These
variables will be visible to your container process at runtime.
spec :
  containers:
  - :...
    env:
      - name: ENVVAR
        valueFrom:
          configMapKeyRef:
            name: my-configmap
            key: mykey


**Configuration Volumes**
Configuration data from ConfigMaps and Secrets can also be passed to containers in the
form of mounted volumes. This will cause the configuration data to appear in files available to
the container file system.
Each top-level key in the configuration data will appear as a file containing all keys below that
top-level key.


-------------------------------------------------------------------------------------------------
configMap/busybox.yml

~~~yml
apiVersion: v1
kind: Pod
metadata:
  name: env-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'echo "configmap: $CONFIGMAPVAR secret: $SECRETVAR"']
    env:
    - name: CONFIGMAPVAR
      valueFrom:
        configMapKeyRef:
          name: my-configmap
          key: key1
    - name: SECRETVAR
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: secretkey1


configmap/my-configmap.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  key1: Hello, world!
  key2: |
    Test
    multiple lines
    more lines


$ echo -n 'secret' | base64
$ echo -n 'anothersecret' | base64

configMap/my-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  secretkey1: c2VjcmV0
  secretkey2: YW5vdGhlcnNlY3JldA==
~~~
-----------------------------------------------------------------------------------------------------
**Configuration Volumes**
Configuration data from ConfigMaps and Secrets can also be passed to containers in the
form of mounted volumes. This will cause the configuration data to appear in files available to
the container file system.
Each top-level key in the configuration data will appear as a file containing all keys below that
top-level key.


~~~yml
[devops@rhel1 configMaps]$ cat volume-pod.yml 
apiVersion: v1
kind: Pod
metadata:
  name: volume-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    volumeMounts:
    - name: configmap-volume
      mountPath: /etc/config/configmap
    - name: secret-volume
      mountPath: /etc/config/secret  
  volumes:
  - name: configmap-volume
    configMap:
      name: my-configmap
  - name: secret-volume
    secret:
      secretName: my-secret

~~~
~~~sh
[devops@rhel1 configMaps]$ vim volume-pod.yml
[devops@rhel1 configMaps]$ kubectl create -f volume-pod.yml 
pod/volume-pod created
[devops@rhel1 configMaps]$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
my-deployment-6d69b4dd5-274vl   1/1     Running   0          16h
my-deployment-6d69b4dd5-2n2rq   1/1     Running   0          16h
my-deployment-6d69b4dd5-j8chw   1/1     Running   0          14h
my-deployment-6d69b4dd5-xmsrn   1/1     Running   0          14h
volume-pod                      1/1     Running   0          16s


[devops@rhel1 configMaps]$ kubectl exec volume-pod -- ls /etc/config/configmap
key1
key2

[devops@rhel1 configMaps]$ kubectl exec volume-pod -- cat /etc/config/configmap/key1
Hello, world!
[devops@rhel1 configMaps]$ kubectl exec volume-pod -- cat /etc/config/configmap/key2
Test
multiple lines
more lines

[devops@rhel1 configMaps]$ kubectl exec volume-pod -- ls /etc/config/secret
secretkey1
secretkey2

[devops@rhel1 configMaps]$ kubectl exec volume-pod -- cat /etc/config/secret/secretkey1
secret
[devops@rhel1 configMaps]$ kubectl exec volume-pod -- cat /etc/config/secret/secretkey2
anothersecret
~~~
-----------------------------------------------------------------------------------------------------
## LAB
You are working for BeeBox, a company that provides regular shipments of bees to customers. The company is working on deploying some applications to a Kubernetes cluster.

One of these applications is a simple Nginx web server. This server is used as part of a secure backend application, and the company would like it to be configured to use HTTP basic authentication.

This will require an htpasswd file as well as a custom Nginx config file. In order to deploy this Nginx server to the cluster with good configuration practices, you will need to load the custom Nginx configuration from a ConfigMap (this already exists) and use a Secret to store the htpasswd data.

Create a Pod with a container running the nginx:1.19.1 image. Supply a custom Nginx configuration using a ConfigMap, and populate an htpasswd file using a Secret.

htpasswd is already installed on the server, and you can generate an htpasswd file like so:

htpasswd -c .htpasswd user
A pod called busybox already exists in the cluster, which you can use to contact your Nginx pod and test your setup.

> Generate an `htpasswd` File and Store It as a Secret

Use htpasswd to generate an htpasswd file.

Create a secret called nginx-htpasswd, and store the contents of the htpasswd file in that Secret. Delete the htpasswd file once the Secret is created.


> Create the Nginx Pod

Create a pod with a single container using the nginx:1.19.1 image.

The Nginx configuration is stored in an existing ConfigMap called nginx-config. Mount the ConfigMap to /etc/nginx in your pod.

Mount your htpasswd secret to /etc/nginx/conf within your pod. The htpasswd data should be in a file in the container at /etc/nginx/conf/.htpasswd.

Expose containerPort 80 on the Nginx container so you can communicate with it to test your setup.



~~~sh
cloud_user@k8s-control:~$ htpasswd -c .htpasswd user
New password:
Re-type new password:
Adding password for user user
cloud_user@k8s-control:~$ rm -rf .htpassword
cloud_user@k8s-control:~$ ll
total 48
drwxr-xr-x 7 cloud_user cloud_user 4096 Nov 29 20:52 ./
drwxr-xr-x 4 root       root       4096 Jul  7  2020 ../
drwx------ 3 cloud_user cloud_user 4096 Feb  1  2022 .ansible/
-rw-r--r-- 1 cloud_user cloud_user    9 Jul  6 20:57 .bash_history
-rw-r--r-- 1 cloud_user cloud_user 3106 Jul 10  2020 .bashrc
drwx------ 2 cloud_user cloud_user 4096 Nov 29 20:49 .cache/
drwx------ 3 cloud_user cloud_user 4096 Dec 16  2021 .config/
-rw-rw-r-- 1 cloud_user cloud_user   43 Nov 29 20:52 .htpasswd
drwxr-xr-x 2 root       root       4096 Nov 29 19:27 .kube/
-rw-r--r-- 1 cloud_user cloud_user  161 Jul 10  2020 .profile
drwx------ 2 cloud_user cloud_user 4096 Nov 29 19:24 .ssh/
-rw-r--r-- 1 cloud_user cloud_user    0 Aug 20  2020 .sudo_as_admin_successful
-rw------- 1 cloud_user cloud_user  825 Feb  8  2022 .viminfo
cloud_user@k8s-control:~$ cat .htpasswd
user:$apr1$ROHGDuIA$3bkIdd4GAGD2lW7DngY0n0
cloud_user@k8s-control:~$ kubectl create secret generic nginx-htpasswd --from-file .htpasswd
secret/nginx-htpasswd created
cloud_user@k8s-control:~$ rm .htpasswd
cloud_user@k8s-control:~$ vim pod.yml
cloud_user@k8s-control:~$ kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      101m
nginx-config       1      100m
cloud_user@k8s-control:~$ kubectl describe cm nginx-config
Name:         nginx-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
nginx.conf:
----
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
  worker_connections  1024;
}
http {
    server {
        listen       80;
        listen  [::]:80;
        server_name  localhost;
        location / {
            root   /usr/share/nginx/html;
            index  index.html index.html;
        }
        auth_basic "Secure Site";
        auth_basic_user_file conf/.htpasswd;
    }
}


BinaryData
====

Events:  <none>
~~~

nginx.yml

~~~yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1
    ports:
    - containerPort: 80
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx
    - name: htpasswd-volume
      mountPath: /etc/nginx/conf
  volumes:
  - name: config-volume
    configMap:
      name: nginx-config
  - name: htpasswd-volume
    secret:
      secretName: nginx-htpasswd
~~~
~~~sh


cloud_user@k8s-control:~$ kubectl apply -f pod.yml
pod/nginx created
cloud_user@k8s-control:~$ kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          104m   192.168.194.65   k8s-worker1   <none>           <none>
nginx     1/1     Running   0          10s    192.168.194.69   k8s-worker1   <none>           <none>


cloud_user@k8s-control:~$ kubectl exec busybox -- curl 192.168.194.69
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   179  100   179    0     0   121k      0 --:--:-- --:--:-- --:--:--  174k
<html>
<head><title>401 Authorization Required</title></head>
<body>
<center><h1>401 Authorization Required</h1></center>
<hr><center>nginx/1.19.1</center>
</body>
</html>


cloud_user@k8s-control:~$ kubectl exec busybox -- curl -u user:asdf1234 192.168.194.69
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   612  100   612    0     0   197k      0 --:--:-- --:--:-- --:--:--  298k
~~~
-------------------------------------------------------------------------------------------------------
# Resource Request
~~~yml
apiVersion: v1
kind: Pod
metadata:
  name: big-request-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    resources:
      requests:
        cpu: '10000m'
        memory: '128Mi'
~                         


[devops@rhel1 pods]$ cat resourceRequestPod.yml 
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    resources:
      requests:
        cpu: '250m'
        memory: '128Mi'
      limits:
        cpu: '500m'
        memory: '256Mi'

~~~

------------------------------------------------------------
## ConfigMap with Volumes
apiVersion: v1
kind: ConfigMap
metadata:
  name: dev-html
data:
  index.html: |
    <H1> Hello Dev Env ConfigMap </H1>

---
apiVersion: v1
kind: ConfigMap 
metadata:
  name: prod-html
data:
  index.html: |
    <H1> Hello Prod Env ConfigMap </H1>

--- 
apiVersion: v1
kind: Pod 
metadata:
  name: configmap-pod
spec:
  containers:
  - image: nginx
    name: configmap-pod-cont
    ports:
    - containerPort: 80
    volumeMounts:
      - name: html-from-configmap
        mountPath: /usr/share/nginx/html/
        readOnly: true 
  volumes:
  - name: html-from-configmap
    configMap:
      name: dev-html

----------------------------------------------------------
