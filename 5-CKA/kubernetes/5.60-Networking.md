
Basics of Networking
DNS Test
Troubleshooting
HPA
Ingress
Helm


## Kubernetes Networking 

> **Since a Kubernetes cluster consists of various nodes and pods, understanding how they communicate between them is essential. The Kubernetes networking model supports different types of open source implementations.**

> The **`Kubernetes networking model,`** on the other hand, natively supports **`multi-host networking`** in which pods are able to communicate with each other by default, regardless of which host they live in.

> Kubernetes **`does not provide a default network implementation,`** it only enforces a model for third-party tools to implement. There is a variety of implementations nowadays, below we list some popular ones.
	
1. **`Flannel`** - a very simple overlay network that satisfies the Kubernetes requirements. Flannel runs an agent on each host and allocates a subnet lease to each of them out of a larger, preconfigured address space.Flannel creates a flat network called as overlay network which runs above the host network.
	
2. **`Project Calico`** - an open source container networking provider and network policy engine. Calico provides a highly scalable networking and network policy solution for connecting Kubernetes pods based on the same IP networking principles as the internet. Calico can be deployed without encapsulation or overlays to provide high-performance, high-scale data center networking.

3. **`Weave Net`** - a cloud native networking toolkit which provides a resilient and simple to use (does not require any configuration) network for Kubernetes and its hosted applications. It provides various functionalities like scaling, service discovery, performance without complexity and secure networking.
	
4. **`Other options`** include Cisco ACI , Cilium , Contiv , Contrail , Kube-router , Nuage , OVN , Romana , VMWare NSX-T with NSX-T Container Plug-in (NCP) . Some tools even support using multiple implementations, such as Huawei CNI-Genie and Multus .


## Pod to Pod Communication 

> Kubernetes follows an `“IP-per-pod”` model where each pod get assigned an IP address and all containers in a single pod share the same network namespaces and IP address. Containers in the same pod can therefore reach each other’s ports via localhost:<port>. 
	
> However, **`it is not recommended to communicate directly with a pod via its IP address`** due to pod’s volatility (a pod can be killed and replaced at any moment). 
	
> Instead, **`use a Kubernetes service`** which represents a group of pods acting as a single entity to the outside. Services get allocated their own IP address in the cluster and provide a reliable entry point.

> **Kubernetes services allow grouping pods under a common access policy (for example, load-balanced). The service gets assigned a virtual IP which pods outside the service can communicate with. Those requests are then transparently proxied (via the kube-proxy component that runs on each node) to the pods inside the service. Different proxy-modes are supported:**
	
* **`iptables:`** kube-proxy installs iptables rules trap access to service IP addresses and redirect them to the correct pods.

* **`userspace:`** kube-proxy opens a port (randomly chosen) on the local node. Requests on this “proxy port” get proxied to one of the service’s pods (as retrieved from Endpoints API).

* **`ipvs (from Kubernetes 1.9):`** calls netlink interface to create ipvs rules and regularly synchronizes them with the Endpoints API.
-----------------------------------------------------------------------------------	
## DNS for Services and Pods
	
> Kubernetes provides its own DNS service to resolve domain names inside the cluster in order for pods to communicate with each other. This is implemented by deploying a regular Kubernetes service which does name resolution inside the cluster, and configuring individual containers to contact the DNS service to resolve domain names. Note that this “internal DNS” is compatible and expected to run along with the cloud provider’s DNS service.

> Every service gets assigned a DNS name which resolves to the cluster IP of the service. The naming convention includes the service name and its namespace. For example:
* **`my-service.my-namespace.svc.cluster.local`**

> A pod inside the same namespace as the service does not need to fully qualify its name, for example a pod in my-namespace could lookup this service with a DNS query for my-service , while a pod outside my-namespace would have to query for my-service.my-namespace .

> For headless services (without a cluster IP), the DNS name resolves to the set of IPs of the pods which are part of the service. The caller can then use the set of IPs as it sees fit (for example round-robin).

> By default pods get assigned a DNS name which includes the pod’s IP address and namespace. In order to assign a more meaningful DNS name, the pod’s specification can specify a hostname and subdomain:
		

devops@T480:~$ kubectl -n default run mynginx --image nginx --expose --port 80
service/mynginx created
pod/mynginx created

devops@T480:~$ nslookup mynginx.default.svc
Server:         172.22.80.1
Address:        172.22.80.1#53


# exam question: find dns name of a particular object
devops@T480:~$ kubectl -n testns run dnstest --image=lerndevops/netshoot
--rm -it -- /bin/bash
bash 5.0 ~ cat /etc/resolv.conf
bash 5.0 ~ nslookup mynginx.default.svc
bash 5.0 ~ nslookup 10-1-34-8.testns.pod

> objectname.namespace.objecttype.cluster.local [kubernetes naming style for svc]
> objectIP.namespace.objecttype.cluster.local [kubernetes naming style for pods]

# dns service/pod test

## Step1: create pod nginx & service nginx in a namespace called "testns"

```
# create

kubectl create namespace testns  ## creates testns namespace 
kubectl -n testns run nginx --image=nginx --expose --port=80 ## create a pod & service called nginx in testns namespace
```
```
#validate 

root@master:~# kubectl get all -n testns -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
pod/nginx   1/1     Running   0          66s   10.244.1.54   node1   <none>           <none>

NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/nginx   ClusterIP   10.102.75.3   <none>        80/TCP    66s   run=nginx
```

## Step2: create pod mynginx & service mynginx in a namespace called "default"

```
# create


kubectl -n default run mynginx --image=nginx --expose --port=80 ## create a pod & service called nginx in testns namespace
```
```
# validate

root@master:~# kubectl get all -n default -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
pod/mynginx   1/1     Running   2          18h   10.244.1.49   node1   <none>           <none>

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/mynginx      ClusterIP   10.102.183.125   <none>        80/TCP   10m     env=test
```


## Step3: create another pod in any namespace which has netutils so that we can run a nslookup command

```
# Create

kubectl -n testns run dnstest --image=lerndevops/samples:netshoot --rm -it -- /bin/bash

the above command will create new pod called dnstest in testns namespace & get you insideit, from inside pod we can try accessing pods using their service dns & pod dns 
```

## Step4: test Service DNS 

```
Note: in step3 we are getting inside pod, ensure we are inside pod to run below commands

# check default dns values configured for the pod
 
	bash-5.0# cat /etc/resolv.conf
	nameserver 10.96.0.10
	search testns.svc.cluster.local svc.cluster.local cluster.local us-central1-a.c.devops-262910.internal c.devops-262910.internal google.internal
	options ndots:5

# Note:
	to form the service DNS get the service name then add .namespace.svc
	example: servicename.namespace.svc
			  nginx.testns.svc

# nslookup the service nginx using its DNS in testns namespace
 
	bash-5.0# nslookup nginx.testns.svc
	Server:         10.96.0.10
	Address:        10.96.0.10#53

	Name:   nginx.testns.svc.cluster.local
	Address: 10.102.75.3

# nslookup the service mynginx using its DNS in default namespace

	bash-5.0# nslookup mynginx.default.svc
	Server:         10.96.0.10
	Address:        10.96.0.10#53

	Name:   nginx-svc.default.svc.cluster.local
	Address: 10.102.183.125
```

## Step5: test POD DNS

```
# Note:
	to form the pod DNS get the podip & replace "." with "-" then add .namespace.pod
	example: covnert 1.2.3.4 to 1-2-3-4 & add .namespace.pod
			  1-2-3-5.testns.pod
		  
# nslookup the pod nginx using its DNS in testns namespace

	bash-5.0# nslookup 10-244-1-54.testns.pod
	Server:         10.96.0.10
	Address:        10.96.0.10#53

	Name:   10-244-1-54.testns.pod.cluster.local
	Address: 10.244.1.54

# nslookup the pod mynginx using its DNS in default namespace

	bash-5.0# nslookup 10-244-1-49.default.pod
	Server:         10.96.0.10
	Address:        10.96.0.10#53

	Name:   10-244-1-49.default.pod.cluster.local
	Address: 10.244.1.49
```
----------------------------------------------------------------------------------


> EXAM QUESTION
find 404 error from pod and save it to a file
root@worker:~# curl 10.32.0.2:80/shush
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.25.3</center>
</body>
</html>
root@worker:~#
logout
Connection to 143.198.53.104 closed.
devops@T480:~$ kubectl logs mynginx | grep 404 > /tmp/errorfile
devops@T480:~$ cat /tmp/errorfile
10.32.0.1 - - [19/Dec/2023:13:17:52 +0000] "GET /shush HTTP/1.1" 404 153 "-" "curl/8.2.1" "-"

> EXAM QUESTION
if a pod is not creating or is in pending state:

kubectl describe pod pod1
.
.
.
Events:
  Type     Reason                  Age                   From
   Message
  ----     ------                  ----                  ----
   -------                                                     > 1st step is SCHEDULING
  Normal   Scheduled               43m                   default-scheduler  Successfully assigned default/mynginx to worker
  Warning  FailedCreatePodSandBox  43m                   kubelet
   Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "3d5ecbd4bc37f6bcddf78549e193aca0eeafad78e6eba95c7dd56ce31ed4a07f": plugin type="weave-net" name="weave" failed (add): unable to allocate IP address: Post "http://127.0.0.1:6784/ip/3d5ecbd4bc37f6bcddf78549e193aca0eeafad78e6eba95c7dd56ce31ed4a07f": dial tcp 127.0.0.1:6784: connect: connection refused
  Normal   SandboxChanged          39m (x21 over 43m)    kubelet
   Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling                 39m                   kubelet
   Pulling image "nginx"
  Normal   Pulled                  39m                   kubelet
   Successfully pulled image "nginx" in 7.041s (7.041s including waiting)
  Normal   Created                 39m                   kubelet
   Created container mynginx
  Normal   Killing                 3m14s (x12 over 36m)  kubelet
   Stopping container mynginx

   IF YOU SEE NOTHING IN EVENTS => THEN IT MEANS SCHEDULAR HAS MALFUNCTIONED
   GO STRAIGHT TO /etc/kubernetes/manifests/schedular.yml and see whats wrong


> EXAM QUESTION
   if you place a static pod on a node and its not coming up, check the kubelet service of the node, because static pods are managed by the kubelet
   if pod is still not coming up after restarting kubelet of the node, check kubelet config file in /var/lib/kubelet, it has the path for static pods as /etc/kubernetes/manifests
   root@worker:~# cd /var/lib/kubelet/
root@worker:/var/lib/kubelet# ll
total 48
drwxr-xr-x  8 root root 4096 Dec 18 15:34 ./
drwxr-xr-x 48 root root 4096 Dec 19 12:39 ../
-rw-r--r--  1 root root 1044 Dec 18 15:34 config.yaml
-rw-------  1 root root   62 Dec 18 15:15 cpu_manager_state
drwxr-xr-x  2 root root 4096 Dec 18 15:34 device-plugins/
-rw-r--r--  1 root root  149 Dec 18 15:34 kubeadm-flags.env
-rw-------  1 root root   61 Dec 18 15:15 memory_manager_state
drwxr-xr-x  2 root root 4096 Dec 18 15:34 pki/
drwxr-x---  2 root root 4096 Dec 18 15:15 plugins/
drwxr-x---  2 root root 4096 Dec 18 15:15 plugins_registry/
drwxr-x---  2 root root 4096 Dec 18 15:34 pod-resources/
drwxr-x---  6 root root 4096 Dec 19 13:08 pods/
root@worker:/var/lib/kubelet# cat config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests        <----------------------------------------------------->
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s

--------------------------------------------------------------------------------------------------------
## EGRESS - out sending traffic policy
## INGRESS - in sending traffic policy

if you want traffic to flow only in 1 certain direction between pods:
create Egress Network Policy

 ----------                                ----------
| app: web |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| app:db |

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-netpol
spec:
  podSelector:
    matchLabels:
      app: web
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: db
#    ports:
#    - port: 5432


---------------------------------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-netpolicy
spec:
  podSelector:
    matchLabels:
      app: db
  ingress:      
  - from:
    - podSelector:
        matchLabels:
          app: web
          #    ports:
          #    - port: 80 
---------------------------------------

## Network Policies 

> A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints.

1.   Network policies allow you to specify which pods can talk to other pods. This helps when securing communication between pods, allowing you to identify `ingress` and `egress` rules. 
2.   we can apply a network policy to a pod by using pod or namespace selectors. 
3.   we can even choose a CIDR block range to apply the network policy

## Prerequisites

> Network policies are implemented by the network plugin, so we must be using a networking solution which supports NetworkPolicy. Ex: calico, canal provides these features ( in cka exam you may see canal already setup )

>  simply creating the resource without a controller to implement it will have no effect 

### By Default , all pods in the cluster can communicate with any other pod and reach out to any available IP.

### Network Policies allow you to limit what network traffic is allowed to and from pods in your cluster.

# Demo

## Setp1: Deploy Spring Java Application & MongoDB Pods 
```
kubectl apply -f https://raw.githubusercontent.com/lerndevops/educka/master/examples/springboot-mongo-app.yml
```

## Step2: access the Sping Java Application & write some data to mongo db

```
kubectl get services springboot-app-svc

use the NodPort to access the springboot java in the browser 
```

> this proves that the You are able to access application from app pods 

> app is able to communicate to mongodb pods & write the data to it

## Step3: Now lets block the request / traffic to springa app & mongo db using Network Policies
```
kubectl apply -f https://raw.githubusercontent.com/lerndevops/educka/master/6-networking/policies/deny-ingress-to-mongodb-and-springapp.yaml
```

## Step4: Now try to access application from browser it shoudn't respond 

```
kubectl get services springboot-app-svc

use the NodPort to access the springboot java in the browser 
```

> This proves we successfully block all ingress (incoming) traffic to spring app 


## Step5: Now lets allows ingress(incoming) traffic to spring java app fromm all using Network Policies 

```
kubectl apply -f https://raw.githubusercontent.com/lerndevops/educka/master/6-networking/policies/allow-ingress-to-springapp-from-all.yaml
```
```
kubectl get services springboot-app-svc

use the NodPort to access the springboot java in the browser
```

> This should allow the traffic to Spring java App & you should see the app in browser 

> But if you try to submit the data to DB it will not respond, we still need to allow traffic to mongodb 

## Step6: Now lets allow ingress(incoming) traffic to mongodb only from spring app pods using Network Policies 

```
kubectl apply -f https://raw.githubusercontent.com/lerndevops/educka/master/6-networking/policies/allow-ingress-to-mongodb-from-springapp.yaml
```

> Now we should be able to write the data to mongodb from spring java app