Low-Latency NoSQL database 
serverless, integrates well with lambda, autoscales
performance is good with SSD storage 
resiliance is good, spread across 3 distinct data centers
supports both document and key-value data models.
supported formats are JSON, HTML, XML
item = row 
attribute = column
PARTITION KEY is also called HASH  .
        A GOOD PARTITION KEY IS SHIP CLASS in STARSHIP TABLE because ITs a GROUPING OF THINGS - and as long as you have a unique value with combination of partition and sort key, it is ok
        3 ships have same ship class, etc but 
SORT KEY is also called RANGE .
DynamoDB is a no SQL key/value and Document Store database
key/value = {Title: "S01E019 DS9 Duet"}
Document Store: nested data structure
{
    Series: "DS9",
    Episodes: [{
        Season: 1,
        Episodes: 19,
        Title: "Duet"
    }]
}
DynamoDB is AWS Flagship database:
Fully managed
Multi-region
Multi-master
Durable
Built-in security
Backup and restore
in-memory caching 

rows = items 
columns = attributes 
keys = identifying names of your data 
values = the actual data itself
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Amazon DocumentDB (with MongoDB compatibility)

Amazon DocumentDB is a fully managed document database from AWS. A document database is a type of NoSQL database you can use to store and query rich documents in your application. These types of databases work well for the following use cases: content management systems, profile management, and web and mobile applications. Amazon DocumentDB has API compatibility with MongoDB. This means you can use popular open-source libraries to interact with Amazon DocumentDB, or you can migrate existing databases to Amazon DocumentDB with minimal hassle.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Amazon Keyspaces (for Apache Cassandra)

Amazon Keyspaces is a scalable, highly available, and managed Apache Cassandra compatible database service. Apache Cassandra is a popular option for high-scale applications that need top-tier performance. Amazon Keyspaces is a good option for high-volume applications with straightforward access patterns. With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra Query Language (CQL) code, Apache 2.0 licensed drivers, and tools that you use today.
Amazon Keyspaces (for Apache Cassandra) implements the Apache Cassandra Query Language (CQL) API, so you can use CQL and Cassandra drivers that you already use. Updating your application is as easy as updating your Cassandra driver or cqlsh configuration to point to the Amazon Keyspaces service endpoint.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
DynamoDB security

DynamoDB provides a number of security features to consider as you develop and implement your own security policies. They include the following:
bullet
DynamoDB provides a highly durable storage infrastructure designed for mission-critical and primary data storage. Data is redundantly stored on multiple devices across multiple facilities in a DynamoDB Region.  
bullet
All user data stored in DynamoDB is fully encrypted at rest. DynamoDB encryption at rest provides enhanced security by encrypting all your data at rest using encryption keys stored in AWS Key Management Service (AWS KMS).
bullet
IAM administrators control who can be authenticated and authorized to use DynamoDB resources. You can use IAM to manage access permissions and implement security policies.
bullet

As a managed service, DynamoDB is protected by the AWS global network security procedures.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
USE CLOUDTRAIL .
When activity occurs in DynamoDB, that activity is recorded in CloudTrail ongoing record of events in DynamoDB and in your AWS files to an Amazon Simple Storage Service (Amazon S3)

If you are using an AWS managed key for encryption at rest, usage of the key is recorded in
AWS CloudTrail. CloudTrail can tell you who made the request, the services used, actions
performed, parameters for the action, and response elements returned.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


DYNAMODB READ CONSISTANCY:
data is saved to 3 locations AZs(data centers) at once on SSDs

it is possible for data to be inconsistant when reading from a copy which has yet to be updated
solution:---------
CONSISTENCY MODELS:

->Eventually Consistent-default
(consistancy across all copies of data = 1sec)
best for read performance
all copies of data eventually become generally consistant within 1sec
Reads are fast but no guarentee of consistancy

->Strongly Consistent
Reads are slower but with guarentee of consistancy (high latency)
use it when consistant data is crucial for your business logic
best for read consistency
==========================================================================
TRANSACTIONS (at no additional cost)
DDB offers ALL or NOTHING changes to multiple items both WITHIN & ACROSS tables
->DynamoDB Transactions -> supports ACID Transactions
Atomic (all or nothing transaction,if my money is deducted,urs is toppd up)
Consistant (consistant with data validation rules)
Isolated (transactions must happen in isolation,independent of one another)
Durable (they dont disappear if a system crashes)
if any of the steps fail, rollback db as if nothing initiated 
e.g. 
`````````````Transaction start`````````````````````
Create new payee 
Verify email in correct format  X (wrong format so it rolls back)
remove $100 from our account 
`````````````Transaction end`````````````````````
all this happens in DynamoDB using 2 functions:
            TransactWriteItems
            TransactGetItems
DDB performs TWO underlying READS or WRITES of evry item in the transaction
            one to Prepare the Transaction 
            one to Commit the Transaction
These underlying R/W operations are visible in your CloudWatch Metrics
U can also do conditional check with DDB transactions to check if item Exist/Specific Attribute can also be checked
            ConditionCheck

---------------------------------------------------------------------------
DynamoDB TTL 
DDB doesnt have a DateTime datatype
expressed as unix/posix/Epoch time, sec elapsed since Jan 1st, 1970, 12am
1544023618...
defines an expiry time for your data
helps reduce cost and makes db fast
e.g. session data, event logs, old data, tmp data

when the current time is greater than the TTL,
the item is going to be expired and it gets marked for deletion.
And then within the next 48 hours, it's actually going to be deleted.
So that's actually quite a long window between being marked for deletion and
actually getting deleted. So in order to cater for that,
you can actually filter out any expired items from your queries and scans.
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

PARTITIONS:
A partition is hwen you slice your table up into smaller chunks of data (a partition) it speeds up reads for very large tables by logically grouping similar data together
it makes Table FASTER
DynamoDB automatically creates partitions for you as your Data grows
How it works?
you start off with single partition
DynamoDB adds partition when:
    ----- exceed 10GB of data
    ----- exceed RCUs / WCUs for single partition 
          each partition has maximum of 3000 RCUs and 1000 WCUs

 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
PRIMARY KEYS:
allows us to query the data 
can be set only once - CANNOT BE CHANGED LATER
dynamoDB stores and retrieves data based on a primary key
2 types -
Partition Key - unique attribute, e.g. customer id, email id, car reg. no.
->value of the partition key is input to an internal hash function,
    which determines the partition or physical location on which,
    the data is stored. (nobody knows inner workings of hash algorithm)
->if u using partition key as ur prmary key, no 2 items can have same,
    partition key
Composite Key (partition key + sort key)
-> you use composite key, when your partition key is not necessarily unique within your table
e.g. a forum where users are posting multiple comments,
here, user id will not be a unique identifier to retrieve data.
so you user user id + a sort key(timestamp) to save data 
it gives you unique combination required for a primary key in table
-> all items with same partition key are stored together and are then sorted according to the sort key value.

 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

LAB
create a user with access keys
launch an instance 
login
install git 
run command 
aws dynamodb create-table --table-name ProductCatalog --attribute-definitions \
AttributeName=Id,AttributeType=N --key-schema \
AttributeName=Id,KeyType=HASH \ (*hash=primaryKey)
--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

then
3  git clone https://github.com/ACloudGuru-Resources/course-aws-certified-developer-associate.git
    4  ls
    5  cd course-aws-certified-developer-associate/
    6  ll
    7  cd Create_A_DynamoDB_Table_Demo/
    8  ll
    
or
curl -O https://raw.githubusercontent.com/ACloudGuru-Resources/course-aws-certified-developer-associate/main/Create_A_DynamoDB_Table_Demo/items.json

9  aws dynamodb batch-write-item --request-items file://items.json


[ec2-user@ip-172-31-36-129 ~]$ aws dynamodb get-item --table-name ProductCatalog --key '{"Id":{"N":"403"}}'
{
    "Item": {
        "Description": {
            "S": "Womens Cycling Helmet"
        },
        "Color": {
            "S": "Black"
        },
        "Price": {
            "S": "99"
        },
        "ProductCategory": {
            "S": "Helmet"
        },
        "Id": {
            "N": "403"
        },
        "Size": {
            "S": "Small"
        }
    }
}
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
INDEXES - for efficient Sorting
a database index is a copy of selected columns of data which is used to quickly sort
DynamoDB ACCESS CONTROL 
fine-grained Access Control with IAM 
IAM Condition parameter dynamodb:LeadingKeys 
--->allow users to access only the items where partition key value matches their user-id 
e.g. game scoreboard and points. user can only see their own profile

DynamoDB Secondary Indexes 
allow you to query based on attribute that is not the primary key
using global secondary indexes and local seconday indexes
you select the columns you want included in the index and run your searches on the index, rather than on the entire dataset

LOCAL Secondary Index
How-> same partition key as your original table, diff sort key. COMPOSITE
    -> needs both partition and sort key
-> gives you diff view of your data organized acc to alt sort key
-> cannot be added, modified or deleted after creation 
-> faster queries using index rather than main table 
-> provide strong CONSISTENCY 
-> shares provisioned throughput settings of table RCU/WCU
Limitation->can only be created when you are creating the table
          ->total size of indexed items for any one partition key value cant exceed 10GB 
          ->limited to 5 per table
          
GLOBAL Secondary Index 
AWS recommends using Global indexes
-> no size restrictions
-> have their own provisioned throughput settingslimited to 20 per table
much flexible, create anytime 
allows you to pick different partition key and diff sort key to main table
or same partition key and different sort key
SIMPLE+COMPOSITE --^
Cannot provide strong CONSISTENCY
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

SCAN vs QUERY API Calls
QUERY:
By Default, reads as "EventuallyConsistant", even if Str Cons is in settings
A query finds items in a table based on the primary key attribute and a dinstinct value to search for
refine your query using your sort key.
Query results are always sorted by the sort key 
you can use "ProjectionExpression" parameter to refine the results
e.g. if you only want to see id and email, not all attributes
if sort key is numeric,it returns data in ascending numeric order bydefault
if you want to reverse the order, set the "ScanIndexForward" to false
by default, query are eventually Consistent
you need to explicitly set the query to be strongly consistent
SCAN:
it examines all the items in the table, by default, it returns all data attributes
you can use "ProjectionExpression" parameter to refine the results
e.g. if you only want to see id and email, not all attributes

Query is more Efficient than Scan 
A Scan Operation on a large table can use up the provisioned throughput for a large table in a single operation
SCAN is sequential by default
can be done in parrallel by logically dividing the table or index into segments and scanning each sagment in parrellel
but its best to avoid parrallel scan if your table/index is heavy

Improve performance by reducing page size , limit to 40 items 
"Avoid scans, design tables in a way that you can use Query,Get or 
BatchGetItem APIs"
A large Table can eat your ProvisionedThroughput in a single Scan

-----------------------------------------------------------------------
DynamoDB API CALLS (programatically)

CLI:            API:
create-table    CreateTable table name must be unique within each Region .
put-item        PutItem
get-item        GetItem
transact-get-item =
                synchronous op, retrieve multiple items from 1ormore table (but not indexes) in a single account and region. can contain upto 25 objects. aggregate size of items < 4 MB
batch-get-item  single operation = limit upto 16 MB of data, max 100 items
batch-write-item = limit 16 MB of data, 25 put/del req, each item max 400kb
transact-write-item = 
                synchronous write operationthat groups upto 25 action requests. these actions can target items in d/f tables, but not in d/f AWS accounts or regions, and no two actions can target the same item

update-item     UpdateItem
update-table    UpdateTable modifies provisioned throughput of a table
                                     global secondary indexes 
                                     DynamoDB Streams settings
list-table      ListTables
describe-table  DescribeTable
scan            Scan
query           Query
delete-item     DeleteItem
delete-table    DeleteTable

if a user...
run put-item cli command runs PutItem API call on the database
They will need IAM permissions to call PutItem

[cloudshell-user@ip-10-4-79-87 ~]$ aws dynamodb describe-table --table-name ProductCatalog
{
    "Table": {
        "AttributeDefinitions": [
            {
                "AttributeName": "Id",
                "AttributeType": "N"
            }
        ],
        "TableName": "ProductCatalog",
        "KeySchema": [
            {
                "AttributeName": "Id",
                "KeyType": "HASH"
            }
        ],
        "TableStatus": "ACTIVE",
        "CreationDateTime": "2023-08-16T21:44:00.344000+00:00",
        "ProvisionedThroughput": {
            "NumberOfDecreasesToday": 0,
            "ReadCapacityUnits": 5,
            "WriteCapacityUnits": 5
        },
        "TableSizeBytes": 0,
        "ItemCount": 0,
        "TableArn": "arn:aws:dynamodb:us-east-1:077794698366:table/ProductCatalog",
        "TableId": "ca15d5ff-8519-4a23-85af-55a5ce76fc1f",
        "DeletionProtectionEnabled": false
    }
}
(END)

---------------------------------------------------------------------------
DynamoDB Provisioned Throughput
5 write capacity unit ---$2.91/month
5 read capacity unit  ---^
while creating table, specify...
1 write capacity unit = 1 x 1KB write per second
1 read capacity unit = 1 x 4KB read per second (strongly consistent)
or
1 read capacity unit = 2 x 4KB read per second (eventually consistent)
default
e.g. your app needs to read 80 items per second 
    each item is 3kb in size 
        you need strongly consistent reads
How many read capacity units will you need? 
size of each item / 4kb = 3 / 4 = 0.75 = 1 x 80
3kb/4kb = 0.75 rounded to 1kb ,so need 80 units (sc) and 40 units (ec)

e.g. you need to write 100 items per sec. each item is 512byte 
how many write capacity units you require?
size of each item / 1kb = 512 / 1024 = 0.5 rounded to 1
so we need 100 capacity units
use it when....
capacity can be forecasted 
predictable application traffic 
traffic is consistent 
you will hav more control over costs

DynamoDB on-Demand Capacity 
charges apply on reading,writing and storing data
use it when...you have 
unknown workloads 
unpredictable application traffic 
spiky, short-lived peaks 
a pay-per-use model is desired
more difficult to predict the cost b/c db will scale up n down

You have an application that needs to read 25 items per second and each item is 13KB in size. Your application uses eventually consistent reads. What should you set the read throughput to?
50 RCU
25 RCU
100 RCU
10 RCU
Sorry!
Correct Answer
Each Read Capacity unit represents 1 x 4KB Strongly Consistent Read. 13KB / 4KB = 3.25 rounded up to 4 multiply by 25 = 100 
each items eats --^ 4kb so 25 items = 100RCU StronglyC
& 100/2=50RCU EventuallyC
which gives you the figure for Strongly consistent reads. Divide that by 2 for Eventually consistent. So the answer is 50 RCU.

You have a motion sensor which writes 600 items of data every minute. Each item consists of 5KB. What should you set the write throughput to?
20
40
10
50
Sorry!
One write request unit represents one write for an item up to 1 KB in size. If you write 600 items every minute, it means 10 items are written per second ( 600 / 60 ). The total number of write capacity units required depends on the item size. If your item size is 5 KB, just multiply 10 items by 5 KB, so you require 50 write capacity units to sustain one write request. Read/Write Capacity Mode.
------------------------------------------------------------------------
DYNAMODB GLOBAL TABLES 
provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution

To create Global Tables: you need to.....
1- use KMS CMK
2- Enable Streams 
3- Stream Type of New and Old Image
then add region 
then add replica to global table
------------------------------------------------------------------------
DynamDB Accelerator DAX:
DAX is a fully managed clustered in-memory cache for DynamoDB
Deliver upto 10x read performance 
microsecond performance for millions of requests per second
e.g. black friday sale, big promotion day
how it works?......
DAX is a write-through caching servince. Data is written to the cache and the backend store at same time

instead of quering your db, your app can query DAX Cluster first
it item you are looking for is in the cache (cache hit). DAX returns the result
if CacheMiss -> DAX do GetItem from DB -> stores it in cache
So it gets the item out of DynamoDB, it writes into its cache,
and it also hands it back to the application.

|---------------------------------------|
|VPC                                    |
|    ____________        _____________  |
|   |Your APP   |       |cache | cache| |
|   |     ^     | <->   |cache | cache| | <-> DynamoDB Table
|   |     |     |       |             | |
|   |DAX Client |       |             | |
|   |___________|       |_____________| |
|      EC2                DAX Cluster   |
|                           4 nodes     |
|_______________________________________|
incoming requests are evenly distributed across all nodes in the cluster
one of the node serves as the primary node in the cluster 
additional nodes (if present) serve as Read Replicas
Each node runs its own INSTANCE of the DAX CACHING SOFTWARE .
app can access DAX by using endpoint for DAX cluster
    DAX CLIENT software works with Cluster  endpoint to perform INTELLIGENT LOAD BALANCING and ROUTING .

when DAX is not Suitable?......
it caters for eventually consistent reads only.
So it's not going to be suitable for applications that require strongly
consistent reads. so if you done want DAX, use ELASTICACHE .

It's also not really suitable for write-intensive applications.
So youre not going to get a benefit from using DAX because it only helps with read operations. 
Also,
applications that dont perform that many read operations are
not really going to see a benefit from configuring DAX,
and the same goes for applications that dont require microsecond response
times. Theres no point in configuring it if you dont need that low latency.


---------------------------------------------------------------------------
DynamoDB Streams (versions like)
Time Ordered Sequence of Modifications
is a time-ordered sequence, or stream,
and it records any modifications that are made to the items
in your DynamoDB table. So that's any time there's an insert, update,
or delete operation, you can send that change to lambda

For each item that is modified, the stream records appear in the same sequence as the actual modifications

Logs - encrypted at rest and stored for 24 hours
Dedicated Endpoint - Accessed using a dedicated endpoint 
By Default, Primary Key is recorded
Before and after Images can be captured 
Uses:
Audit or Archive transactions 
trigger an event based on a particular transaction 
replicate data across multiple tables 

Application--aws sdk--db api---> DynamoDB Endpoint 
Application--aws sdk--db streams api---> DynamoDB Streams Endpoint 
NEAR REAL_TIME 
Apps can take action based on contents of the stream
STREAMS is a great Event Source for Lambda



e.g. 
And in this example,
we have an invoicing and payment system,
which is recording invoice data into a DynamoDB table.
And each time a new invoice entry
is made into the DynamoDB table,
there is a Lambda function which reads the DynamoDB stream
to check for new transactions.
And then when it sees a new event,
it's sending a notification using SNS,
which in turn creates a message in an SNS queue.
And you can then have your payments application
polling that SQS queue, processing the message.
And let's say maybe it performs various tasks
like generating a payment request
or adding an item to a customer's bill, etc.
So you can see that this is a really powerful way
to generate triggers to Lambda,
to trigger your application
to take actions based on the changing contents
of your DynamoDB table.

---------------------------------------------------------------------------'
EXPONENTIAL BACKOFF - when provisioned throughput exceeds 
ProvisionedThroughputExceeded Error
ProvisionedThroughputExceededException
if you are using aws sdk , it will automallically retries
aws sdks use Exponential backoff by default
means - progressivly longer waits b/w consecutive retries for imp workflow
if you are not using aws sdk, you need to configure app to do 1 or 2 things
1- you can reduce the request frequency
2-implement exponential backoff...

DB<-failed request<-50 ms<-retry<-100 ms wait<-retry<-200ms<-retry<-400ms
this keeps on going until Request is successful

if it does not successfully happen within 1 min, it means request size may be exceeding throughput of read/write capacity
after improving capacity, if it still fails 
then use DAX / Elasticache

"exp backoff is a feature of all aws sdk, not limited to dynamodb"

---------------------------------------------------------------------------
Your low latency web application needs to store its session state in a scalable way so that it can be accessed quickly. Which service do you recommend?


In memory on your EC2 instance


Elastic File Store


RDS


DynamoDB

Good work!
Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. Amazon DynamoDB provides an effective solution for sharing session state across web servers without incurring drawbacks. Reference: Managing ASP.NET Session State with Amazon DynamoDB.




----------------------------------------------------
Using the AWS Console, you are trying to scale DynamoDB past its pre-configured maximums. Which service limits can you increase by raising a ticket to AWS support?

Choose 2

13/
Local Secondary Indexes


Global Secondary Indexes per table


Provisioned throughput limits


Item Sizes

Sorry!
You can define a maximum of 5 local secondary indexes. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

Correct Answer
There is an initial quota of 20 global secondary indexes per table. To request a service quota increase, see https://aws.amazon.com/support. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

AWS places some default quotas on the throughput you can provision. These are the quotas unless you request a higher amount. To request a service quota increase, see https://aws.amazon.com/support. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

------------------------------------------------------------

What is the difference between a Global Secondary Index and a Local Secondary Index

Choose 2


You can delete a Global Secondary Index at any time


You can create a Local Secondary Index at any time but you can only create a Global Secondary Index at table creation time


You can delete a Local Secondary Index at any time


You can create a Global Secondary Index at any time but you can only create a Local Secondary Index at table creation time

Sorry!
Correct Answer
Only Global secondary Indexes can be created or deleted at anytime. Local Secondary Indexes must be created when you first create the table and they cannot be modified or deleted.