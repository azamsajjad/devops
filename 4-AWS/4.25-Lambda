Lambda Charges: (pay only when your code executes)
1 - Based on Requests
first million requests per month are free 
$0.20 per month per 1 million requests 

2- Duration 
You are charged in millisecond increments 
price depend on amount of memory you allocate to your lambda function 
Price per GB-Second 
$0.00001667 per GB-Second 
A function that uses 512mb and runs for 100ms 
0.5GB x 0.1s = 0.05 GB-Seconds 
cost = $0.0000000083 
First 400,000 GB-Seconds per month are free

Lambda is Event-driven 
Lambda is Independent - each event will trigger a single function
Lambda is triggered by Events e.g. changes to s3 bucket/DB run function
Lambda is also triggered User Request e.g. alexa

LAMBDA TRIGGERS: (aws services)
DynamoDB
Kinesis 
SQS 
Application Load Balancer 
API Gateway 
Alexa 
CloudFront 
S3
SNS 
SES 
CloudFormation 
CloudWatch 
CodeCommit 
CodePipeline

When building and testing a function, you must specify three primary configuration settings: memory, timeout, and concurrency. These settings are important in defining how each function performs. Deciding how to configure memory, timeout, and concurrency comes down to testing your function in real-world scenarios and against peak volume. As you monitor your functions, you must adjust the settings to optimize costs and ensure the desired customer experience with your application.


Your Lambda function is billed based on runtime in 1-ms increments. Avoiding lengthy timeouts for functions can prevent you from being billed while a function is simply waiting to time out.

Duration is calculated from the time your code begins running until it returns or otherwise terminates, rounded up to the nearest 1 ms. Price depends on the amount of memory you allocate to your function, not the amount of memory your function uses. If you allocate 10 GB to a function and the function only uses 2 GB, you are charged for the 10 GB. This is another reason to test your functions using different memory allocations to determine which is the most beneficial for the function and your budget. 

The AWS Lambda Free Tier includes 1 million free requests per month and 400,000 GB-seconds of compute time per month.

The balance between power and duration:

Depending on the function, you might find that the higher memory level might actually cost less because the function can complete much more quickly than at a lower memory configuration.

You can use an open-source tool called Lambda Power Tuning to find the best configuration for a function. The tool helps you to visualize and fine-tune the memory and power configurations of Lambda functions. The tool runs in your own AWS account—powered by AWS Step Functions—and supports three optimization strategies: cost, speed, and balanced. It's language-agnostic so that you can optimize any Lambda functions in any of your languages. '

Having more than one invocation running at the same time is the function's concurrency.'



API : Application Programming Interface 
we use APIs to interact with web applications 
Applications use APIs to communicate with each other

API GATEWAY : is serverless, supports throttling, logged to CloudWatch
API GATEWAY provides endpoints for your apps running in aws
publish maintain and monitor APIs 
API TYPES :
RESTful APIs are optimized for stateless, serverless workloads
REST=REpresentational State Transfer, supports JSON
Websocket APIs are for real-time, two-way, stateful communication e.g. chat apps
users -> API GATEWAY -> Lambda
users -> API GATEWAY -> EC2
users -> API GATEWAY -> DynamoDB

$LATEST is the latest version of code you uploaded into lambda 
how to version control 
upload code > action > publish new version > create alias 
(weighted traffic routing also supported between versions)

Concurrent Execution Limit is 1000 functions per region per account
HTTP status code 429
you can also reserve some concurrency for critical lambda functions
they will be prioritized
but it also sets limit for that function.e.g if you reserve 500 simultaneous function execution for a particular function, it will never go beyond 500 concurrent runs

LAMBDA and VPC Access 
it is possible to enabe lambda access resources that are inside private VPC
these resources can be EC2/RDS/etc
Lambda creates ENIs using IP from private subnets
configuration > permissions > Execution role url
AWSLambdaVPCAccessExecutionRole (add this policy for lambda to access VPC)


---------------------------------------------------------------------------
Severless Architectures are Event-Driven and Asynchronous
->so an event may trigger an action, but no response is required/expected
SQS allows you to queue messages
EventBridge helps to handle events & route them to application components
Characteristics of Event-Driven Architecture 
        Event Source    --> Event Router -->    Event Destination
            s3              EventBridge         Lambda
            dynamoDB                            SNS
                                                Lambda
---------------------------------------------------------------------------
STEP Functions: 
provide visual interfac for serverless apps app is series of steps excuted in order as defined by your business logic
Output of one step can be input of next step
step functions also logs state of each step

STEP Functions Workflows
Standard Workflows: 
     -> Long-Running (upto 1 yar)
     -> At-Most-Once Model  - (tasks are never executed more than once, unless explicitly specified by retry actions.)
     -> Non-Idempotent Actions e.g. when processing payments, you want payment to be processed once
     -> Change in State? a req is non-idempotent if it always change state

Express Workflows:
    -> Short-Lived (upto 5 minutes)
    -> At-Least-Once 
    -> Idempotent Actions
    -> Identical Request - has no side effect
    2 types of Express Workflows
        -> Synchronous - begins workflow->wait till complete->return result
            workflows hold u up until they are complete
            e.g. job interview, successful payment b4 processing order
        -> Asynchronous - begins->confirm that its started->cloudwatch logs
            runs in the background
            e.g. email to a colleage, messaging system

---------------------------------------------------------------------------
LAMBDA STORAGE PATTERNS:

Lambda is stateless (you cannot permanently store any data in the function)
Lambda is Ephemeral (not used for apps that need to run 4longer than 15min)
To make Lambda Persistent -> make it interact with a data store.
                            e.g. S3,EFS,DynamoDB or /tmp,layers

/tmp , lambda layers = Native Lambda storage options
/tmp = by default 512mb, configurable upto 10GB (but NON_PERSISTENT)
/tmp = used while function is executing
/tmp = like a cached file system
/tmp = shared within execution environments

Storing Lambda Libraries - can be included with the code as zip file 
LAYERS                     but it makes deployment heavy
LAYERS = limit 50mb zipped, 250mb unzipped
LAYERS = shared across execution environments
- Best Practice (Add libraries & SDKs as layers)
    (However, if you want to change version of a library included)
    (you can''t update it dynamically)
    (you need to create a new layer & reference that)

PERSISTENT_STORAGE:
S3  -> elastic, means no size limit
    -> but there are constraints, because S3 is object storage 
    -> allows you to store & retrieve objects, not a file system 
    -> Cannot Append data 
    -> if you want to change data, you need to upload a new object
better option = EFS ~ Shared file system 
EFS = dynamically updated, no size limit
    = mounted by the function when execution environment is created 
    = can be shared across invocations 
    = VPC-to use EFS, lambda function must be in same VPC as EFS file system

---------------------------------------------------------------------------
LAMBDA ENVIRONMENT VARIABLES 
-> adjust your function behavior, without changing your code
-> make your function behave differently in dev env. than it does in prod.
-> key-value pairs (key=environment, value=development)
ENV Variables are locked once the Version is published 
ENV Variables are defined before Version is published
USE-CASES = references S3 resources, SNS Topic, DynamoDB Table
             BUCKET=my-bucket  SNSTOPIC=my-Topic  TABLE=my-table

---------------------------------------------------------------------------
LAMBDA INVOCATIONS 
when invoking a function, you can invoke it synchronously or asynchronously
SYNC Invocation -> lambda runs the function, wait for its response, returns the response
                -> the service calling the function will know if the function completed successfully or not
                -> e.g. API Gateway invoking a function and returning error code to the caller
ASYNC Invocation -> No Acknowledgement to let you know invocation was successful
                -> the service calling the function will not know if the function completed successfully or not
LAMBDA Retries:
default -> performs 2 retries 
lambda waits 1 min before first retry, it waits 2 minutes before 2nd retry

===========================================================
===========================================================
Error handling for synchronous and asynchronous events
===========================================================
In a synchronous invocation, like between API Gateway and Lambda, no retries are built in. You have to write code to handle errors and retries for all error types.  



With asynchronous event sources, like Amazon S3, Lambda provides built-in retry behaviors. 

When Lambda gets an asynchronous event, it handles it a lot like the SQS example we reviewed earlier. The Lambda service returns a success to the event source and puts the request in its own internal queue. Then it sends invocation requests from its queue to your function. 

If the invocation request returns function errors, Lambda retries that request two more times by default. You can configure this retry value between 0 and 2. 



If the invocation request returns invocation errors, Lambda retries that invocation for up to 6 hours. You can decrease the default duration using the maximum age of event setting. 

To handle events that continue to fail beyond the retry or maximum age settings, you can configure a dead-letter queue on the Lambda function. 

===========================================================
===========================================================

===========================================================================
===========================================================================
===========================================================================
Error handling with Amazon SQS as an event source
===========================================================================
For polling event sources that aren’t stream based, for example Amazon SQS, if an invocation fails or times out, the message is available again when the visibility timeout period expires.

Lambda keeps retrying that message until it is either successful or the queue’s Maxreceivecount limit has been exceeded. 

As noted earlier, it’s a best practice to set up a dead-letter queue on the source queue to process the failed messages.

When you’re building serverless applications, you need to execute performance tests, and adjust retries and timeouts to find the optimal combination that allows your processes to complete but doesn’t create bottlenecks that can cascade throughout the system.

Let’s go back to the connection between an Amazon SQS queue and Lambda as an example of how you manage timeouts across services.

You can set a timeout on your Lambda functions, and you can set the visibility timeout on SQS queues.

You can also set the batch size for the queue from 1 to 10 messages per batch, which can impact both your function timeout and your visibility timeout configurations.

You choose your Lambda timeout to allow the function to complete successfully under most circumstances.

You also want to consider at what point to give up on individual invocation to avoid additional costs or prevent a bottleneck.

A larger batch size can reduce polling costs and improve efficiency for fast workloads. But for longer running functions, you might need a lower batch size so that everything in the batch is processed before the function timeout expires.

For example, a batch size of 10 would require fewer polling processes and fewer invocations than a batch size of 3. 

If your function typically can process a message in 2 seconds, then a batch size of 10 would typically process in 20 seconds, and you might use a function timeout of 30 seconds. But if your function takes 2 minutes to process each message, then it would take 20 minutes to process a batch of 10.

However, the maximum timeout for Lambda is 15 minutes, so that batch would fail without processing all of its messages, and any messages in that batch that weren’t deleted by your function would again be visible on the queue. Which brings us back to the visibility timeout setting on the queue.

You need to configure the visibility timeout to allow enough time for your Lambda function to complete a message batch. So if we stick with the example of a batch size of 10 and a Lambda function that takes 20 seconds to process the batch, you need a visibility timeout that is greater than 20 seconds.

You also need to leave some buffer in the visibility timeout to account for Lambda invocation retries when the function is getting throttled. You don’t want your visibility timeout to expire before those messages can be processed. The best practice is to set your visibility timeout to 6 times the timeout you configure for your function.

You’ll find a summary table beneath the video, as well as links to the developer guide pages for each service’s relevant timeout features.
===========================================================================
===========================================================================
===========================================================================
Error handling for stream-based events
===========================================================================
A better way to manage failures is to modify the default behaviors using four configuration options introduced in 2019:

Bisect batch on function error
Maximum retry attempts
Maximum record age
On-failure destination
Bisect batch on function error tells Lambda to split a failing batch into two and retry each batch separately.

Maximum retry attempts and maximum record age let you limit the number or duration of retries on a failed batch.

And on-failure destination lets you send failed records to an SNS topic or SQS queue to be handled offline without having to add additional logic into your function.

Here’s an illustration of how these options work together. In this example, BisectOnFunctionError = True, MaximumRetryAttempts = 2, and DestinationConfig includes an OnFailure Destination that points to an SNS topic.

Assume you have a batch of 10 records, and the third record in this batch of 10 returns a function error. When the function returns an error, Lambda splits the batch into two, and then sends those to your function separately, still maintaining record order. Lambda also resets the retry and max age values whenever it splits a batch.

Now you’ve got two batches of five. Lambda sends the first batch of five and it fails. So the splitting process repeats. Lambda splits that failing batch yielding a batch of two and a batch of three records. Lambda resets the retry and max age values, and sends the first of those two batches for processing.

This time, the batch of two records processes successfully. So Lambda sends the batch of three to the function. That batch fails. Lambda splits it, and now it has a batch with one record (the bad one) and another with two records.

Lambda sends the batch with a bad record and it fails, but there’s nothing left to split.

So now the max retry and max age settings come into play. In this example, the function retries the record twice, and when it continues to fail, sends it to the SNS topic configured for the on-failure destination.

With the erroring record out of the way, Lambda works its way back through each of the smaller batches it created, always maintaining record order.

So Lambda is going to process the unprocessed batch of two, then the unprocessed batch of five. At that point, the original batch of 10 is marked as successful, and Lambda moves the pointer on the stream to the start of the next batch of records.

These options provide flexible error handling, but they also introduce the potential for a record to be processed multiple times. In the example, the first two records are processed before the function returns an error. Then they’re processed a second time in the smaller batch of five records, which also fails, and then they are processed a third time in their own batch.

This means you have to handle idempotency in your function rather than assuming only-once record processing.
========================================================================
========================================================================
========================================================================
========================================================================
Failed-event destinations
Use the Lambda destinations OnFailure option to send failures to another destination for processing. 
========================================================================
For both asynchronous and streaming event sources, you can specify an on-failure destination for a Lambda function. For asynchronous sources, you have the option of an SNS topic, SQS queue, EventBridge event bus, or another Lambda function. For streaming event sources, you can specify an SNS topic or an SQS queue.

The previous video gave an example of using an on-failure destination on a stream. Let’s go back to the order submission example to illustrate where you might use an on-failure destination with an asynchronous event.

The SendOrder Step Functions task kicks off the SNS fulfillment topic, whose subscribers handle additional fulfillment requirements. Let’s say that one of the subscribers is a Lambda function that decides if the order qualifies for a promotional gift. If it does, the function initiates steps to send the gift from a third-party system.

To handle potential failures caused by the third-party system, set the function’s on-failure destination equal to the ARN of an SNS topic that notifies the team responsible for fulfillment.

There are a couple of advantages to using an on-failure destination rather than using a dead-letter queue. First, the invocation record that is sent to the on-failure destination contains more data than the event object available to a dead-letter queue.

Second, it provides more flexibility to change or modify the failure behaviors. A dead-letter queue is part of a function’s version-specific configuration.  

You can also set on-success destinations to route successfully processed events without modifying your Lambda function code.
========================================================================
========================================================================
========================================================================
========================================================================
Step Functions for failure management
========================================================================
RETRY 
What if the call to the DynamoDB table fails when trying to retrieve the connection ID? To address this, you should include a Retry field with exponential backoff to retry the connection.


CATCH
What if the Lambda function fails to write the execution ARN and WebSocket URL? In this case, a retry wouldn’t resolve it. So you could use a Catch field to catch the error and transition to a fallback state.


LOOPING
What if the Step Functions work completes before the client has connected via the WebSocket API? The notification step wouldn't find the connection ID in the DynamoDB table. To address this type of issue, you could nest a looping pattern within the GetConn task that uses a Lambda function to execute the GetItem API call to the database a set number of times before failing that step.'
========================================================================
Which of these statements about AWS Step Functions States Language are true? (Select THREE.)

"
Task and Parallel states can have a field named Retry. An individual retrier represents a certain number of retries."

Correctly checked
Choice states can have a field named Catch. When a state reports an error that cant be resolved by retriers, Step Functions scans through the catchers to find the error name in the ErrorEquals field.

Correctly unchecked
You cant use a Retry or Catch field to handle timeouts.

"Correctly checked
The Amazon States Language defines a set of built-in strings that name well-known errors, all beginning with the States. prefix."

"Correctly checked
The reserved name States.ALL is a wildcard that matches any error name."

Correctly checked
Choice states can have a field named Retry. An individual retrier represents a certain number of retries.
========================================================================
========================================================================
========================================================================
========================================================================
========================================================================
Dead-Letter Queues for failure management
========================================================================
It is a best practice to enable dedicated dead-letter queues for individual Lambda functions that are invoked asynchronously. You can use Amazon SNS or SQS as dead-letter queues for Lambda functions.

You need to create the queue or SNS topic separately and then reference it on the function. Make sure that the Lambda execution role has permission to write to the topic or queue.

When you use SQS as an event source, you configure the dead-letter queue on the source queue, not the Lambda function. The primary distinction between a dead-letter queue on the source queue versus on a Lambda function is that a dead-letter queue attached to the source queue is actually part of the queue policy.

The policy describes how many times a message is retried before putting it on the dead-letter queue. This gives you visibility about the queue itself, regardless of its target. 

In addition to configuring the dead-letter queue, you can use the ApproximateAgeOfOldestMessage CloudWatch metric to alarm on a backup in the queue, and publish to an Amazon SNS topic for notification. When the dead-letter queue is configured on a Lambda function, messages that error out after the two built-in retries are routed to the dead-letter queue, where you can investigate the failure.

In either type of dead-letter queue, you need a mechanism to redrive the messages back to the original source.

AWS Event Fork Pipelines—available from the Serverless Application Repository—can facilitate this without you having to recreate something from scratch.
========================================================================
========================================================================
========================================================================
========================================================================
Distributed tracing with AWS X-Ray

Another important best practice for event-driven, decoupled applications is the use of distributed tracing to give you insight into issues or bottlenecks across your distributed architecture. 

Example: AWS X-Ray service graph

When X-Ray is enabled, it gets data from services as segments and groups them by request into traces. X-Ray then creates a service graph that gives you a visual representation of what’s happening at each service integration point, highlighting successful and failed service calls. To learn more, choose each numbered marker.

SERVICE GRAPH
Customers have reported issues using a service, and you need a quick sense of status.
TRACES
Youve identified failures at an integration point and want to review individual requests.
SUBSEGMENTS
You need a more granular breakout of the work done in a request to resolve the issue.
ANNOTATIONS
You want to be able to group traces across application operations to compare performance.
========================================================================
========================================================================
========================================================================
========================================================================
[DLQs and Destinations]

Dead-Letter Queues (DLQs):
save failed invocations for further processing 
associated with a particular version of a function 
can be an event source for a function, allowing you to re-process events.
handles failures only 
SQS->holds failed events in the queue until they are retrieved 
SNS->send notification about failed events to one or more destinations

Lambda Destinations: optionally, configure lambda to send invocation records to another service 
lambda --invocation_success-->EventBridge-->so successful inv are tracked
lambda --invocation_success-->SQS-->queue for review
lambda --invocation_failure-->SNS-->mail or sms
lambda --invocation_failure-->lambda-->trigger another function

LAB
create function 
create SNS topic -> add subscription -> add email
configuration -> destinations -> ASYNC & Condition & Destination Type
$ aws lambda invoke --function-name myfunction --invocation-type Event response.json
you will receive an email from SNS
// {"version":"1.0","timestamp":"2023-08-15T19:56:55.223Z","requestContext":{"requestId":"0d2b2356-82c9-4380-8fbf-ef268102c8e4","functionArn":"arn:aws:lambda:us-east-1:145794000460:function:myFunction1:$LATEST","condition":"Success","approximateInvokeCount":1},"requestPayload":{},"responseContext":{"statusCode":200,"executedVersion":"$LATEST"},"responsePayload":{"statusCode": 200, "body": "\"Hello from Lambda!\""}}
useful information

Now delete this destination 

go to configuration -> asynchronous invocation & enable SNS for DeadLetterQueue
you will receive an email from DLQ
// {}
No useful information for us but it let lambda know that there was failed invocation
---------------------------------------------------------------------------
LAMBDA DEPLOYMENT PACKAGE
when you paste code in lambda, lambda automatically creates a deployment package for you in .zip which includes your code and dependencies.

Other Method -> create deployment package yourself and upload zip file 
             -> limit is 50 mb
             -> if deployment package is greater than 50 mb 
             -> upload it to S3 in same region as u create your function
             -> then specify S3 object when u create your function 

Other Method -> Lambda Layers
             -> libraries, custon runtimes, etc 
             -> a layer can be used by multiple functions 
             -> helps reduce the size of deployment package 
             -> BEST PRACTICE
---------------------------------------------------------------------------
LAMBDA PERFORMANCE TUNING 
memory 128 mb to 10,240 mb
adding memory will improve function performance because with more memory, you get more cpu.
adding memory may reduce duration the function runs for
Steps:
DOWNLOADS CODE ---> CONFIGURE ---> STATIC INITIALIZATION ---> FUNCTION CODE
set ups execution   memory,         import libraries,sdks     tmp, re-use 
environment         runtime          (ADDS LATENCY)           execution env
                                                        for next function

How to optimize STATIC INITIALIZATION
three factors to reduce latency:
1- code - the amount of code that needs to run during initialization phase 
2- function package size 
3- performance - libraries/other services that require connections to be set up e.g. connections to S3 or database
e.g. dont import entire aws-sdk if your code can run on just one or two services 
instead of (aws-sdk), import (aws-sdk/clients/dynamodb)
---------------------------------------------------------------------------


API GATEWAYS (ADVANCED)

SOAP - legacy protocol - returns a response in XML format instead of JSON
        came out in 1990s - you can configure API Gateway as a SOAP web service passthrough - how?
        https://www.rubix.nl/blogs/how-configure-amazon-api-gatewaysoap-        webservice-passthrough-minutes
REST - Latest protocol

IMPORT APIs
You can use the API Gateway Import API feature to import an API from an
external definition file into API Gateway. Currently, the Import API feature supports Swagger v2.0 definition files. aka OpenAPI
With the Import API, you can either create a new API by submitting
a POST request that includes a Swagger definition in the payload and
endpoint configuration, or you can update an existing API by using a PUT
request that contains a Swagger definition in the payload. You can update
an API by overwriting it with a new definition, or merge a definition with an
existing API. You specify the options using a mode query parameter in the
request URL .
Import


API THROTTLING
By default, API Gateway limits the steady-state request rate to
10,000 requests per second (rps).
The maximum concurrent requests is 5000 requests across all
APIs within an AWS account.
If you go over 10,000 requests per second or 5000 concurrent
requests you will receive a 429 Too Many Request error
response.
API
Import API’s using Swagger 2.0 definition files
• API Gateway can be throttled
• Default limits are 10,000 RPS or 5000 concurrently
• You can configure API Gateway as a SOAP Webservice   
passthrough
--------------------------------------------------
API GATEWAY MOCK ENDPOINTS
Create, Test, Debug
backend is not ready yet but you want to test some new website features
e.g. cart,payment
allows team to continue development without depending on backend to be build
API GATEWAY RESPONSE - 
You definee the response 
status code and message 
forms the mock integration response 
---------------------------------------------------
API GATEWAY STAGEs 
references the lifecycle state of the API e.g. dev,prod 
Each stage can be associated with a different endpoint e.g. dev,prod
each stage has a unique invoke url e.g. 
https://dwncjrevn.execute-api.us-east-1.amazonaws.com/dev 
https://dwncjrevn.execute-api.us-east-1.amazonaws.com/prod
        API ID                                      Stage name

if we have 2 lambda functions, dev&prod, we can create API gateway with dev&prod stage 


LAB Creating Stages 
Create 2 functions stageDevevFunction,stageProdFunction with following code 
// export const handler = async (event) => {
//   // TODO implement
//   const response = {
//     statusCode: 200,
//     body: JSON.stringify('This is my Prod/Dev function'),
//   };
//   return response;
// };
goto API Gateway
BUILD REST API --> NewAPI & name=MyAPI -> Create API
ACTION->CREATE METHOD->"GET"->Lambda Function = ${stageVariables.lmbfunction}->
it gives you a command to run in cloudshell 
// aws lambda add-permission --function-name "arn:aws:lambda:us-east-1:965329023955:function:myprod/devfunction" --source-arn "arn:aws:execute-api:us-east-1:965329023955:nzxaisdv5m/*/GET/" --principal apigateway.amazonaws.com --statement-id 0e68dff5-42c2-4116-81f7-a26a4ae2a2bc --action lambda:InvokeFunction
save 
ACTION->DEPLOY API->newstage(test)
CREATE->prod 
STAGE VARIABLE->key=lmbfunction value=stageProdFunction
CREATE->dev
STAGE VARIABLE->key=lmbfunction value=stageDevFunction
-----------------------------------------------------
API RESPONSE TRANSFORMATIONS
AppFrontend->APIRequest->  APIGateway  ->APIRequest->AppBackend
                          modify Request

AppFrontend<-APIResponse<-  APIGateway  <-APIResponse<-AppBackend
                          modify Response

HTTP APIs:
parameter mapping is used to modify API requests and responses
we can change the Header, Query String, Request Path in API Request
we can change the Header, Status Code,               in API Response
-----------------------------------------------------
API GATEWAY CACHING:
CACHES your endpoints response (this reduces no. of calls made to endpoint)
TTL (when u enable caching, api gateway caches responses from endpoints for a specified TTL, default is 300 seconds)
API GATEWAY returns cached response to new requests, instead of making new request to endpoint. 
THIS REDUCES LATENCY

API GATEWAY THROTTLING is to prevent your API from being overwhelmed by too many requests
by-default, API Gateway limits the steady-state request rate to 10,000 requests per second,per region.
by-default, API Gateway limits the concurrent request to 5,000 across all APIs per second,per region.
if you exceed any of these limits, you get 429 ERROR (Too Many Requests)

Throttling Example 
5,000 requests in 1st millisecond 
5,000 requests evenly spread across 999 milliseconds
this is within limits of 10,000 req per second and 5,000 concurrent request
this will happen without any errors

------------------------------------------------------
X-Ray
X-Ray [Service Map] provides end-to-end view of API requests as they travel through your application
X-Ray can be integrated with EC2, ECS, Lambda, EB, SNS, SQS, DynamoDB, ELB, API Gateway, S3
X-Ray can be integrated with with your own application written in java, node.js, .net, go, ruby, python
X-Ray SDK automatically captures metadata for API calls made to AWS services using AWS SDK

Procedure:
Install X-Ray Agent on EC2 instance
Instrument your application using X-Ray SDK (sdk has libraries)
X-Ray sdk gathers informationfrom request and response headers, the code in your application and metadata about aws resources on which it runs and send this trace data to X-Ray e.g. HTTP requests, error codes, latency data

You need instrument(configure) both the X-Ray SDK and X-Ray Daemon on your systems
sdk sends data to daemon, which uploads them to X-Ray in batches

for docker container, install X-Ray daemon in its own docker container, and your application in its own container, all in same ECS cluster



---------------------------------------------------------------------------
You are a developer for a busy real estate company, and you want to enable other real estate agents to have the ability to show properties on your books, but skinned so that it looks like their own website. You decide the most efficient way to do this is to expose your API to the public using API Gateway. The project works well, but one of your competitors starts abusing this by sending your API tens of thousands of requests per second. This generates an HTTP 429 error. Each agent connects to your API using individual API keys. What actions can you take to stop this behavior?

Choose 2


Place an AWS Web Application Firewall (AWS WAF) in front of API Gateway and filter the requests


Deploy multiple API Gateways and give the agent access to another API Gateway


Use AWS Shield Advanced API protection to block the requests


Throttle the agent's API access using the individual API Keys

Good work!
AWS WAF helps protect your web applications or APIs against common web exploits that could impact availability, compromise security, or consume excessive resources.

To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. You can enable usage plans to restrict client request submissions to within specified request rates and quotas. This restricts the overall request submissions so that they don't go significantly past the account-level throttling limits in a Region. Amazon API Gateway provides Per-client throttling limits that are applied to clients that use API keys associated with your usage policy as client identifier.

=======================================================================
.
Which of the following are true about Amazon Simple Queue Service (Amazon SQS) and AWS Lambda? (Select TWO.)

If a Lambda function returns errors when processing messages, Lambda decreases the number of processes polling the queue.

Lambda has a default of five parallel processes to get things off of a queue.
=======================================================================

Which of these patterns for communicating status updates do you use to get status information on a long-running transaction?

WebSockets with AWS AppSync

Client polling pattern

Saga pattern with AWS Step Functions

Webhooks with Amazon Simple Notification Service (Amazon SNS)


Client polling is a common way to get status information on a long-running transaction. With the client polling, the client can use the ID returned from Amazon Simple Queue Service (Amazon SQS) to get the status of the request. 
=======================================================================
API Gateway (synchronous event source)
–
Timeout considerations – API Gateway has a 30-second timeout. If the Lambda function hasn't responded to the request within 30 seconds, an error is returned.
Retries – There are no built-in retries if a function fails to execute successfully.
Error handling – Generate the SDK from the API stage, and use the backoff and retry mechanisms it provides.'

Amazon SNS (asynchronous event source)
–
'Timeout considerations – Asynchronous event sources do not wait for a response from the function's execution. Requests are handed off to Lambda, where they are queued and invoked by Lambda.
Retries – Asynchronous event sources have built-in retries. If a failure is returned from a function's execution, Lambda will attempt that invocation two more times for a total of three attempts to execute the function with its event payload. You can use the Retry Attempts configuration to set the retries to 0 or 1 instead.

If Lambda is unable to invoke the function (for example, if there is not enough concurrency available and requests are getting throttled), Lambda will continue to try to run the function again for up to 6 hours by default. You can modify this duration with Maximum Event Age.

Amazon SNS has unique retry behaviors among asynchronous events based on its delivery policy for AWS Lambda(opens in a new tab). It will perform 3 immediate tries, 2 at 1 second apart, 10 backing off from 1 second to 20 seconds, and 100,000 at 20 seconds apart.
Error handling – Use the Lambda destinations(opens in a new tab) OnFailure option to send failures to another destination for processing. Alternatively, move failed messages to a dead-letter queue on the function. When Amazon SNS is the event source, you also have the option to configure a dead-letter queue on the SNS subscription(opens in a new tab).'

Kinesis Data Streams (polling a stream as an event source)
–
'Timeout considerations – When the retention period for a record expires, the record is no longer available to any consumer. The retention period is 24 hours by default. You can increase the retention period at a cost.

As an event source for Lambda, you can configure Maximum Record Age to tell Lambda to skip processing a data record when it has reached its Maximum Record Age.
Retries – By default, Lambda retries a failing batch until the retention period for a record expires. You can configure Maximum Retry Attempts so that your Lambda function will skip retrying a batch of records when it has reached the Maximum Retry Attempts (or it has reached the Maximum Record Age).
Error handling – Configure an OnFailure destination on your Lambda function so that when a data record reaches the Maximum Retry Attempts or Maximum Record Age, you can send its metadata, such as shard ID and stream Amazon Resource Name (ARN), to an SQS queue or SNS topic for further investigation.

Use BisectBatchOnFunctionError to tell Lambda to split a failed batch into two batches. Retry your function invocation with smaller batches to isolate bad records and work around timeout and retry issues.

For more information on these error handling features, see the blog post AWS Lambda Supports Failure-Handling Features for Kinesis and DynamoDB Event Sources(opens in a new tab).'

SQS queue (polling a queue as an event source)
–
'Timeout considerations – When the visibility timeout expires, messages become visible to other consumers on the queue. Set your visibility timeout to 6 times the timeout you configure for your function.
Retries – Use the maxReceiveCount on the queues policy to limit the number of times Lambda will retry to process a failed execution.
Error handling – Write your functions to delete each message as it is successfully processed. Move failed messages to a dead-letter queue configured on the source SQS queue.'