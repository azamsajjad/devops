Why is monitoring important for troubleshooting?

You should collect monitoring data from all parts of your AWS solution to help troubleshoot a multipoint failure if one occurs. Before you start monitoring Amazon Elastic Container Service (Amazon ECS), first create a monitoring plan to answer the following questions.

1
What are your monitoring goals?

2
What resources will you monitor?

3
How often will you monitor these resources?

4
What monitoring tools will you use?

5
Who will perform the monitoring tasks?

6
Who should be notified when something goes wrong?

========================================================================

CloudWatch
CloudTrail
X-Ray 
AWS Systems Manager
---------------------------
CloudWatch

IAM>roles>attach policy
CloudWatchAgentServerPolicy (permission to use CW agent on EC2)
>create role
Create instance with this attached role 

1. Bootstrap script: 
#!/bin/bash	
dnf update -y	

2. Log in to the instance and install rsyslog, which will generate a readable text file of the operating system messages in /var/log/messages. 

sudo su  
dnf install rsyslog
systemctl start rsyslog
systemctl enable rsyslog

3. Install the CW Agent: 
dnf install amazon-cloudwatch-agent -y

4. Configure the CW agent: 
/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard

****Say no to monitoring CollectD****
Monitor /var/log/messages

5. View the CloudWatch agent config file:
cd /opt/aws/amazon-cloudwatch-agent/bin
cat config.json

6. Start the CloudWatch Agent
/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json

7. Generate some activity on our system by installing stress:
dnf install stress -y
stress --cpu 1



----------------------------------------------
[root@ip-172-31-37-171 ec2-user]# /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard
================================================================
= Welcome to the Amazon CloudWatch Agent Configuration Manager =
=                                                              =
= CloudWatch Agent allows you to collect metrics and logs from =
= your host and send them to CloudWatch. Additional CloudWatch =
= charges may apply.                                           =
================================================================
On which OS are you planning to use the agent?
1. linux
2. windows
3. darwin
default choice: [1]:

Trying to fetch the default region based on ec2 metadata...
Are you using EC2 or On-Premises hosts?
1. EC2
2. On-Premises
default choice: [1]:

Which user are you planning to run the agent?
1. root
2. cwagent
3. others
default choice: [1]:

Do you want to turn on StatsD daemon?
1. yes
2. no
default choice: [1]:

Which port do you want StatsD daemon to listen to?
default choice: [8125]

What is the collect interval for StatsD daemon?
1. 10s
2. 30s
3. 60s
default choice: [1]:

What is the aggregation interval for metrics collected by StatsD daemon?
1. Do not aggregate
2. 10s
3. 30s
4. 60s
default choice: [4]:
2
Do you want to monitor metrics from CollectD? WARNING: CollectD must be installed or the Agent will fail to start
1. yes
2. no
default choice: [1]:
2
Do you want to monitor any host metrics? e.g. CPU, memory, etc.
1. yes
2. no
default choice: [1]:

Do you want to monitor cpu metrics per core?
1. yes
2. no
default choice: [1]:

Do you want to add ec2 dimensions (ImageId, InstanceId, InstanceType, AutoScalingGroupName) into all of your metrics if the info is available?
1. yes
2. no
default choice: [1]:

Do you want to aggregate ec2 dimensions (InstanceId)?
1. yes
2. no
default choice: [1]:

Would you like to collect your metrics at high resolution (sub-minute resolution)? This enables sub-minute resolution for all metrics, but you can customize for specific metrics in the output json file.
1. 1s
2. 10s
3. 30s
4. 60s
default choice: [4]:
1
Which default metrics config do you want?
1. Basic
2. Standard
3. Advanced
4. None
default choice: [1]:
2


Are you satisfied with the above config? Note: it can be manually customized after the wizard completes to add additional items.
1. yes
2. no
default choice: [1]:
1
Do you have any existing CloudWatch Log Agent (http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html) configuration file to import for migration?
1. yes
2. no
default choice: [2]:
2
Do you want to monitor any log files?
1. yes
2. no
default choice: [1]:

Log file path:
/var/log/messages
Log group name:
default choice: [messages]

Log stream name:
default choice: [{instance_id}]

Log Group Retention in days
1. -1
2. 1
3. 3
4. 5
5. 7
6. 14
7. 30
8. 60
9. 90
10. 120
11. 150
12. 180
13. 365
14. 400
15. 545
16. 731
17. 1096
18. 1827
19. 2192
20. 2557
21. 2922
22. 3288
23. 3653
default choice: [1]:
2
Do you want to specify any additional log files to monitor?
1. yes
2. no
default choice: [1]:
2
Saved config file to /opt/aws/amazon-cloudwatch-agent/bin/config.json successfully.


[root@ip-172-31-37-171 ec2-user]# cd /opt/aws/amazon-cloudwatch-agent/
bin/  doc/  etc/  logs/ var/  
[root@ip-172-31-37-171 ec2-user]# cd /opt/aws/amazon-cloudwatch-agent/bin
[root@ip-172-31-37-171 bin]# ll
total 348668
-rw-r--r--. 1 root root        11 Jul 18 21:53 CWAGENT_VERSION
-rwxr-xr-x. 1 root root 122814024 Jul 18 21:53 amazon-cloudwatch-agent
-rwxr-xr-x. 1 root root  12153768 Jul amazon-cloudwatch-agent-config-wizard
-rwxr-xr-x. 1 root root     14107 Jul 18 21:53 amazon-cloudwatch-agent-ctl
-rwxr-xr-x. 1 root root  11584824 Jul 18 21:53 config-downloader
-rwxr-xr-x. 1 root root 109939464 Jul 18 21:53 config-translator
-rw-r--r--. 1 root root      1512 Aug 19 22:23 config.json
-rwxr-xr-x. 1 root root 100508040 Jul 18 21:53 start-aws-cloudwatch-agent
[root@ip-172-31-37-171 bin]# cat config.json 

========================================================================
CloudWatch pricing

With Amazon CloudWatch, there is no up-front commitment or minimum fee. You pay for what you use, and you will be charged at the end of the month for that usage. A good best practice is to understand when charges occur using CloudWatch. For example, if you have a single CloudWatch alarm with multiple metrics, you are charged for each metric associated with a CloudWatch alarm. 

Charges are also incurred:

When you exceed three dashboards with up to 50 metrics.
By ingesting and storing logs, as well as the amount of ingested logs scanned for each CloudWatch Insights query.
Based on the number of custom events.
Charges are also incurred when you monitor more than 10 custom metrics. Custom metrics can be metrics you create and also metrics from tools such as the CloudWatch agent. Metrics collected by the CloudWatch agent are billed as custom metrics. 

Recommendations to cost optimize your account:

Only use detailed monitoring when needed.
Remove any unnecessary alarms.
Disable monitoring of custom metrics.
Delete unnecessary dashboards.
Stop ingesting logs that are not needed.
Run queries for shorter durations.  
Most AWS services automatically send metrics to CloudWatch for free. Many AWS accounts and applications should be able to operate within these AWS Free Tier limits. If not, you can use the AWS Pricing Calculator to estimate your monthly bill; however, know that pricing in AWS varies by:

AWS Regions
CloudWatch metric API calls
Data transferred
------------------------------------------------------------------------
CloudWatch Metrics
CloudWatch Namespaces
CloudWatch Dimensions 
CloudWatch Dashboard
allows you to add multi-region resources 
remember to save


CloudWatch Metrics
A metric is a variable to monitor
CloudWatch metrics consists
of a time-ordered sequence of values or data points,
which are published to CloudWatch.
Each data point in a metric has a timestamp
and optionally, a unit of measurement.
For example, think about the CPU usage on an EC2 instance
and the unit of measurement will be a percentage.


CloudWatch Namespaces
CloudWatch metrics are uniquely defined by a name,
a namespace, and zero or more dimensions.
So what do we mean by that?
Well, a namespace is simply a container
for CloudWatch metrics.
For example, EC2 uses the AWS/EC2 namespace.
And you can create your own namespaces
to publish custom metric data.
So when you want to publish custom metric data,
you must specify the namespace for each data point or value
that you want to publish to CloudWatch.
And you specify the name of the namespace
when you create the metric,
and metrics from different namespaces are
completely isolated from each other.
So they are not aggregated.
So metrics from different applications are not aggregated
into the same set of statistics.
So this means that you can keep all the metrics
that relate to a specific application in its own namespace.

CloudWatch Dimensions 
and a CloudWatch dimension is just like a filter
and its a name/value pair that can be used
to filter your CloudWatch data.
For example, you can use the "InstanceId dimension"
to search for metrics relating to a specific EC2 instance.
And CloudWatch can also aggregate data
across dimensions for some services.
For example, you can review the EC2 metrics
across all of your instances.
So from the console,
if you search for CPU metrics in the AWS/EC2 namespace,
without specifying a dimension,
CloudWatch can provide aggregate data
across all the instances

---------------------------------------------------------------------
You can use CloudWatch to do the following:

Detect anomalous behavior in your environments.

Set alarms to alert you when something is not right.

Visualize logs and metrics with the AWS Management Console.

Take automated actions like scaling.

Troubleshoot issues.

Discover insights to keep your applications healthy.CloudTrail
user-activity
---------------------------------------------------------------------
CloudTrail is all about recording API calls
for your AWS account, so it's going to have
an API activity history, related to the creation,
deletion, and modification of AWS resources,
and it will also record any failed logins as well.
So ask yourself, do you need an audit log
of user activity in your AWS account?
And if the answer is yes then it's CloudTrail.
So just remember that CloudWatch is all about performance
and CloudTrail is an audit trail.


-----------------------------------------------------------------------
CloudWatch Actions 
Actions allow you to publish, monitor and alert a variety of metrics
PutMetricData
PutMetricAlarm


dummy error to create alarms
#!/bin/bash
# update the timestamp to today's date and an appropriate time using the following format YYYY-MM-DD HH:MM:SS
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:05:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:06:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:07:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:08:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:09:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:10:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:11:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:12:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:13:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:14:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:15:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:16:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:17:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:18:00.000Z 
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:19:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:20:00.000Z
aws cloudwatch put-metric-data --metric-name CriticalError --namespace MyService --value 1 --timestamp 2023-08-19T11:21:00.000Z

'After modifying and running the script, create an alarm that will be triggered if more than 1 CriticalError data point is received by CloudWatch within any time period of 5 minutes:

aws cloudwatch put-metric-data \
--metric-name PageViewCount \
--namespace MyService \
--value 25 \
--timestamp 2022-01-10T12:00:00.000Z

# pageviewcount exceeds a threshold of 50 with 300seconds
aws cloudwatch put-metric-alarm \
--alarm-name PageViewMon \
--alarm-description "PageView Monitor" \
--metric-name PageViewCount \
--namespace Myservice \
--statistic Average \
--period 300 \
--threshold 50 \
--comparison-operator GreaterThanThreshold \
--evaluation-periods 1

# pageviewcount exceeds a threshold of 1 within 300seconds
aws cloudwatch put-metric-alarm \
--alarm-name CriticalErrorMon \
--alarm-description "CriticalError Monitor" \
--metric-name CriticalError \
--namespace MyService \
--statistic SampleCount \
--period 300 \
--threshold 1 \ 
--comparison-operator GreaterThanThreshold \
--evaluation-periods 1


---------------------------------------------------------------------------
CLOUDWATCH LOGS.
Log events
–
Log events are a record of some activity recorded by the application or resource being monitored. The log event record that CloudWatch Logs understands contains two properties: the timestamp of when the event occurred, and the raw event message. Event messages must be UTF-8 encoded.


Log streams
–
Log streams are a sequence of log events that share the same source. More specifically, a log stream is generally intended to represent the sequence of events coming from the application instance or resource being monitored. 


Log groups
–
Log groups define groups of log streams that share the same retention, monitoring, and access control settings. Each log stream has to belong to one log group. For example, if you have a separate log stream for the Apache access logs from each host, you can group those log streams into a single log group called MyWebsite.com/Apache/access_log.
There is no limit on the number of log streams that can belong to one log group.


Metric filters
–
Metric filters can extract metric observations from ingested events and transform them to data points in a CloudWatch metric. Metric filters are assigned to log groups and all of the filters assigned to a log group are applied to their log streams.


Retention settings
–
Retention settings can be used to specify how long log events are kept in CloudWatch Logs. Expired log events get deleted automatically. Just like metric filters, retention settings are also assigned to log groups, and the retention assigned to a log group is applied to its log streams. By default, log data is stored in CloudWatch Logs indefinitely. However, you can configure how long to store log data in a log group. Any data older than the current retention setting is deleted automatically. You can change the log retention for each log group at any time. You can also export log data from your log groups to an Amazon Simple Storage Service (S3) bucket and use this data in custom processing and analysis, or to load onto other systems.
---------------------------------------------------------------------------
CLOUDWATCH LOGS INSIGHT.
Note: CloudWatch Logs Insights queries incur charges based on the amount of data that is queried.
A single request can query up to 20 log groups. Queries time out after 15 minutes, if they have not completed. Query results are available for 7 days.


Perform queries to help you more efficiently and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.


1- Create a Basic Lambda Function:
a new log group is automatically created, enabling it to send logs to cloudwatch

we can use CloudWatch Logs Insights
to interactively query and analyze our data
that is stored in CloudWatch Logs.
We can query the logs directly
and we can also generate visualizations like bar graphs,
line graphs, pie charts, and stacked areas as well.
And they also give you loads of cool example queries.
For instance,
you can display the 25 most recently added log events.

Search your VPC flow logs
to find out which IP addresses are using a specific protocol
or even find the most expensive Lambda requests.



QUERY LANGUAGE:
Includes a purpose-built query language with a few simple but powerful commands. 
Provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started. Sample queries are included for several types of AWS service logs.
Supports a query language you can use to perform queries on your log groups. Each query can include one or more query commands separated by Unix-style pipe characters (|).
Six query commands are supported, along with many supporting functions and operations, including regular expressions, arithmetic operations, comparison operations, numeric functions, datetime functions, string functions, and generic functions.

Comments are also supported.
Lines in a query that start with the # character are ignored.
Fields that start with the @ symbol are generated by CloudWatch Logs Insights.

TYPES OF LOGS:
CloudWatch Logs Insights supports all types of logs. 
For every log sent to CloudWatch Logs, five system fields are automatically generated:

>@message contains the raw unparsed log event. This is equivalent to the message field in InputLogevent.
>@timestamp contains the event timestamp contained in the log events timestamp field. This is equivalent to the timestamp field in InputLogevent.
>@ingestionTime contains the time when the log event was received by CloudWatch Logs.
>@logStream contains the name of the log stream that the log event was added to. Log streams are used to group logs by the same process that generated them.
>@log is a log group identifier in the form of account-id:log-group-name. This can be useful in queries of multiple log groups, to identify which log group a particular event belongs to.
CloudWatch Logs Insights inserts the @ symbol at the start of fields for logs it automatically processes. Logs not automatically processed by CloudWatch can be processed by using parse command to extract and create ephemeral fields for use in that query.

SCHEDULE A QUERY:
To schedule a query of a log group using CloudWatch Logs Insights, specify the: 

Log group
Time range to query
Query string to use


USEFUL QUERIES:
Top 15 packet transfers across your hosts.
stats sum(packets) as packetsTransferred by srcAddr, dstAddr    
| sort packetsTransferred  desc   
| limit 15


Top 15 byte transfers for hosts in a given subnet.
filter isIpv4InSubnet(srcAddr, "192.X.X.X/24")    
| stats sum(bytes) as bytesTransferred by dstAddr    
| sort bytesTransferred desc    
| limit 15


IP addresses that use specific protocols.
filter protocol=6 | stats count(*) by srcAddr 
This is for TCP protocol.  Replace the 6 with 17 for UDP, and so on. 
For more information: Protocol Numbers


IP addresses that skipped flow log records during the capture window.
filter logStatus="SKIPDATA"   
| stats count(*) by bin(1h) as t    
| sort t


10 DNS resolvers with the highest number of requests.
stats count(*) as numRequests by resolverIp    
| sort numRequests desc   
| limit 10


25 most recently added log events.
fields @timestamp, @message | sort @timestamp desc | limit 25
List of log events that are not exceptions.

---------------------------------------------------------------------------
CLOUDWATCH ALARMS:
Create up to 5,000 alarms per Region per AWS account.
Only use ASCII characters for alarm names.
Receiving Notifications with CloudWatch
1- launch EC2 instance 
2- Create CloudWatch Alarm 
3- Configure Email Alert 
4- Max Out CPU

cpu utilization 99% script
for i in 1 2 3 4; do while : ; do : ; done & done

Metric alarm states

When you create a CloudWatch alarm that watches a single CloudWatch metric, an alarm invokes actions only when the alarm changes state.

The alarm can be in one of three states:

OK means the metric is within the defined threshold.
ALARM means the metric is outside the defined threshold.
INSUFFICIENT_DATA means enough information has yet to be gathered that can determine whether the metric is within or outside of the threshold range.
For each alarm you create, you can specify CloudWatch to treat missing data points as: 

notBreaching – Missing data points are treated as good and within the threshold.
breaching – Missing data points are treated as bad and breaching the threshold.
ignore – The current alarm state is maintained.
missing – If all data points in the alarm evaluation range are missing, the alarm transitions to INSUFFICIENT_DATA.


"Recommendations of alarms to configure"
ALARM TRIGGERS:
Ensure that a CloudWatch alarm is initiated each time a configuration change is made to:


VPC internet gateway or a VPC VPN customer gateway---------------------
+
When a VPC internet gateway or a VPC VPN customer gateway configuration change is made, the CloudWatch alarm should fire every time an AWS API call is performed to create, update, or delete a VPC customer or internet gateway.

Network access control list (network ACL)------------------------------
+
When a network access control list (network ACL) configuration change is made, the CloudWatch alarm should fire every time an AWS API call is performed to create, update, or delete a network ACL.

Security group---------------------------------------------------------
+
When a security group configuration change is made, the CloudWatch alarm should fire every time an AWS API call is performed to create, update, or delete a security group.

CloudTrail-------------------------------------------------------------
+

Root account-----------------------------------------------------------
+
When the root account is used, the CloudWatch alarm should be initiated every time an AWS root account is used.

VPC Flow Logs----------------------------------------------------------
+
Ensure that a CloudWatch alarm is created and configured for the metric filter attached to the VPC Flow Logs CloudWatch log group to receive notifications when IP packets are rejected inside the specified VPC. The CloudWatch alarm must be configured to watch the VPC Flow Logs metric filter over a specified period and perform an action based on the value of the metric relative to a given threshold over a number of time periods. 
Ensure that a metric filter that matches the pattern of the rejected traffic is created for the CloudWatch log group assigned to VPC Flow Logs. VPC Flow Logs is a feature that records information about the IP traffic (accepted, rejected, or all traffic) going to and from the elastic network interfaces (ENIs) available within your VPC. The captured log data is stored using CloudWatch Logs service. You can manage flow log records as you would with any other log events collected by CloudWatch Logs. A metric filter defines the terms and patterns to look for in the flow log data as this is sent to CloudWatch Logs. CloudWatch uses this metric filter to turn log data into numerical metrics that you can graph or set an alarm on.


Alarm events and EventBridge

CloudWatch sends events to Amazon EventBridge whenever a CloudWatch alarm changes alarm state. You can use EventBridge and these events to write rules that take actions, such as notifying you, when an alarm changes state
---------------------------------------------------------------------------
EventBridge
easily configure event-driven systems
Eventbridge is all about event-bridge architecture
An event is a change in state

AwsConfig -events-> EventBridge (Rules) (Target) ->Actions-> SNS email
CloudTrail -events-> EventBridge (Rules) (Target) ->Actions-> triggerLambda
CloudWatch -events-> EventBridge (Rules) (Target) ->Actions-> Shutdown EC2


Scheduled Events: 
EventBridge rules can also run on a schedule
e.g. once a hour or day, or using a cron expression,
we can set a rule to run at the same time on a specified day,week,month

similar to CloudWatch Events 
but AWS recommends using EventBridge 
both use same underlying service and API but EventBridge has more features

Changes you make in either CloudWatch or EventBridge will appear in each console


configure event 

                        Event pattern
                        Event source
                        AWS service or EventBridge partner as source

                        AWS services
                        AWS service
                        The name of the AWS service as the event source

                        EC2
                        Event type
                        The type of events as the source of the matching pattern

                        EC2 Instance State-change Notification
                        Event Type Specification 1
                        Any state
                        Specific state(s)
                        

                        Event Type Specification 2
                        Any instance
                        Specific instance Id(s)

---------------------------------------------------------------------------
Common error codes HTTP/s 
Client-Side Errors 
4xx
400 - access denied exception
403 - missing authentication token - req didnt contain valid x.509 certtificate or AWS access key ID 
404 - malformed query string - syntax error in query string 
object doesnt exist or file not found
Always Your Fault - user


Server-side Errors 
5xx
500- internal failure - req failed due to unknown error, failure, exception
503- service unavailable - temporary failure of the server 
b/c of high traffic on website or internal failure

---------------------------------------------------------------------------
Common SDK Exceptions when using DynamoDB 
SDK Exception? -> 
A response to an error that occured when processing SDK or API Request

Common Exceptions with BatchGetItems?
BatchGetItems is limited to 16mb of data and upto 100 items
or request exceeded the provisioned throughput of the table
if dynamoDB cant return all items, it returns partial result, alongwith exception
Example errors for BatchGetItems:
    ValidationException - too many items requested
    UnprocessedKeys-when 1 item was processed successfully, reduce req size
    ProvisionedThroughputExceededException - no items processed 
    then add capacity e.g. add DAX

Common Exceptions with BatchWriteItems?
BatchWriteItems is limited to 16mb of data & upto 25 put/delete operations
Example errors for BatchWriteItems:
    UnprocessedItems - some operations failed, retry unprocessed items
    ProvisionedThroughputExceededException - no items processed 
    then add write capacity units



--------------------------------------------------------------------
Which of the following CloudWatch actions enables you to publish metric data points to CloudWatch?


PutMetricData


PutDashboard


PublishMetricData


PutMetricAlarm

Good work!
PutMetricData publishes metric data points to CloudWatch.


-----------------------
An application support team would like to receive an email notification whenever a particular server experiences CPU utilization exceeding 85% for 5 minutes or more. How can you configure this?


Create a CloudWatch alarm that will trigger if the CPU Utilization metric exceeds the threshold for 5 minutes. Configure CloudWatch to send an SNS notification if the alarm is triggered. 


---------------------------------------------------------------------------



---------------------------------------------------------------------------



---------------------------------------------------------------------------



---------------------------------------------------------------------------

X-Ray helps developers Analyze and Debug applications that utilize microservice architecture
X-Ray
X-Ray is a Distributed Tracing System 
X-Ray [Service Map] provides end-to-end view of API requests as they travel through your application
X-Ray can be integrated with EC2, ECS, Lambda, EB, SNS, SQS, DynamoDB, ELB, API Gateway, S3
X-Ray can be integrated with with your own application written in java, node.js, .net, go, ruby, python
X-Ray SDK automatically captures metadata for API calls made to AWS services using AWS SDK

Procedure:
Install X-Ray Agent on EC2 instance
Instrument your application using X-Ray SDK (sdk has libraries)
X-Ray sdk gathers informationfrom request and response headers, the code in your application and metadata about aws resources on which it runs and send this trace data to X-Ray e.g. HTTP requests, error codes, latency data

You need instrument(configure) both the X-Ray SDK and X-Ray Daemon on your systems
sdk sends data to daemon, which uploads them to X-Ray in batches

for docker container, install X-Ray daemon in its own docker container, and your application in its own container, all in same ECS cluster

================================ = = = = = = = = = = = = = = = = = = = =
ANATOMY:
X-Ray Daemon -------------> Ray API ------------------->  X-Ray console   
  ^           ^                 ^
  |             \               |
X-Ray SDK          X-Ray SDK & X-Ray CLI

X-Ray SDK :
Provides -> Interceptors to add to ur code to trace incoming HTTP requests
         -> Client Handlers to "instrument" AWS SDK clients that ur app uses to call other aws services
         -> HTTP client to instrument calls to other internal & external HTTP web services 
         -> SDK also supports "instrument" calls to SQL dbs, automatic AWS SDK client instrumentation, and other features.


= = = = = = = = = = = = = = = = = = = =
-----------------What is Instrumenting ? e.g. nodejs app----------------
The Ability to Monitor or Measure the level of product's performance, to diagnose errors, and to write trace information'

var app = express();
var AWSXRay = require('aws-xray-sdk');

app.use(AWSXRay.express.openSegment('MyApp'));  <----------------------

app.get('/', function (req, res) {
    res.render('index');
});

app.use(AWSXRay.express.closeSegment());        <----------------------
= = = = = = = = = = = = = = = = = = = = = =

-----------------------"How it works ?"-----------------------------
SDK sends JSON ___segment documents to a daemon process listening for UDP traffic
|
X-Ray  Daemon buffers segments in a queue and uploads them to X-Ray in batches
"X-Ray Daemon uses UDP port 2000"
|
> Daemon is avl for linux,mac,windows,EB,lambda 
> X-Ray uses trace data from aws resources that power ur app to generate a detailed service graph

-------------------------CONCEPTS----------------------------

-------------------------"Segments"
Compute resources running on your aplication logic /send data/ about their work as "Segments"
X-Ray receives data from services as "Segments"
segment = host info, request info, response codes, work done times, issues
|
|
-------------------------"SubSegemnts"
more granular timing information and details about downstream calls that your app made to fullfil the original request
-------------------------"Sampling"
sampling is an algorithm that decides which requests should be traced. by default, X-Ray Sdk Records The First Request Each Second And 5% Of Any Additional Request
if you cant find an info you expected to see in traces, it was sampled out
-------------------------"Traces"
A "Trace" collects all segments generated by a single request
A "Trace ID" tracks the path of a request through your application
->   The First supported service that HTTP request interacts with , adds 
     a "Trace ID header" to the request, and propagates it downstream to, track the latency, disposition, and other request data
->   A Trace Header can include a parent if the request originated from an instrumented application

All requests are traced, upto a configurable minimum

The "Sampling Decision" & "Trace ID" are added to "HTTP Requests" in "Tracing Headers" named "X-Amzn-Trace-Id"
|
|
|
|
|
|
X-Ray groups segments that have common request into "Traces"
X-Ray processes traces to generate "Service Graph" that provide visuals 
|
--------------------------"Service Graph"
Purpose: identifying bottlenecks, latency spikes, other issues
clients ----- front-end services (sns,lambda)------ back-end services (db)
|
|
|
--------------------------"Filter Expression"
Even with Sampling, a complex application generates a lot of data
You can use "Filter Expressions" to narrow down specific paths or users
--------------------------"Groups"
You can "Group" these results by using filter expressions 
You can also assign filter expressions to "Group" you create
You can CALL these groups by name or ARN to generate ur own service graphs, trace summaries and amazon cloudwatch metrics 
KEEP in mind, updating groups filter expression doesnt change processd data
--------------------------"Annotations and Metadata"
You can add other information to the segment document as annotations and metadata. A & M are aggregated at Trace Level & can be added to any segment/subsegment
Annotation = key value pair which is indexed for use with filter expression
             X-Ray indexes upto 50 annotations per trace
             use A to record data you want to use to group traces or,
             when calling the GetTraceSummaries API
Metadata = key-value pairs that are not indexed 
            values can be of any type , including objects & lists
-------------------------"Errors and Exceptions"
when exceptions happen while your app is serving an instrumented request, 
The X-Ray SDK records exception details and the (stack trace) (if avl)
--------Errors      4XX 
--------Fault       5XX 
--------Throttle    429 too many requests

-----------------------------"Integrations"-------------------------------
Lambda,ApiG,Fargate,ELB,SNS,SQS,CloudTrail,CloudWatch,EC2,ECS,EB,aws config
-----------------------------"Languages"----------------------------------
go,java,nodejs,python,ruby,php,.net         no powershell



---------------------------------------------------------------------------



---------------------------------------------------------------------------



---------------------------------------------------------------------------



---------------------------------------------------------------------------
AWS SYSTEMS MANAGER



AWS Systems Manager Agent (SSM Agent)

The SSM Agent is Amazon software that can be installed and configured on an Amazon Elastic Compute Cloud (Amazon EC2) instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources.

AWS Systems Manager (formerly known as SSM) is an AWS service that you can use to view and control your infrastructure on AWS. 

Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. 

Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. For more hands-on information: AWS Systems Manager Workshop.

When you install the SSM Agent onto a server, Amazon EC2 instance, or virtual machine, that server becomes a managed instance. A managed instance is a machine that has been configured for use with Systems Manager. Systems Manager also helps you configure and maintain your managed instances. 

If you monitor traffic, you will see your Amazon Elastic Compute Cloud (Amazon EC2) instances, any on-premises servers, or virtual machines in your hybrid environment, communicating with ec2messages.* endpoints. AWS 



Systems Manager Agent (SSM Agent):
Writes information about executions, commands, scheduled actions, errors, and health statuses to log files on each instance. 
Logs files by manually connecting to an instance or automatically send logs to CloudWatch Logs.
You can also store agent configuration settings in the Systems Manager Parameter Store for use with the CloudWatch Agent. Parameter Store is a capability of AWS Systems Manager.



Using the SSM Agent:
SSM Agent processes requests from the Systems Manager service in AWS and configures your machine as specified in the request. You can manage servers without having to log in to them using automation. SSM Agent sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).

You can improve the security posture of your managed instances (including managed instances in your hybrid environment) by configuring Systems Manager to use an interface VPC endpoint in Amazon VPC. An interface VPC endpoint (interface endpoint) helps to connect to services powered by AWS PrivateLink, a technology that enables you to privately access Amazon Elastic Compute Cloud (Amazon EC2) and Systems Manager APIs by using private IP addresses. 

AWS PrivateLink restricts all network traffic between your managed instances, Systems Manager, and Amazon EC2 to the Amazon network. This means that your managed instances don't have access to the internet. If you use AWS PrivateLink, you don't need an internet gateway, a network address translation (NAT) device, or a virtual private gateway. You aren't required to configure AWS PrivateLink, but it's recommended.



Other uses for the SSM Agent:

Use SSM to connect to your hosts, not exposing port 22.

Log SSM API calls with CloudTrail.

Sending instance logs to CloudWatch Logs.

Configure CloudWatch Logs for Run command.

Monitor Run Command metrics using CloudWatch.



SSM Agent compared to CloudWatch Agent:

Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time consuming. A best practice is to configure the SSM Agent to automatically send its log data to a log group in CloudWatch Logs for analysis. You can configure and use the CloudWatch Agent to collect metrics and logs from your instances instead of using Systems Manager Agent (SSM Agent) for these tasks. 

For more efficient monitoring, the CloudWatch Agent gathers more metrics on EC2 instances and on-premises servers than are available using SSM Agent. Using CloudWatch Logs, you can monitor log data in real time, search and filter log data by creating one or more metric filters, and archive and retrieve historical data when you need it. 


CloudWatch Logs provides the following benefits:

Centralized log file storage for all SSM Agent log files

Quicker access to files to investigate errors

Indefinite log file retention (configurable)

Ability to maintain and access logs regardless of the status of the instance

Access to other CloudWatch features such as metrics and alarms




Systems Manager pricing:

With AWS Systems Manager, you pay for what you use on the following priced features. There are no minimum fees or up-front commitments.