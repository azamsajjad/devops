CodeCommit
CodeDeploy
CodePipeline
CodeArtifact
Elastic Container Service 
CloudFormation 
--sam 
--nested stacks 
Cloud Development Kit 
Amplify 

----------------------------------------

CI/CD 
Small incremental changes
it all starts with 
CI Shared Code Repository: CodeCommit (Private Source and Version Control)
CDEL Automated Build: CodeBuild (produces packages)
Automated Test:
Code is Merged
Prepared for Deployment:
CD Continous Delivery - Manual Delivery:
CD Continous Deployment - Automated Delivery: CodeDeploy (on EC2,Lambda)
CDEP Managing Entire Workflow: CodePipeline (end-to-end)

"CodeCommit" Repository:
store source code, binaries, libraries - allows input from multiple sources
same as git 
create user to use as [aws configure] - with CodeCommitFullAccess
create access key IAM
    $aws configure
create git key from IAM to be used for 
    $aws codecommit create-repository --repository-name devopsfromcli
    username:
    password:
---------------------------------------------------------------------------
"CodeDeploy":
2 deployment approaches -
in-place deployment : rolling update - app stopped on each instance one by one and is updated.
if you change your mind after deployment, no easy fix, redeploy old version
no lambda support

blue-green deployment : new release installed on new instances 
blue(active) 
green(new release)
u pay for 2 environments - old and new uptil you switch and delete old
CodeDeploy install the new verion as "REVISION"

CodeDeploy AppSpec File 
configuration file with parameters which will be used during CodeDeploy deployment
for EC2 and on-premise systems - YAML only 
Lambda based deployment with ec2 - YAML or JSON
AppSpec file structure 
   - version 
   - OS
   - files (define location of app files)
   - hooks (lifecycle event hoosts) (scripts) e.g. unzip, run tests, to deal with re-register and re-register instances with a load balancer
Typical Folder Setup (root)
appspec.yml     /Scripts   /Config     /Source
---
version: 0.0
os: linux 
files: 
    - source: Config/config.txt 
      destination: /webapps/config
    - source: Source
      destination: /webapps/myApp
hooks:
  BeforeInstall:
    - location: Scripts/unzipresourcebundle
    - location: Scripts/unzipdatabundle
  AfterInstall:
    - location: Scripts/unzipdatabundle
      timeout: 100
  ApplicationStart:
    - location: Scripts/runFunctionTest.sh 
      timeout: 3600
  ValidateService:
    - location: Scripts/unzipdatabundle
      timeout: 3600
      runas: codedeployuser
...

LifecycleEventHooks:
run in specific order (RUN-ORDER)
BeforeBlockTraffic
phase1 : de-register instances from a Load Balancer - BlockTraffic
AfterBlockTraffic
phase2 : real nuts and bolts of application deployment - ApplicationStop
DownloadBundle
BeforeInstall
Install 
AfterInstall 
ApplicationStart
ValidateService
BeforeAllowTraffic
phase3 : re-register instances from a Load Balancer - AllowTraffic 
AfterAllowTraffic

---------------------------------------------------------------------------
"CodePipeline" CI/CD Service
Workflow is defined 
workflow begins when there is a change detected in your source code 
automatic trigger pipeline 
integrates with aws and 3rd party tools such as jenkins 

LAB 
1) To create your EC2 instance using CloudFormation, first save CF_Template.json to your own S3 bucket, 
then update the command below to reference your bucket as well as the name of a Key pair that you own in the 
region that you are working in. 

WINDOWS users will need to use ^ (Shift + 6) instead of \ for line continuation.

aws cloudformation create-stack --stack-name CodeDeployDemoStack \
--template-url https://cloudformation12329.s3.amazonaws.com/CF_Template.json \
--parameters ParameterKey=InstanceCount,ParameterValue=1 \
ParameterKey=InstanceType,ParameterValue=t2.micro \
ParameterKey=KeyPairName,ParameterValue=azam \
ParameterKey=OperatingSystem,ParameterValue=Linux \
ParameterKey=SSHLocation,ParameterValue=0.0.0.0/0 \
ParameterKey=TagKey,ParameterValue=Name \
ParameterKey=TagValue,ParameterValue=CodeDeployDemo \
--capabilities CAPABILITY_IAM

2) Verify that the Cloud Formation stack has completed using: 
aws cloudformation describe-stacks --stack-name CodeDeployDemoStack --query "Stacks[0].StackStatus" --output text

3) Log in to your instance (created by CF) and check that the CodeDeploy agent has correctly installed:  
sudo service codedeploy-agent status
or
sudo systemctl status codedeploy-agent

4) Create another S3 bucket and upload version 1 of your code 
dont forget to enable Versioning on your S3 bucket
5) go to codedeploy and deploy your code stored in s3 bucket 
6) go to codepipeline and create a pipeline 
7) uploaded version 2 of code to s3 -> automatically deployed by pipeline 


--------------------------------------------
LAB 
Our lab use case involves solving a pending problem with our code deployment pipeline. There have been a series of deployments that resulted in unexpected downtime to our client's web application. There have been cases where our QA team did not approve source code before being deployed to the production environment. Our management team has instructed our DevOps team to implement an approval process to keep this from reoccurring. Our solution is to build an AWS CodePipeline pipeline and add a stage that requires manual approval before deploying the source code to production.

Luckily, AWS CodePipeline enables us to easily implement a manual approval process. In this lab, we will build an AWS CodePipeline pipeline that will deploy a web application. We will build in a stage with a manual approval action right after the code repository and before the deployment stage.'

1-Create an AWS IAM Role

In order to allow CloudFormation to act on our behalf, it is necessary to create an IAM role for CloudFormation. You will need to navigate to Identity and Access Management (IAM) and create a role. This role can be granted AdministratorAccess for the purposes of this lab, but keep in mind the principle of least privilege in a production environment.


2-Create an AWS CodeCommit Repository and SNS Topic

Create a CodeCommit respository and upload the s3Retain.yaml file from the GitHub repository. You will also need to create a Simple Notification Service (SNS) topic and subscription using an email to which you have access in order to setup the manual approval for this lab.


3-Create an AWS CodePipeline Pipeline

Create an AWS CodePipeline pipeline with a source provider of AWS CodeCommit using the repository that was created in the previous objective. Make sure to skip the build stage since we'll be using code from AWS CodeCommit instead of building within the pipeline. In the deploy stage, you will need to deploy a CloudFormation stack with the Identity and Access Management (IAM) role that was created earlier. Once the pipeline has been deployed, modify the pipeline by implementing a manual approval step and go through the process of deploying the pipeline again. Observe the notification in your email and then approve the change in the AWS console.

---------------------------------------------------------------------------
"CodeStar" 
Welcome to the AWS CodeStar sample static HTML website
This sample code helps get you started with a simple static HTML website deployed by AWS CodeDeploy and AWS CloudFormation to an Amazon EC2 instance.

What's Here
This sample includes:

README.md - this file
appspec.yml - this file is used by AWS CodeDeploy when deploying the website to EC2
scripts/ - this directory contains scripts used by AWS CodeDeploy when installing and deploying your website on the Amazon EC2 instance
webpage/ - this directory contains static web assets used by your website
index.html - this file contains the sample website
template.yml - this file contains the description of AWS resources used by AWS CloudFormation to deploy your infrastructure
template-configuration.json - this file contains the project ARN with placeholders used for tagging resources with the project ID

---------------------------------------------------------------------------
CodeArtifact
an artifact repository makes it easy for developers to find the software packages they need 
securely store, publish, share packages 
a packag is a bundle of software
includes software from open source
it integrates with publics repos such as npm registry for node.js,
python package index, maven central for commonly used libraries 
it integrates with CodeBuild

CodeArtifact
        my-domain__________________________________   External Connection
        |    Repository  <->  Upstream Repository  |-> Public Repo
        |    my-repos    <->  npn-store            |-> NPM registry
        -------------------------------------------
Create a Domain 
Add an External Connection 
Associate the Upstream Repo 
Install a Package using npm cli

1) Create a domain:
aws codeartifact create-domain --domain my-domain

2) Create a repository in your domain:
aws codeartifact create-repository --domain my-domain --repository my-repo

3) Create an upstream repository for your my-repo repository:
aws codeartifact create-repository --domain my-domain --repository npm-store

4) Add an external connection to the npm public repository to your npm-store repository:
aws codeartifact associate-external-connection --domain my-domain --repository npm-store --external-connection "public:npmjs"

5) Associate the npm-store repository as an upstream repository to the my-repo repository:
aws codeartifact update-repository --repository my-repo --domain my-domain --upstreams repositoryName=npm-store

6) Configure the npm package manager with your my-repo repository (fetches an authorization token from CodeArtifact using your AWS credentials):
aws codeartifact login --tool npm --repository my-repo --domain my-domain

7) Use the npm CLI to install an npm package. For example, to install the popular npm package express, use the following command, if we don’t specify a version, this command will install the latest version available in the external repo:
npm install express

(express is a Node.js web application framework used to develop web and mobile applications)

8) View the package you just installed in your my-repo repository:
aws codeartifact list-packages --domain my-domain --repository my-repo


You now have three CodeArtifact resources:
* The domain my-domain.
* The repository my-repo that is contained in my-domain. This repository has an npm package available to it.
* The repository npm-store that is contained in my-domain. This repository has an external connection to the public npm repository and is associated as an upstream repository with the my-repo repository.

9) To avoid further AWS charges, delete the resources you created:
aws codeartifact delete-repository --domain my-domain --repository my-repo
aws codeartifact delete-repository --domain my-domain --repository npm-store
aws codeartifact delete-domain --domain my-domain
---------------------------------------------------------------------------
Elastic Container Service 
ECS will run your containers on clusters of virtual machines
linux containers and windows containers 
it uses Fargate for serverless
https://12factor.net/
https://d1.awsstatic.com/whitepapers/DevOps/running-containerized-microservices-on-aws.pdf

https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html

https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html

$sudo amazon-linux-extras install docker
$sudo service docker start
$sudo usermod -a -G docker ec2-user
Log out and log back in again to pick up the new docker group permissions. You can accomplish this by closing your current SSH terminal window and reconnecting to your instance in a new one. Your new SSH session will have the appropriate docker group permissions.

Verify that the ec2-user can run Docker commands without sudo.


$docker info
Step 1: Create a Docker image
$touch Dockerfile
        FROM public.ecr.aws/docker/library/ubuntu:18.04

        # Install dependencies
        RUN apt-get update && \
        apt-get -y install apache2

        # Install apache and write hello world message
        RUN echo 'Hello World!' > /var/www/html/index.html

        # Configure apache
        RUN echo '. /etc/apache2/envvars' > /root/run_apache.sh && \
        echo 'mkdir -p /var/run/apache2' >> /root/run_apache.sh && \
        echo 'mkdir -p /var/lock/apache2' >> /root/run_apache.sh && \ 
        echo '/usr/sbin/apache2 -D FOREGROUND' >> /root/run_apache.sh && \ 
        chmod 755 /root/run_apache.sh

        EXPOSE 80

        CMD /root/run_apache.sh


Build the Docker image from your Dockerfile.
$docker build -t hello-world .
Run docker images to verify that the image was created correctly.
$docker images --filter reference=hello-world
$docker run -t -i -p 80:80 hello-world
$docker-machine ip machine-name


Step 2: Authenticate to your default registry
After you have installed and configured the AWS CLI, authenticate the Docker CLI to your default registry. That way, the docker command can push and pull images with Amazon ECR. The AWS CLI provides a get-login-password command to simplify the authentication process.
$aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com

Get-ECRLoginCommand (AWS Tools for Windows PowerShell)
(Get-ECRLoginCommand).Password | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com



Step 3: Create a repository
Now that you have an image to push to Amazon ECR, you must create a repository to hold it. In this example, you create a repository called hello-repository to which you later push the hello-world:latest image. To create a repository, run the following command:
aws ecr create-repository \
    --repository-name hello-repository \
    --image-scanning-configuration scanOnPush=true \
    --region region

Step 4: Push an image to Amazon ECR
PUSH
To tag and push an image to Amazon ECR
List the images you have stored locally to identify the image to tag and push.

$docker images
$docker tag hello-world:latest aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository
$docker push aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository

Step 5: Pull an image from Amazon ECR
PULL
docker pull aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository:latest

DELETE
aws ecr batch-delete-image \
      --repository-name hello-repository \
      --image-ids imageTag=latest \
      --region region

"DELETE REPO" ECR: elastic container registry
aws ecr delete-repository \
      --repository-name hello-repository \
      --force \
      --region region


---------------------------------------------------------------------------
Very useful CloudFormation Snippets:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html


CLOUDFORMATION
Infrastructure as Code YAML/JSON
free to use , charged only for resources 
easy roll back 
manage updates and version control 
consistent 

AWSTemplateFormatVersion: "2010-09-09"
Description: "Template to create an EC2 Instance"
Metadata:
  Instances:
    Description: "Web Server Instance"

Parameters:  #input custom values
  EnvType:
    Description: "Environment type"
    Type: String
    AllowedValues:
      - prod
      - test 

Conditions: # provision resources based on environment
  CreateProdResources: !Equals [ !Ref EnvType, prod ]
  
Mappings: #e.g. create custom mappings based on a region 
  RegionMap:
    us-east-1:
      "ami": "ami-0453898e98046c639"

Transform: #include snippets of code outside the main template 
  Name: 'AWS::Include'
  Parameters:
    Location: 's3://MyAmazonS3BucketName/MyFileName.yml'

Resources: #the aws resources you are deploying <---------mendatory section
  EC2Instance:
    Type: AWS::EC2::Instance 
    Properties:
    Outputs:
  InstanceID:
    Description: the instance ID 
    Value: !Ref EC2Instance 
      InstanceType: t2.micro 
      ImageID: ami-0453898e98046c639





Template sections
Templates include several major sections. The Resources section is the only required section. Some sections in a template can be in any order. However, as you build your template, it can be helpful to use the logical order shown in the following list because values in one section might refer to values from a previous section.

Format Version (optional)
The AWS CloudFormation template version that the template conforms to. The template format version isn't the same as the API or WSDL version. The template format version can change independently of the API and WSDL versions.

Description (optional)
A text string that describes the template. This section must always follow the template format version section.

Metadata (optional)
Objects that provide additional information about the template.

Parameters (optional)
Values to pass to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.

Rules (optional)
Validates a parameter or a combination of parameters passed to a template during a stack creation or stack update.

Mappings (optional)
A mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. You can match a key to a corresponding value by using the Fn::FindInMap intrinsic function in the Resources and Outputs sections.

Conditions (optional)
Conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.

Transform (optional)
For serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it's processed.

You can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates.

Resources (required)
Specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.

Outputs (optional)
Describes the values that are returned whenever you view your stack's properties. For example, you can declare an output for an S3 bucket name and then call the aws cloudformation describe-stack'


Exporting CloudFormation Stack Values
EXPORT:

"Outputs" : {
    "VPCId" : {
      "Description" : "VPC ID",
      "Value" :  { "Ref" : "VPC" },
      "Export" : { "Name" : {"Fn::Sub": "${AWS::StackName}-VPCID" }}
    },
    "PublicSubnet" : {
      "Description" : "The subnet ID to use for public web servers",
      "Value" :  { "Ref" : "PublicSubnet" },
      "Export" : { "Name" : {"Fn::Sub": "${AWS::StackName}-SubnetID" }}
    },
    "WebServerSecurityGroup" : {
      "Description" : "The security group ID to use for public web servers",
      "Value" :  { "Fn::GetAtt" : ["WebServerSecurityGroup", "GroupId"] },
      "Export" : { "Name" : {"Fn::Sub": "${AWS::StackName}-SecurityGroupID" }}


IMPORT:

      "Resources": {
    "WebServerInstance": {
      "Type": "AWS::EC2::Instance",
      "Properties": {
        "InstanceType": "t3.micro",
        "ImageId": "ami-0889a44b331db0194",
        "NetworkInterfaces": [
          {
            "GroupSet": [
              {
                "Fn::ImportValue": {        <-----------------------------
                  "Fn::Sub": "${NetworkStackParameter}-SecurityGroupID" <--
                }
              }
            ],
            "AssociatePublicIpAddress": "true",
            "DeviceIndex": "0",
            "DeleteOnTermination": "true",
            "SubnetId": {
              "Fn::ImportValue": {      <---------------------------------
                "Fn::Sub": "${NetworkStackParameter}-SubnetID" <----------
    

---------------------------------------------------------------------------
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html

CloudFormation for Serverless 
Serverless Application Model (SAM) is an extension to CloudFormation used to define Serverless Applications 

SAM uses a simplified syntax for defining serverless resources:
$ sam package (packages your application and uploads to S3)
$ sam deploy (deploys your serverless app using CloudFormation)


Run all the commands using the AWS CloudShell, it already has the AWS CLI, SAM CLI installed. 

1) Copy hello-from-lambda.js and template.yml to the CloudShell:
curl -O https://raw.githubusercontent.com/ACloudGuru-Resources/course-aws-certified-developer-associate/main/CloudFormation_SAM_Demo/hello-from-lambda.js

curl -O https://raw.githubusercontent.com/ACloudGuru-Resources/course-aws-certified-developer-associate/main/CloudFormation_SAM_Demo/template.yml

2) Create an S3 bucket (add some random numbers to the bucket name to create a unique name):
aws s3 mb s3://cfsambucket

3) Package the files and save them to the S3 bucket you just created:
sam package --template-file template.yml --output-template-file sam-template.yml --s3-bucket cfsambucket

4) Deploy the code: 
sam deploy --template-file sam-template.yml --stack-name mystack --capabilities CAPABILITY_IAM

5) In the Lambda console, view the code, and test it. 



template.yml 
'# This is the SAM template that represents the architecture of your serverless application
# https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template-basics.html

# The AWSTemplateFormatVersion identifies the capabilities of the template
# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/format-version-structure.html
AWSTemplateFormatVersion: 2010-09-09
Description: >-
  sam-app

# Transform section specifies one or more macros that AWS CloudFormation uses to process your template
# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-section-structure.html
Transform:
- AWS::Serverless-2016-10-31

# Resources declares the AWS resources that you want to include in the stack
# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.html
Resources:
  # Each Lambda function is defined by properties:
  # https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction

  # This is a Lambda function config associated with the source code: hello-from-lambda.js
  helloFromLambdaFunction:
    Type: AWS::Serverless::Function
    Properties:
      Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler
      Runtime: nodejs18.x
      Architectures:
        - x86_64
      MemorySize: 128
      Timeout: 100
      Description: A Lambda function that returns a static string.
      Policies:
        # Give Lambda basic execution Permission to the helloFromLambda
        - AWSLambdaBasicExecutionRole'

---------------------------------------------------------------------------
CF Nested Stacks 
nested stacks
allow you to reuse your CloudFormation code.
So for common use cases you dont need to copy and paste,
you can just reference the CloudFormation code
from within your CloudFormation template.
They are really useful for frequently used configurations.
So for example, for a frequently used configuration
for a load balancer, web server or application server.
And, all you need to do is create a CloudFormation template
defining the resource that you want to create,
store in S3 and then you can reference it
from the resources section of any CloudFormation template
using the stack resource type.

-----------------------------------------------------------------------
CDK Cloud Development Kit :
Well, its an open-source development framework
which allows you to build applications,
define and deploy AWS resources
all using a programming language of your choice.
And the ones that are supported,
for instance, TypeScript, Python, Java, .NET, and Go.


First, you need to create a new CDK project,
so we run 
--$cdk init -> inside an empty directory.
--$npm run build -> to compile your application.
--$cdk synth -> to create a CloudFormation template,
and it's synthesizing a new CloudFormation template
based on the code you've provided.
And then, finally, deploy your stack.
--$cdk deploy -> to deploy the stack using CloudFormation.


Run all commands using the CloudShell: 
1. Install the latest version of the CDK, and update to the latest version of TypeScript. :
sudo npm install -g aws-cdk
sudo npm update -g typescript

2. Initialize a sample app using a language of our choice - e.g. typescript. :
mkdir myapp
cd myapp
cdk init sample-app --language=typescript



3. Check your files have been successfully created, the following files should be present in your directory: bin  cdk.json  jest.config.js  lib  node_modules  package.json  package-lock.json  README.md  test  tsconfig.json
The file that contains the details of the stack and its constructs is lib/cdk-init-stack.ts. The cdk.json file tells CDK how to execute your app : 
ls

4. Manually compile the app:
npm run build
cdk ls (to check - the name of our app’s directory is the name of the stack)

5. Synthesise a CloudFormation template:
cdk synth

(It stores the template in cdk.out folder)

6. Deploy the stack: 
(Before we deploy we need to run the bootstrap commend. This is to provision required resources for CDK before we can deploy CDK apps into an AWS account and Region - an S3 bucket to store files and IAM roles with permission to perform deployments.)

cdk bootstrap 
cdk deploy


7. In the console, check CloudFormation, review our stack - there should be an SQS Queue, and an SNS Topic. 


---------------------------------------------------------------------------
AMPLIFY 
Amplify helps people who are primarily
front-end developers to create full stack web
and mobile applications on AWS.
It includes libraries that integrate
with Cognito, S3, Lambda
and API Gateway to create a reliable backend.

And you've got two different services.
So firstly, you've got 
1- Amplify Hosting 
(auto update website when change in code is detected)
which gives you static website hosting that integrates
with your code repository for CICD functionality
and 
2- Amplify Studio, which provides a simple visual tool
and it can be used to configure the front end
and backend of your application.


-----------------------------------------------------------
Q
Which of the following are suitable to store in CodeArtifact?


Compiled applications
All of these are suitable
Deployable packages
Libraries
Documentation relating to your application

CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process. This includes everything needed to build your application, including libraries, deployable packages, compiled applications, and documentation relating to your application.


Q 
How can you prevent AWS CloudFormation from deleting successfully provisioned resources during a stack create operation, while allowing resources in a failed state to be updated or deleted upon the next stack operation?

Choose 2


`Use the --enable-termination-protection flag with the AWS CLI
`Set Termination Protection to Enabled in the CloudFormation console
`In the CloudFormation console, for Stack failure options, select "Preserve successfully provisioned resources"
`Use the "--disable-rollback" flag with the AWS CLI

Sorry!
This AWS CLI option keeps the stack unchanged if a user attempts to delete it.

This AWS CloudFormation console option keeps the stack unchanged if a user attempts to delete it.

Correct Answer
Create operations set to "Preserve successfully provisioned resources" preserves the state of successful resources, while failed resources will stay in a failed state until the next update operation is performed. Stack failure options.

Specifying the disable-rollback option or on-failure DO_NOTHING enumeration during a create-stack operation will preserve successfully provisioned resources during a stack create operation. Stack failure options.


----------------------------------------
Q 
Part of your CloudFormation deployment fails due to a misconfiguration, by default what will happen?


CloudFormation will ask you if you want to continue with the deployment
CloudFormation will rollback only the failed components
CloudFormation will rollback the entire stack
Failed components will remain available for debugging purposes

Good work!
Correct. By default, the “automatic rollback on error” feature is enabled. This will direct CloudFormation to only create or update all resources in your stack if all individual operations succeed. If they do not, CloudFormation reverts the stack to the last known stable configuration.

-------------------------------------------------------
Which AWS service can be used to compile source code, run tests and also package code?
CodeDeploy
CodeCommit
CodePipeline
CodeBuild

Good work!
Correct. CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.
-------------------------------------------------------
In AWS CloudFormation, which top-level section of the donor stack's CloudFormation template would you use to declare the output values that you want to use as inputs for another CloudFormation stack (the recipient stack)?

Resources
Export
Outputs
Transforms

Sorry!
The Resources section defines the resources you are provisioning.

Correct Answer
The Outputs section is used to output user-defined data relating to the resources you have built. You can also use the Export field within the Outputs section to export values to be used as input for another CloudFormation stack.
------------------------------------------------------
You are running your application using Docker provisioned with Elastic Beanstalk. You would like to upgrade the application to a new version. How should you approach this?


Create a new Docker image using the latest code, deploy it using Elastic Beanstalk, then terminate your old environment.


Create a new Docker image using the latest code, log in to the underlying EC2 instance and install the new Docker image.


Bundle your Dockerfile and the application code into a zip file, then upload and deploy it using the Elastic Beanstalk console.


Bundle your code into a zip file, upload and deploy it using Cloud Formation.

Good work!
Elastic Beanstalk supports the deployment of web applications as Docker containers. Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. When using the console you can choose to upload and deploy your code in a single step. Elastic Beanstalk will manage the previous version of your code so you can revert back to it if necessary.
-------------------------------------------------------
You want users to receive an email notification whenever they push code to their AWS CodeCommit repositories. How can you configure this?


Configure Notifications in the AWS CodeCommit console, this will create a CloudWatch Events rule to send a notification to an Amazon SNS topic which will trigger an email to be sent to the user


Configure a CloudWatch Events rule to send a message to SQS which will trigger an email to be sent whenever a user pushes code to the repository


Configure a CloudWatch Events rule to send a message to SES which will trigger an email to be sent whenever a user pushes code to the repository


Create a new SNS topic and configure it to poll for CodeCommit events. Ask all your users subscribe to the topic to receive notifications

Sorry!
Incorrect. CodeCommit can use a trigger to send events to Amazon SNS, but Amazon SNS can't poll for CodeCommit events.

Correct Answer
Correct. You can set up notification rules for a repository so that repository users receive emails about the repository event types you specify. Notifications are sent when events match the notification rule settings. You can create an Amazon SNS topic to use for notifications or use an existing one in your AWS account. You can use the CodeCommit console and the AWS CLI to configure notification rules.