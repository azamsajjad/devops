s3 is object based storage
not suitable for OS or DB installation 

Object storage is a method of storing files in a flat address space based on attributes and metadata. 


rather than in file systems or data blocks
s3 objects can be of 5tb of size 
Universal Namespace, globally unique 
s3 url 
https://bucket-name.s3.region.amazonaws.com/key-name
HTTP 200 code if upload is successfull (api/cli)
s3 is KEY VALUE store 
key = object name 
value = sequence of data that make up the object 
Version ID = multiple versions of same object 
Metadata = data about the data you are storing e.g. last-modified
data is spread across multiple devices and facilities to ensure availability and durability >= 3AZs
99.9x11 % durability 
99.95-99.99% availability depending on s3 tier 


PRICING
S3 standard 
S3 standard in-frequent Access 30days(min storage duration: 30 days)
S3 one zone in-frequent Access  30days(min storage duration: 30 days)
(stored redundantly within single AZ) 20% less than regular S3-IA 
S3 Glacier (pay each time you access your data) only for archiving
retrieval time 1 min to 12 hours - 90 days min storage duration
S3 Glacier Instant Retrieval - 
S3 Glacier Flexible Retrieval - retrieve at no cost 
S3 Glacier Deep Archive - default retrieval time of 12 hours - 180 Days 
(e.g. finincial records for legal purposes)
S3 Intelligent Tiering
Frequent and inFrequent - $0.0025 monthly fee per 1000 objects

HIGHEST COST IS S3 standard
Then Intelligent Tiering 
Retrieval fee applies to all infrequent access



-----------------E N C R Y P T I O N-------------------------------------  

S3 is very secure by Default
server-side encryption 
ACLs who has access 
BUCKET POLICIES - which actions are allowed which are not.
BUCKET POLICIES applied at bucket level , not on object level 
BUCKET ACLs applied at object level 
[3] Ecryption Options
1- Encryption in transit 
    ` SSL/TLS 
    ` HTTPS 
2- Encryption at rest: Server-Side Encryption
` Server-side encryption with Amazon S3 managed keys (SSE-S3) AES256-bit
                (ENABLED BY DEFAULT when you create bucket)
` Server-side encryption with AWS Key Management Service keys (SSE-KMS) 
` Dual-layer server-side encryption with AWS Key Management Service keys (DSSE-KMS)
// Secure your objects with two separate layers of encryption. For details on pricing, see DSSE-KMS pricing on the Storage tab of the Amazon S3 pricing page. 
// Bucket Key
// Using an S3 Bucket Key for SSE-KMS reduces encryption costs by lowering calls to AWS KMS. S3 Bucket Keys aren't supported for DSSE-KMS. Learn more 

3- Encryption at rest: Client-Side Encryption SSE-C
You encrypt yourself before uploading to S3 
you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. With this option, the customer is responsible for managing and rotating the keys, and without access to these keys the Amazon S3 data can not be decrypted.

4- Client-side encryption

Client-side encryption is the act of encrypting sensitive data before sending it to Amazon S3. When using client-side encryption, the encryption performs locally and your data never leaves the run environment unencrypted. You maintain possession of your master encryption keys, and they are never sent to AWS therefore, it is important that you safely store them (i.e., as a file or using a separate key management system) and load them when uploading or downloading objects. This ensures that no one outside of your environment has access to your master keys and without access to the master keys; your data cannot be decrypted. If your master encryption keys are lost, you will not be able to decrypt your own data, therefore it is essential that if you use client-side encryption, that you store your keys safely.

To enable client-side encryption, you have the following options:

Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS). With this option, you use an AWS KMS CMK for client-side encryption when uploading or downloading data in Amazon S3. 

Use a master key that you store within your application. With this option, you provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. 
-----------------------------------------------------------------

TO HOST STATIC WEBSITE OR
TO make contents of a bucket or a file PUBLIC
goto bucket > permissions
Block public access (bucket settings) = OFF
BUCKET POLICY - Generate 
    type of plicy = S3 Bucket Policy
    principal = * (anyone can access)
    Actions = Get Object 
    Amazon Resource Name (ARN) = arn:aws:s3:::reason320320/* for all files */
    GENERATE POLICY 
    Then copy and paste into Bucket Policy
then enable Static Website in Properties of Bucket 

S3 Access Logs - everytime user reads, writes creates, deletes a file 
can be written to a different S3 Bucket


CORS - cross origin resource sharing 
allowing code in 1 bucket to access code in another bucket 
index.html(bucket1) --> loadpage.html(bucket2)
copy index bucket URL and paste it in Loadpage bucket CORS section in permission.

[
    {
        "AllowedHeaders": [
            "Authorization"
        ],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "http://my-index-bucket-329329.s3-website-us-east-1.amazonaws.com"
        ],
        "MaxAgeSeconds": 3000
    }
]

Versioning states

Buckets can be in one of three states. The versioning state applies to all objects in the bucket. Storage costs are incurred for all objects in your bucket, including all versions. To reduce your Amazon S3 bill, you might want to delete previous versions of your objects when they are no longer needed.

To learn more, expand each of the following three categories.


Unversioned (default)
–
No new and existing objects in the bucket have a version.


Versioning-enabled
–
Versioning is enabled for all objects in the bucket. After you version-enable a bucket, it can never return to an unversioned state. However, you can suspend versioning on that bucket.


Versioning-suspended
–
Versioning is suspended for new objects. All new objects in the bucket will not have a version. However, all existing objects keep their object versions.


========================================================
Multipart Upload API

You can upload or copy objects of up to 5 GB in a single PUT operation. For objects, up to 5 TB you must use the multipart upload API. The multipart upload API allows you to upload a single object as a set of parts. Each part is a contiguous portion of the objects data. You can upload these object parts independently and in any order.

GET operations

You can use the GET operation to retrieve a whole object or parts of an object directly from Amazon S3. 

If you need to retrieve the object in parts, use the Range HTTP header in a GET request. Doing this allows you to retrieve a specific range of bytes from an object stored in Amazon S3. You can then resume fetching other parts of the object whenever you or your application is ready. This resumeable download is useful if you only need portions of your object data, in cases where network connectivity is poor, or if your application must process only subsets of object data.


Use lifecycle rules to clean up incomplete multipart uploads automatically.

An Amazon S3 Lifecycle configuration is an XML file that consists of a set of rules with predefined actions that you want Amazon S3 to perform on objects during their lifetime. As a best practice, we recommend you configure a lifecycle rule using the AbortIncompleteMultipartUpload action to minimize your storage costs.



Managing your storage lifecycle

If you keep manually changing your objects, such as your employee photos, from storage tier to storage tier, you might want to automate the process by configuring their Amazon S3 lifecycle. When you define a lifecycle configuration for an object or group of objects, you can choose to automate between two types of actions: transition and expiration.


The following use cases are good candidates for the use of lifecycle configuration rules:

Periodic logs: If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them.
Data that changes in access frequency: Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them. But your organization or regulations might require you to archive them for a specific period. After that, you can delete them.
----------------------------------------------------------------------
Amazon S3 Transfer Acceleration
–
You can use Amazon S3 Transfer Acceleration for fast, easy, and secure transfers of files over long distances. It takes advantage of Amazon CloudFront globally distributed edge locations, routing data to Amazon S3 over an optimized network path.

Transfer Acceleration is best suited for scenarios in which you want to transfer data to a central location from all over the world or transfer significant amounts of data across continents regularly. It can also help you use your available bandwidth when uploading to Amazon S3.




Amazon S3 Transfer Acceleration
    Secure transfers of files over long distances
AWS Transfer Family
    Supports SFTP, FTPS
Amazon Kinesis Data Firehose
    Near-real-time analytics with the business intelligence
AWS DataSync
    Moves large amounts of data online between on premises and Amazon S3
Amazon Kinesis Data Streams
    Continuously captures and stores TBs of data per hour.
Amazon Partner Network
    Use of third-party connectors



"Online data transfer services"

Online data transfer services are a group of offerings that allows you to move your data into and out of the AWS Cloud and Amazon S3 via online, internet based, connections. 

AWS DataSync
AWS Transfer Family
Amazon S3 Transfer Acceleration
Amazon Kinesis Data Firehose
Amazon Kinesis Data Streams
Amazon Partner Network

"Offline data transfer services"

AWS Snowcone
AWS Snowball
AWS Snowmobile


"Hybrid cloud storage services"

If you want to take advantage of the benefits of cloud storage but have applications running on-premises that require low-latency access to their data, or you need rapid data transfer to the cloud; then you can use AWS hybrid cloud storage architectures. 

AWS Direct Connect
AWS Storage Gateway
// You can use the AWS Storage Gateway, in File Gateway mode, to store your on premises data in an existing Amazon S3 bucket. You can deploy AWS Storage Gateway as a virtual appliance or purchase a hardware appliance version.

// AWS Storage Gateway configured as a File Gateway enables you to connect your Amazon S3 bucket using either the Network File System (NFS) or Server Message Block (SMB) protocol with local caching. You can transfer your data using an AWS File Storage Gateway over the internet or over an AWS Direct Connect connection.
The two requirements that must be met before using Amazon S3 File Gateway are:

Configure your private networking, VPN, or AWS Direct Connect between your Amazon Virtual Private Cloud (Amazon VPC) and the on-premises environment where you are deploying your gateway.

Configure Microsoft Active Directory (AD).

These are essential steps to ensure proper connectivity and authentication between your on-premises environment and the AWS resources when using Amazon S3 File Gateway.
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
SECURING DATA.
Pre-Signed URLs
–
Pre-Signed URLs are used to grant time-limited access to others with temporary URLs.

        Permissions to the object
        –
        Anyone with valid security credentials can create a presigned URL. However, to successfully access an object, someone who has permission to perform the operation must create the presigned URL.


        Credentials
        –
        The credentials that you can use to create a presigned URL include:

        IAM instance profile: Valid up to 6 hours
        AWS Security Token Service: Valid up to 36 hours when signed with permanent credentials, such as the credentials of the AWS account root user or an IAM user
        IAM user: Valid up to 7 days when using AWS Signature Version 4
        To create a presigned URL that's valid for up to 7 days, first designate IAM user credentials (the access key and secret access key) to the SDK that youre using. Then, generate a presigned URL using AWS Signature Version 4.


        Token expiration
        –
        If you created a presigned URL using a temporary token, then the URL expires when the token expires, even if you created the URL with a later expiration time. 
--------------------
Access control lists
–
Access Control List (ACLs) to make individual objects accessible to authorized users.

Note: Amazon S3 ACLs are a legacy access control mechanism that predates IAM. AWS recommends using Amazon S3 bucket policies or IAM policies for access control.

"Access Policies: or Resource Based Policies:"

Access policy describes who has access to what resources. They attach to your resources, such as buckets and objects, and are also called resource policies. For example, bucket policies and access control lists are resource based policies because you attach them directly to buckets and objects.

User policies or IAM policies are access policies attached to users in your account. You may choose to use one type of policy or a combination of both, to manage permissions with your Amazon S3 resources.

"Bucket policies"

In order to grant other AWS accounts or IAM users access to the bucket and the objects in it, you need to attach a bucket policy. Because you are granting access to a user or account, a bucket policy must define a PRINCIPAL (which is an account, user, role, or service) entity within the policy. You will notice that the "Principal" statement is listed in the policy. Consult the image below as an example. max policy size of 20kb

    "When to use a bucket policy
    Use a bucket policy if:

    You need to grant cross-account permissions to other AWS accounts or users in another account, without using IAM roles.
    Your IAM policies reach the size limits for users, groups, roles. 
    You prefer to keep access control policies in the Amazon S3 environment.
    Although both bucket and user policies support granting permission for all Amazon S3 operations, the user policies are for managing permissions for users in your account. "

    Because bucket policies grant access to another AWS account or IAM user, you must specify the principal, or the user to whom you are granting access, as a "Principal" in the bucket policy.
----------------------------------------------------------------------
Decoupling storage from compute
–
In traditional Hadoop and data warehouse solutions, storage and compute remain tightly coupled, making it difficult to optimize costs and data processing workflows. With Amazon S3, you can cost-effectively store all data types in their native formats. Then, you can launch as many or as few virtual servers as you need using Amazon Elastic Compute Cloud (EC2), and use AWS analytics tools to process your data. You can also optimize your EC2 instances to provide the right ratios of CPU, memory, and bandwidth for best performance. 


Centralized data architecture
–
Amazon S3 makes it easy to build a multi-tenant environment, where many users can bring their own data analytics tools to a common set of data. This improves both cost and data governance over that of traditional solutions, which require multiple copies of data to be distributed across multiple processing platforms.


Integration with clusterless and serverless AWS services
–
You can use Amazon S3 with Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue to query and process data. Amazon S3 also integrates with AWS Lambda serverless computing to run code without provisioning or managing servers. With all of these capabilities, you only pay for the actual amounts of data you process or for the compute time consumed. 


Standardized APIs
–
Amazon S3 REST APIs are easy to use, and supported by most major third-party independent software vendors (ISVs), including leading Apache Hadoop and analytics tool vendors. This allows customers to bring the tools they are comfortable and knowledgeable about to help them perform analytics on data in Amazon S3.


DATA CATALOG.

Thus, an essential component of an Amazon S3-based data lake is the data catalog. The data catalog provides a query-able interfaceE oOf all assets stored in the data lake’s S3 buckets. The design of the data catalog is to provide a single source of truth about the contents of the data lake. 

AWS GLUE.

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data. You can use AWS Glue to organize, cleanse, validate, and format data for storage in a data warehouse or data lake. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. In order to create your data warehouse or data lake, you must catalog this data.

AMAZON ATHENA.

Amazon Athena is an interactive query service that makes it easy for you to analyze data directly in Amazon S3, using standard SQL. You can get results in a matter of seconds. 

AMAZON REDSHIFT SPECTRUM.

A second way to perform in-place querying of data assets in an Amazon S3-based data lake is to use Amazon Redshift Spectrum. Amazon Redshift is a large-scale, managed data warehouse service used with data assets in Amazon S3. However, data assets must be loaded into Amazon Redshift before queries run. 

Because Amazon Athena and Amazon Redshift share a common data catalog and data formats, you can use both Athena and Redshift Spectrum against the same data assets.

You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads.

"Amazon FSx for Lustre and Amazon S3 data lakes"

Amazon FSx for Lustre, is a fully managed file system that is optimized for compute-intensive workloads, such as high performance computing, machine learning, and media data processing workflows.

With Amazon FSx for Lustre, you can launch and run a Lustre file system that can process massive data sets at up to hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.  Amazon FSx for Lustre file systems can link to Amazon S3 buckets, allowing you to access and process data concurrently from a high-performance file system.

Amazon FSx for Lustre can use Amazon S3 as a raw data repository as well as a repository for processed data. It makes it easy to process your cloud datasets in Amazon S3. 

When linked to an S3 bucket, FSx for Lustre transparently presents objects as files, allowing you to run your workload without managing data transfer from S3. As the contents of your S3 bucket change, FSx for Lustre automatically updates your file system with the latest data available to run your workload.
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
CLOUDFRONT - Amazons CDN content delivery network
CLOUDFRONT Origin - EC2,S3,ELB,R53 address
CLOUDFRONT EDGE LOCATION - location where content is cached
CLOUDFRONT Distribution - name given to config settings

AWS operates 200+ edge locations
it also works with non-aws origin server 
default TTL is 1 day - time to live is age for cached objects
you can clear the cache yourself if you have made changes to your page

Cloudfront S3 Transfer Acceleration enables fast, secure transfer of files b/w your s3 bucket and your end-users - if you want your users to upload files at your s3 bucket at london, they can upload via edge location via their local network.--> As data arrives at edge location, it is routed to your Amazon S3 in london, over an optimized network path of AWS
           in cloudfront settings - for Transfer acceleration, select 3rd
            Allowed HTTP methods
            GET, HEAD
            GET, HEAD, OPTIONS
            GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE
Edge locations are not Read-only, you can write to them too.

Restrict viewer access (GOOD OPTION FOR PAID CONTENT)
If you restrict viewer access, viewers must use CloudFront signed URLs or signed cookies to access your content.
No
Yes
INVALIDATION - give path to delete cache for that file

OAI - Origin Access Identity
to block public access at bucket level and serve content only through cloudfront url
When creating cloudfront distribution - set 
        Origin accessInfo Public
        `Bucket must allow public access.
        `Origin access control settings (recommended)
        `Bucket can restrict access to only CloudFront.
         Legacy access identities
Use a CloudFront origin access identity (OAI) to access the S3 bucket.


CLOUDFRONT Allowed HTTP Methods
Allowed HTTP methods
GET, (allow you to read data)
GET, HEAD (allow you to read data and header)
GET, HEAD, OPTIONS
GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE (if your users need read & write access to your website)
   PUT is idenpotent - write data (send data to create new resource)
   PATCH (partial modify)= modify contents of shopping cart 
   POST (insert data)= comment on a blog post
   DELETE (also write action) = deleting your email address
   OPTIONS (receive a list of supported HTTP methods)

LAMBDA@EDGE--------------- 
to override behavior of request and responses 
4 avl functions 
1-Viewer request -when cloudfront receives a request from the viewer
2-Origin request -before cloudfront forwards a request to the origin
3-Origin response -when cloudfront receives a response from the origin
4-Viewer response -before cloudfront returns the response to the viewer

--> Viewer request -->          --> Origin request -->
                        CF
<-- Viewer response <--         <-- Origin response <--

    Usecase: you have protected content, paid content
    you can authenticate with cognito
    A to B testing website 

CLOUDFRONT_PROTECTION-------------------(Use signed URLs)
Access to Cached content can be protected via signed urls/signed cookies
by default a distribution allows everyone to have access 
Original Access Identity: OAI 
A virtual user identity that will be used to give your cloudfront distribution permission to fetch a private object 

IN-ORDER TO USE SIGNED URLS OR SIGNED COOKIES, YOU NEED TO HAVE OAI .
Signed URLs: not same as S3 pre-signed url
A url that provides temporary access to cached objects 
Signed Cookies:
A cookie which is passed along with the request to cloudfront, 
adv- provide access to restricted files e.g. Video Streaming

once a signed url passes a cookie, you can have access for as long as cookie is valid
BLOCK INDIA.
Amazon CloudFront: CloudFront can be used in combination with AWS WAF to restrict access based on geolocation. You can configure CloudFront to use AWS WAF rules for filtering traffic, including blocking or allowing access from specific countries.
--------------------------------------------------------------------------
ATHENA
interactive query service using standard SQL
serverless 
no need to set up complex Extract/Transform/Load (ETL) processes
works directly with data stored on S3
USES:
    QUERY log files stored on s3 
    PERFORM aws costs and usage reports
    GENERATE business reports on data stored on s3
    ANALYZE - run queries on click-stream data

LAB: 
1- configure a trail in CloudTrail 
2- CloudTrail sends logs to S3 
3- create another S3 for athena results. add it in setting of athena
3- Create an Athena Table to query data stored in S3 with SQL
QUERY 1 - creates a database
CREATE DATABASE athenadb
QUERY 2 - creates a table
CREATE EXTERNAL TABLE cloudtrail_logs (
eventversion STRING,
useridentity STRUCT<
               type:STRING,
               principalid:STRING,
               arn:STRING,
               accountid:STRING,
               invokedby:STRING,
               accesskeyid:STRING,
               userName:STRING,
sessioncontext:STRUCT<
attributes:STRUCT<
               mfaauthenticated:STRING,
               creationdate:STRING>,
sessionissuer:STRUCT<  
               type:STRING,
               principalId:STRING,
               arn:STRING, 
               accountId:STRING,
               userName:STRING>>>,
eventtime STRING,
eventsource STRING,
eventname STRING,
awsregion STRING,
sourceipaddress STRING,
useragent STRING,
errorcode STRING,
errormessage STRING,
requestparameters STRING,
responseelements STRING,
additionaleventdata STRING,
requestid STRING,
eventid STRING,
resources ARRAY<STRUCT<
               ARN:STRING,
               accountId:STRING,
               type:STRING>>,
eventtype STRING,
apiversion STRING,
readonly STRING,
recipientaccountid STRING,
serviceeventdetails STRING,
sharedeventid STRING,
vpcendpointid STRING
)
ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION 's3://aws-cloudtrail-logs-928151643990-20c29fa1/AWSLogs/928151643990/';

QUERY 3 - outputs mentioned fields from data
SELECT
 useridentity.arn,
 eventname,
 sourceipaddress,
 eventtime
FROM cloudtrail_logs
LIMIT 100;

------------------------------------------------------------------------
You would like to configure your S3 bucket to only serve content over HTTPS/SSL and explicitly deny all unencrypted HTTP access. In your bucket policy, how should you specify the type of request that should be denied?
{
  "Id": "ExamplePolicy",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowSSLRequestsOnly",
      "Action": "s3:*",
      "Effect": "Deny",
      "Resource": [
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
      ],
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      },
      "Principal": "*"
    }
  ]
}
Explicitly denying requests that are identified as "aws:SecureTransport": "false" would deny requests that are using HTTP and are unencrypted.

A developer is working on an application in her AWS account that uses Amazon S3 to store sensitive data. To enhance security, the developer wants to ensure that all S3 buckets in the application are not publicly accessible. Which of the following actions should the developer take to meet this requirement? Choose the best option.
    Enable block public access settings at the account level to apply to all current and future S3 buckets in the account.


You would like to configure your S3 bucket to deny put object requests that do not use server-side encryption. Which bucket policy can you use to deny permissions to upload objects, unless the request includes server-side encryption?

{
                "Version": "2012-10-17",
                "Id": "PutObjPolicy",
                "Statement": [
                    {
                        "Sid": "DenyUnEncryptedObjectUploads",
                        "Effect": "Deny",
                        "Principal": "*",
                        "Action": "s3:PutObject",
                        "Resource": "arn:aws:s3:::bucket/*",
                        "Condition": {
                            "Null": {
                                "s3:x-amz-server-side-encryption": "true"
                            }
                        }
                    }
                ]
            }
The condition above looks for a Null value for the s3:x-amz-server-side-encryption key. If this condition is true, it means the request is Null and does not include server-side encryption. Setting the condition in the condition policy to "s3:x-amz-server-side-encryption": "true" with "Effect": "Deny" and "Action": "s3:PutObject" would deny put object requests that do not use server-side encryption


Which S3 storage classq is ideal for backup and disaster recovery use cases, when large sets of data occasionally need to be retrieved in minutes, without concern for costs?
        Amazon S3 Glacier Flexible Retrieval (Formerly S3 Glacier)
    S3 Glacier Flexible Retrieval (formerly S3 Glacier) is the ideal storage classs for archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases.


-------------------------------------
A storage specialist of a company created a bucket named 'demobucket2022' for the purpose of storing files for the creation of a proof of concept (POC). After the successful implementation of proof of concept, the team wanted to leverage the bucket for the development work because of the content which was generated during POC. However, they didnt want to name it 'demobucket2022'.

What is the solution for the team to proceed with this implementation of the requirement?


Raise a support ticket and work with AWS support to rename the bucket.

``````````Amazon S3 buckets cannot be renamed once created. So, a new bucket needs to be created and copy the content using cp/sync command from the AWS CLI.

Use the AWS Management Console and rename the bucket using rename bucket option.

Use AWS CLI and update the name of the bucket using the update bucket api.

=======================================
A company runs a batch application in the AWS Cloud hosted on 200+ Amazon EC2 instances. As a solutions architect, you are asked to push debug logs to an Amazon S3 bucket every 2:00 AM for all the EC2 instances.

What is the best possible solution from an operation point of view?


Use Systems Manager Distributor to transfer the logs every 2:00 AM on all the AWS Systems Manager Managed instances.

Inject a user script via Ops Work to all of the Amazon EC2 instances that will push the logs to this Amazon S3 bucket.

``````Create a schedule in AWS Systems Manager Maintenance window to move the logs to S3 bucket every 2:00 AM in the morning.

Use SSM Session Manager to run a shell script on all Amazon EC2 instances 2:00 AM in the morning.
===========================================================================
A customer wants to host their domain in AWS using Amazon Route 53 service. They plan to use Amazon CloudFront for distributing their website globally. All the static assets of the website will be served from Amazon S3 and Amazon EC2 will be used for serving the dynamic content via Amazon CloudFront. The customer needs a way to integrate Amazon Route 53 with AWS CloudFront and Amazon S3.

Which strategy would you recommend that is the easiest to configure?


"They are integrated by default."

It is possible to integrate Amazon Route53 with other AWS services like CloudFront and S3 via creating an alias record in Route 53 that points to CloudFront distributions and S3 buckets.

It is possible, however, we need to insert the code for the integration of other AWS Services in our application which will be using Amazon Route 53.

Use an AWS Lambda function with API Gateway to redirect the API calls from Amazon Route53 to Amazon CloudFront and from there to Amazon S3 and Amazon EC2.
============================================================================
A multi-national company has services across the globe which has a web application as its customer-facing frontend. One of the features of the app is to allow users to be able to upload huge amounts of files. As part of their architecture, they use a single bucket in the us-east-1 region and all the data from users is uploaded to this bucket. Now, as the demand for applications has grown, more and more users are using the application, which has led to an increase in file uploads. Because the users are across the globe, the upload takes time when the users are geographically far from the us-east-1region, which leads to a degraded user experience. You need to improve the upload experience without making major code level changes.

Which is the best service that you could use to achieve this requirement?


AWS DataSync

Amazon Partner Network

``````Amazon S3 Transfer Acceleration

AWS Transfer Family

Amazon S3 Transfer Acceleration utilizes Amazon CloudFronts globally distributed edge locations to accelerate the upload of objects to an S3 bucket. This service optimizes the transport path and provides a faster and more consistent user experience for uploads, regardless of the user's location.

This means that users from around the world can upload their files with improved speed and efficiency. It doesn't require significant code changes in your application, making it a suitable solution for your scenario.

====================================================================
Your organizations operational lead has decided to build a cost effective centralized logging solution for multiple AWS accounts. The solution includes an Amazon S3 bucket in the centralized logging account. The logs must be deleted after three months to minimize cost.

What is the most operationally efficient way to accomplish this task?


Transition objects to the S3 Standard-IA storage classs 30 days after creating them and delete manually/

````Expiration actions

Transition actions

Archive objects to the S3 Glacier Flexible Retrieval storage classs for three months and delete manually.

With expiration actions, you can set up a lifecycle policy in the S3 bucket to automatically delete objects that are older than a specified number of days. In this case, you would configure the policy to delete logs after three months, which helps minimize cost by removing outdated data.


========================================================================
A public-facing weather application stores millions of weather forecast files in an Amazon S3 bucket. During bad weather, the bucket can experience tens of thousands of GET requests per second. The developer must avoid Amazon S3 reads throttling because of request rate limits. Which solutions meet the requirements? (Select TWO.)

"Adopt an Object key naming pattern that distributes the objects across multiple prefixes."

Open an AWS support ticket to increase the request-per-second quota on the Amazon S3 bucket.

Create multiple access points for the Amazon S3 bucket. Adopt a naming pattern thatdistributes traffic to different access points. Adjust the application to use the multiple accesspoints.

Adjust the Amazon S3 bucket settings for Minimum TTL, Maximum TTL, and Default TTL to control cache behavior.

"Create an Amazon CloudFront web distribution with the Amazon S3 bucket as an origin. Set the distribution's Minimum TTL, Maximum TTL, and Default TTL to control cache behavior. Adjust the web application to use the CloudFront distribution domain name."