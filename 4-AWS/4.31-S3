s3 is object based storage
not suitable for OS or DB installation 

Object storage is a method of storing files in a flat address space based on attributes and metadata. 


rather than in file systems or data blocks
s3 objects can be of 5tb of size 
Universal Namespace, globally unique 
s3 url 
https://bucket-name.s3.region.amazonaws.com/key-name
HTTP 200 code if upload is successfull (api/cli)
s3 is KEY VALUE store 
key = object name 
value = sequence of data that make up the object 
Version ID = multiple versions of same object 
Metadata = data about the data you are storing e.g. last-modified
data is spread across multiple devices and facilities to ensure availability and durability >= 3AZs
99.9x11 % durability 
99.95-99.99% availability depending on s3 tier 


PRICING
S3 standard 
S3 standard in-frequent Access 30days(min storage duration: 30 days)
S3 one zone in-frequent Access  30days(min storage duration: 30 days)
(stored redundantly within single AZ) 20% less than regular S3-IA 
S3 Glacier (pay each time you access your data) only for archiving
retrieval time 1 min to 12 hours - 90 days min storage duration
S3 Glacier Instant Retrieval - 
S3 Glacier Flexible Retrieval - retrieve at no cost 
S3 Glacier Deep Archive - default retrieval time of 12 hours - 180 Days 
(e.g. finincial records for legal purposes)
S3 Intelligent Tiering
Frequent and inFrequent - $0.0025 monthly fee per 1000 objects

HIGHEST COST IS S3 standard
Then Intelligent Tiering 
Retrieval fee applies to all infrequent access



-----------------E N C R Y P T I O N-------------------------------------  

S3 is very secure by Default
server-side encryption 
ACLs who has access 
BUCKET POLICIES - which actions are allowed which are not.
BUCKET POLICIES applied at bucket level , not on object level 
BUCKET ACLs applied at object level 
[3] Ecryption Options
1- Encryption in transit 
    ` SSL/TLS 
    ` HTTPS 
2- Encryption at rest: Server-Side Encryption
` Server-side encryption with Amazon S3 managed keys (SSE-S3) AES256-bit
                (ENABLED BY DEFAULT when you create bucket)
` Server-side encryption with AWS Key Management Service keys (SSE-KMS) 
` Dual-layer server-side encryption with AWS Key Management Service keys (DSSE-KMS)
// Secure your objects with two separate layers of encryption. For details on pricing, see DSSE-KMS pricing on the Storage tab of the Amazon S3 pricing page. 
// Bucket Key
// Using an S3 Bucket Key for SSE-KMS reduces encryption costs by lowering calls to AWS KMS. S3 Bucket Keys aren't supported for DSSE-KMS. Learn more 

3- Encryption at rest: Client-Side Encryption SSE-C
You encrypt yourself before uploading to S3 
you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. With this option, the customer is responsible for managing and rotating the keys, and without access to these keys the Amazon S3 data can not be decrypted.

4- Client-side encryption

Client-side encryption is the act of encrypting sensitive data before sending it to Amazon S3. When using client-side encryption, the encryption performs locally and your data never leaves the run environment unencrypted. You maintain possession of your master encryption keys, and they are never sent to AWS therefore, it is important that you safely store them (i.e., as a file or using a separate key management system) and load them when uploading or downloading objects. This ensures that no one outside of your environment has access to your master keys and without access to the master keys; your data cannot be decrypted. If your master encryption keys are lost, you will not be able to decrypt your own data, therefore it is essential that if you use client-side encryption, that you store your keys safely.

To enable client-side encryption, you have the following options:

Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS). With this option, you use an AWS KMS CMK for client-side encryption when uploading or downloading data in Amazon S3. 

Use a master key that you store within your application. With this option, you provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. 
-----------------------------------------------------------------

TO HOST STATIC WEBSITE OR
TO make contents of a bucket or a file PUBLIC
goto bucket > permissions
Block public access (bucket settings) = OFF
BUCKET POLICY - Generate 
    type of plicy = S3 Bucket Policy
    principal = * (anyone can access)
    Actions = Get Object 
    Amazon Resource Name (ARN) = arn:aws:s3:::reason320320/* for all files */
    GENERATE POLICY 
    Then copy and paste into Bucket Policy
then enable Static Website in Properties of Bucket 

S3 Access Logs - everytime user reads, writes creates, deletes a file 
can be written to a different S3 Bucket


CORS - cross origin resource sharing 
allowing code in 1 bucket to access code in another bucket 
index.html(bucket1) --> loadpage.html(bucket2)
copy index bucket URL and paste it in Loadpage bucket CORS section in permission.

[
    {
        "AllowedHeaders": [
            "Authorization"
        ],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "http://my-index-bucket-329329.s3-website-us-east-1.amazonaws.com"
        ],
        "MaxAgeSeconds": 3000
    }
]

Versioning states

Buckets can be in one of three states. The versioning state applies to all objects in the bucket. Storage costs are incurred for all objects in your bucket, including all versions. To reduce your Amazon S3 bill, you might want to delete previous versions of your objects when they are no longer needed.

To learn more, expand each of the following three categories.


Unversioned (default)
–
No new and existing objects in the bucket have a version.


Versioning-enabled
–
Versioning is enabled for all objects in the bucket. After you version-enable a bucket, it can never return to an unversioned state. However, you can suspend versioning on that bucket.


Versioning-suspended
–
Versioning is suspended for new objects. All new objects in the bucket will not have a version. However, all existing objects keep their object versions.


========================================================
Multipart Upload API

You can upload or copy objects of up to 5 GB in a single PUT operation. For objects, up to 5 TB you must use the multipart upload API. The multipart upload API allows you to upload a single object as a set of parts. Each part is a contiguous portion of the objects data. You can upload these object parts independently and in any order.

GET operations

You can use the GET operation to retrieve a whole object or parts of an object directly from Amazon S3. 

If you need to retrieve the object in parts, use the Range HTTP header in a GET request. Doing this allows you to retrieve a specific range of bytes from an object stored in Amazon S3. You can then resume fetching other parts of the object whenever you or your application is ready. This resumeable download is useful if you only need portions of your object data, in cases where network connectivity is poor, or if your application must process only subsets of object data.


Use lifecycle rules to clean up incomplete multipart uploads automatically.

An Amazon S3 Lifecycle configuration is an XML file that consists of a set of rules with predefined actions that you want Amazon S3 to perform on objects during their lifetime. As a best practice, we recommend you configure a lifecycle rule using the AbortIncompleteMultipartUpload action to minimize your storage costs.



Managing your storage lifecycle

If you keep manually changing your objects, such as your employee photos, from storage tier to storage tier, you might want to automate the process by configuring their Amazon S3 lifecycle. When you define a lifecycle configuration for an object or group of objects, you can choose to automate between two types of actions: transition and expiration.


The following use cases are good candidates for the use of lifecycle configuration rules:

Periodic logs: If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them.
Data that changes in access frequency: Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them. But your organization or regulations might require you to archive them for a specific period. After that, you can delete them.
----------------------------------------------------------------------
Amazon S3 Transfer Acceleration
–
You can use Amazon S3 Transfer Acceleration for fast, easy, and secure transfers of files over long distances. It takes advantage of Amazon CloudFront globally distributed edge locations, routing data to Amazon S3 over an optimized network path.

Transfer Acceleration is best suited for scenarios in which you want to transfer data to a central location from all over the world or transfer significant amounts of data across continents regularly. It can also help you use your available bandwidth when uploading to Amazon S3.




Amazon S3 Transfer Acceleration
    Secure transfers of files over long distances
AWS Transfer Family
    Supports SFTP, FTPS
Amazon Kinesis Data Firehose
    Near-real-time analytics with the business intelligence
AWS DataSync
    Moves large amounts of data online between on premises and Amazon S3
Amazon Kinesis Data Streams
    Continuously captures and stores TBs of data per hour.
Amazon Partner Network
    Use of third-party connectors



"Online data transfer services"

Online data transfer services are a group of offerings that allows you to move your data into and out of the AWS Cloud and Amazon S3 via online, internet based, connections. 

AWS DataSync
AWS Transfer Family
Amazon S3 Transfer Acceleration
Amazon Kinesis Data Firehose
Amazon Kinesis Data Streams
Amazon Partner Network

"Offline data transfer services"

AWS Snowcone
AWS Snowball
AWS Snowmobile


"Hybrid cloud storage services"

If you want to take advantage of the benefits of cloud storage but have applications running on-premises that require low-latency access to their data, or you need rapid data transfer to the cloud; then you can use AWS hybrid cloud storage architectures. 

AWS Direct Connect
AWS Storage Gateway
// You can use the AWS Storage Gateway, in File Gateway mode, to store your on premises data in an existing Amazon S3 bucket. You can deploy AWS Storage Gateway as a virtual appliance or purchase a hardware appliance version.

// AWS Storage Gateway configured as a File Gateway enables you to connect your Amazon S3 bucket using either the Network File System (NFS) or Server Message Block (SMB) protocol with local caching. You can transfer your data using an AWS File Storage Gateway over the internet or over an AWS Direct Connect connection.
The two requirements that must be met before using Amazon S3 File Gateway are:

Configure your private networking, VPN, or AWS Direct Connect between your Amazon Virtual Private Cloud (Amazon VPC) and the on-premises environment where you are deploying your gateway.

Configure Microsoft Active Directory (AD).

These are essential steps to ensure proper connectivity and authentication between your on-premises environment and the AWS resources when using Amazon S3 File Gateway.
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
SECURING DATA.
Pre-Signed URLs
–
Pre-Signed URLs are used to grant time-limited access to others with temporary URLs.

        Permissions to the object
        –
        Anyone with valid security credentials can create a presigned URL. However, to successfully access an object, someone who has permission to perform the operation must create the presigned URL.


        Credentials
        –
        The credentials that you can use to create a presigned URL include:

        IAM instance profile: Valid up to 6 hours
        AWS Security Token Service: Valid up to 36 hours when signed with permanent credentials, such as the credentials of the AWS account root user or an IAM user
        IAM user: Valid up to 7 days when using AWS Signature Version 4
        To create a presigned URL that's valid for up to 7 days, first designate IAM user credentials (the access key and secret access key) to the SDK that youre using. Then, generate a presigned URL using AWS Signature Version 4.


        Token expiration
        –
        If you created a presigned URL using a temporary token, then the URL expires when the token expires, even if you created the URL with a later expiration time. 
--------------------
Access control lists
–
Access Control List (ACLs) to make individual objects accessible to authorized users.

Note: Amazon S3 ACLs are a legacy access control mechanism that predates IAM. AWS recommends using Amazon S3 bucket policies or IAM policies for access control.

"Access Policies: or Resource Based Policies:"

Access policy describes who has access to what resources. They attach to your resources, such as buckets and objects, and are also called resource policies. For example, bucket policies and access control lists are resource based policies because you attach them directly to buckets and objects.

User policies or IAM policies are access policies attached to users in your account. You may choose to use one type of policy or a combination of both, to manage permissions with your Amazon S3 resources.

"Bucket policies"

In order to grant other AWS accounts or IAM users access to the bucket and the objects in it, you need to attach a bucket policy. Because you are granting access to a user or account, a bucket policy must define a PRINCIPAL (which is an account, user, role, or service) entity within the policy. You will notice that the "Principal" statement is listed in the policy. Consult the image below as an example. max policy size of 20kb

    "When to use a bucket policy
    Use a bucket policy if:

    You need to grant cross-account permissions to other AWS accounts or users in another account, without using IAM roles.
    Your IAM policies reach the size limits for users, groups, roles. 
    You prefer to keep access control policies in the Amazon S3 environment.
    Although both bucket and user policies support granting permission for all Amazon S3 operations, the user policies are for managing permissions for users in your account. "

    Because bucket policies grant access to another AWS account or IAM user, you must specify the principal, or the user to whom you are granting access, as a "Principal" in the bucket policy.
----------------------------------------------------------------------
Decoupling storage from compute
–
In traditional Hadoop and data warehouse solutions, storage and compute remain tightly coupled, making it difficult to optimize costs and data processing workflows. With Amazon S3, you can cost-effectively store all data types in their native formats. Then, you can launch as many or as few virtual servers as you need using Amazon Elastic Compute Cloud (EC2), and use AWS analytics tools to process your data. You can also optimize your EC2 instances to provide the right ratios of CPU, memory, and bandwidth for best performance. 


Centralized data architecture
–
Amazon S3 makes it easy to build a multi-tenant environment, where many users can bring their own data analytics tools to a common set of data. This improves both cost and data governance over that of traditional solutions, which require multiple copies of data to be distributed across multiple processing platforms.


Integration with clusterless and serverless AWS services
–
You can use Amazon S3 with Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue to query and process data. Amazon S3 also integrates with AWS Lambda serverless computing to run code without provisioning or managing servers. With all of these capabilities, you only pay for the actual amounts of data you process or for the compute time consumed. 


Standardized APIs
–
Amazon S3 REST APIs are easy to use, and supported by most major third-party independent software vendors (ISVs), including leading Apache Hadoop and analytics tool vendors. This allows customers to bring the tools they are comfortable and knowledgeable about to help them perform analytics on data in Amazon S3.


DATA CATALOG.

Thus, an essential component of an Amazon S3-based data lake is the data catalog. The data catalog provides a query-able interfaceE oOf all assets stored in the data lake’s S3 buckets. The design of the data catalog is to provide a single source of truth about the contents of the data lake. 

AWS GLUE.

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data. You can use AWS Glue to organize, cleanse, validate, and format data for storage in a data warehouse or data lake. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. In order to create your data warehouse or data lake, you must catalog this data.

AMAZON ATHENA.

Amazon Athena is an interactive query service that makes it easy for you to analyze data directly in Amazon S3, using standard SQL. You can get results in a matter of seconds. 

AMAZON REDSHIFT SPECTRUM.

A second way to perform in-place querying of data assets in an Amazon S3-based data lake is to use Amazon Redshift Spectrum. Amazon Redshift is a large-scale, managed data warehouse service used with data assets in Amazon S3. However, data assets must be loaded into Amazon Redshift before queries run. 

Because Amazon Athena and Amazon Redshift share a common data catalog and data formats, you can use both Athena and Redshift Spectrum against the same data assets.

You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads.

"Amazon FSx for Lustre and Amazon S3 data lakes"

Amazon FSx for Lustre, is a fully managed file system that is optimized for compute-intensive workloads, such as high performance computing, machine learning, and media data processing workflows.

With Amazon FSx for Lustre, you can launch and run a Lustre file system that can process massive data sets at up to hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.  Amazon FSx for Lustre file systems can link to Amazon S3 buckets, allowing you to access and process data concurrently from a high-performance file system.

Amazon FSx for Lustre can use Amazon S3 as a raw data repository as well as a repository for processed data. It makes it easy to process your cloud datasets in Amazon S3. 

When linked to an S3 bucket, FSx for Lustre transparently presents objects as files, allowing you to run your workload without managing data transfer from S3. As the contents of your S3 bucket change, FSx for Lustre automatically updates your file system with the latest data available to run your workload.
----------------------------------------------------------------------
----------------------------------------------------------------------
CLOUDFRONT - Amazons CDN content delivery network
CLOUDFRONT Origin - EC2,S3,ELB,R53 address
CLOUDFRONT EDGE LOCATION - location where content is cached
CLOUDFRONT Distribution - name given to config settings
----------------------------------------------------------------------
A developer is managing a distributed system that consists of an Application Load Balancer, an SQS queue, and an Auto Scaling group of EC2 instances. The system has been integrated with CloudFront to better serve clients worldwide. To enhance the security of the in-flight data, the developer was instructed to establish an end-to-end SSL connection between the origin and the end-users.

Which TWO options will allow the developer to meet this requirement using CloudFront? (Select TWO.)

Set up an Origin Access Control (OAC) setting
Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution.
Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config.
Configure the Origin Protocol Policy to use HTTPS only``````
Configure the Viewer Protocol Policy to use HTTPS only``````

======================================================================
A website is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer. It also uses CloudFront with a default domain name to distribute its static assets and dynamic contents. However, the website has a poor search ranking as it doesn’t use a secure HTTPS/SSL on its site.

Which are the possible solutions that the developer can implement in order to set up HTTPS communication between the viewers and CloudFront? (Select TWO.)


Set the Viewer Protocol Policy to use Redirect HTTP to HTTPS.```````
Configure the ALB to use its default SSL/TLS certificate.
Use a self-signed certificate in the ALB.

Set the Viewer Protocol Policy to use HTTPS Only.`````````````
Use a self-signed SSL/TLS certificate in the ALB which is stored in a private S3 bucket.
----------------------------------------------------------------------
CLOUDFRONT - Amazons CDN content delivery network
CLOUDFRONT Origin - EC2,S3,ELB,R53 address
CLOUDFRONT EDGE LOCATION - location where content is cached
CLOUDFRONT Distribution - name given to config settings

AWS operates 200+ edge locations
it also works with non-aws origin server 
default TTL is 1 day - time to live is age for cached objects
you can clear the cache yourself if you have made changes to your page

Cloudfront S3 Transfer Acceleration enables fast, secure transfer of files b/w your s3 bucket and your end-users - if you want your users to upload files at your s3 bucket at london, they can upload via edge location via their local network.--> As data arrives at edge location, it is routed to your Amazon S3 in london, over an optimized network path of AWS
           in cloudfront settings - for Transfer acceleration, select 3rd
            Allowed HTTP methods
            GET, HEAD
            GET, HEAD, OPTIONS
            GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE
Edge locations are not Read-only, you can write to them too.

Restrict viewer access (GOOD OPTION FOR PAID CONTENT)
If you restrict viewer access, viewers must use CloudFront signed URLs or signed cookies to access your content.
No
Yes
INVALIDATION - give path to delete cache for that file

OAI - Origin Access Identity
to block public access at bucket level and serve content only through cloudfront url
When creating cloudfront distribution - set 
        Origin accessInfo Public
        `Bucket must allow public access.
        `Origin access control settings (recommended)
        `Bucket can restrict access to only CloudFront.
         Legacy access identities
Use a CloudFront origin access identity (OAI) to access the S3 bucket.


CLOUDFRONT Allowed HTTP Methods
Allowed HTTP methods
GET, (allow you to read data)
GET, HEAD (allow you to read data and header)
GET, HEAD, OPTIONS
GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE (if your users need read & write access to your website)
   PUT is idenpotent - write data (send data to create new resource)
   PATCH (partial modify)= modify contents of shopping cart 
   POST (insert data)= comment on a blog post
   DELETE (also write action) = deleting your email address
   OPTIONS (receive a list of supported HTTP methods)

LAMBDA@EDGE--------------- 
to override behavior of request and responses 
4 avl functions 
1-Viewer request -when cloudfront receives a request from the viewer
2-Origin request -before cloudfront forwards a request to the origin
3-Origin response -when cloudfront receives a response from the origin
4-Viewer response -before cloudfront returns the response to the viewer

--> Viewer request -->          --> Origin request -->
                        CF
<-- Viewer response <--         <-- Origin response <--

    Usecase: you have protected content, paid content
    you can authenticate with cognito
    A to B testing website 

CLOUDFRONT_PROTECTION-------------------(Use signed URLs)
Access to Cached content can be protected via signed urls/signed cookies
by default a distribution allows everyone to have access 
Original Access Identity: OAI 
A virtual user identity that will be used to give your cloudfront distribution permission to fetch a private object 

IN-ORDER TO USE SIGNED URLS OR SIGNED COOKIES, YOU NEED TO HAVE OAI .
Signed URLs: not same as S3 pre-signed url
A url that provides temporary access to cached objects 
Signed Cookies:
A cookie which is passed along with the request to cloudfront, 
adv- provide access to restricted files e.g. Video Streaming

once a signed url passes a cookie, you can have access for as long as cookie is valid
BLOCK INDIA.
Amazon CloudFront: CloudFront can be used in combination with AWS WAF to restrict access based on geolocation. You can configure CloudFront to use AWS WAF rules for filtering traffic, including blocking or allowing access from specific countries.
--------------------------------------------------------------------------
ATHENA
interactive query service using standard SQL
serverless 
no need to set up complex Extract/Transform/Load (ETL) processes
works directly with data stored on S3
USES:
    QUERY log files stored on s3 
    PERFORM aws costs and usage reports
    GENERATE business reports on data stored on s3
    ANALYZE - run queries on click-stream data

LAB: 
1- configure a trail in CloudTrail 
2- CloudTrail sends logs to S3 
3- create another S3 for athena results. add it in setting of athena
3- Create an Athena Table to query data stored in S3 with SQL
QUERY 1 - creates a database
CREATE DATABASE athenadb
QUERY 2 - creates a table
CREATE EXTERNAL TABLE cloudtrail_logs (
eventversion STRING,
useridentity STRUCT<
               type:STRING,
               principalid:STRING,
               arn:STRING,
               accountid:STRING,
               invokedby:STRING,
               accesskeyid:STRING,
               userName:STRING,
sessioncontext:STRUCT<
attributes:STRUCT<
               mfaauthenticated:STRING,
               creationdate:STRING>,
sessionissuer:STRUCT<  
               type:STRING,
               principalId:STRING,
               arn:STRING, 
               accountId:STRING,
               userName:STRING>>>,
eventtime STRING,
eventsource STRING,
eventname STRING,
awsregion STRING,
sourceipaddress STRING,
useragent STRING,
errorcode STRING,
errormessage STRING,
requestparameters STRING,
responseelements STRING,
additionaleventdata STRING,
requestid STRING,
eventid STRING,
resources ARRAY<STRUCT<
               ARN:STRING,
               accountId:STRING,
               type:STRING>>,
eventtype STRING,
apiversion STRING,
readonly STRING,
recipientaccountid STRING,
serviceeventdetails STRING,
sharedeventid STRING,
vpcendpointid STRING
)
ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION 's3://aws-cloudtrail-logs-928151643990-20c29fa1/AWSLogs/928151643990/';

QUERY 3 - outputs mentioned fields from data
SELECT
 useridentity.arn,
 eventname,
 sourceipaddress,
 eventtime
FROM cloudtrail_logs
LIMIT 100;

------------------------------------------------------------------------
You would like to configure your S3 bucket to only serve content over HTTPS/SSL and explicitly deny all unencrypted HTTP access. In your bucket policy, how should you specify the type of request that should be denied?
{
  "Id": "ExamplePolicy",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowSSLRequestsOnly",
      "Action": "s3:*",
      "Effect": "Deny",
      "Resource": [
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
      ],
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      },
      "Principal": "*"
    }
  ]
}
Explicitly denying requests that are identified as "aws:SecureTransport": "false" would deny requests that are using HTTP and are unencrypted.

A developer is working on an application in her AWS account that uses Amazon S3 to store sensitive data. To enhance security, the developer wants to ensure that all S3 buckets in the application are not publicly accessible. Which of the following actions should the developer take to meet this requirement? Choose the best option.
    Enable block public access settings at the account level to apply to all current and future S3 buckets in the account.


You would like to configure your S3 bucket to deny put object requests that do not use server-side encryption. Which bucket policy can you use to deny permissions to upload objects, unless the request includes server-side encryption?

{
                "Version": "2012-10-17",
                "Id": "PutObjPolicy",
                "Statement": [
                    {
                        "Sid": "DenyUnEncryptedObjectUploads",
                        "Effect": "Deny",
                        "Principal": "*",
                        "Action": "s3:PutObject",
                        "Resource": "arn:aws:s3:::bucket/*",
                        "Condition": {
                            "Null": {
                                "s3:x-amz-server-side-encryption": "true"
                            }
                        }
                    }
                ]
            }
The condition above looks for a Null value for the s3:x-amz-server-side-encryption key. If this condition is true, it means the request is Null and does not include server-side encryption. Setting the condition in the condition policy to "s3:x-amz-server-side-encryption": "true" with "Effect": "Deny" and "Action": "s3:PutObject" would deny put object requests that do not use server-side encryption


Which S3 storage classq is ideal for backup and disaster recovery use cases, when large sets of data occasionally need to be retrieved in minutes, without concern for costs?
        Amazon S3 Glacier Flexible Retrieval (Formerly S3 Glacier)
    S3 Glacier Flexible Retrieval (formerly S3 Glacier) is the ideal storage classs for archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases.


-------------------------------------
A storage specialist of a company created a bucket named 'demobucket2022' for the purpose of storing files for the creation of a proof of concept (POC). After the successful implementation of proof of concept, the team wanted to leverage the bucket for the development work because of the content which was generated during POC. However, they didnt want to name it 'demobucket2022'.

What is the solution for the team to proceed with this implementation of the requirement?


Raise a support ticket and work with AWS support to rename the bucket.

``````````Amazon S3 buckets cannot be renamed once created. So, a new bucket needs to be created and copy the content using cp/sync command from the AWS CLI.

Use the AWS Management Console and rename the bucket using rename bucket option.

Use AWS CLI and update the name of the bucket using the update bucket api.

=======================================
A company runs a batch application in the AWS Cloud hosted on 200+ Amazon EC2 instances. As a solutions architect, you are asked to push debug logs to an Amazon S3 bucket every 2:00 AM for all the EC2 instances.

What is the best possible solution from an operation point of view?


Use Systems Manager Distributor to transfer the logs every 2:00 AM on all the AWS Systems Manager Managed instances.

Inject a user script via Ops Work to all of the Amazon EC2 instances that will push the logs to this Amazon S3 bucket.

``````Create a schedule in AWS Systems Manager Maintenance window to move the logs to S3 bucket every 2:00 AM in the morning.

Use SSM Session Manager to run a shell script on all Amazon EC2 instances 2:00 AM in the morning.
===========================================================================
A customer wants to host their domain in AWS using Amazon Route 53 service. They plan to use Amazon CloudFront for distributing their website globally. All the static assets of the website will be served from Amazon S3 and Amazon EC2 will be used for serving the dynamic content via Amazon CloudFront. The customer needs a way to integrate Amazon Route 53 with AWS CloudFront and Amazon S3.

Which strategy would you recommend that is the easiest to configure?


"They are integrated by default."

It is possible to integrate Amazon Route53 with other AWS services like CloudFront and S3 via creating an alias record in Route 53 that points to CloudFront distributions and S3 buckets.

It is possible, however, we need to insert the code for the integration of other AWS Services in our application which will be using Amazon Route 53.

Use an AWS Lambda function with API Gateway to redirect the API calls from Amazon Route53 to Amazon CloudFront and from there to Amazon S3 and Amazon EC2.
============================================================================
A multi-national company has services across the globe which has a web application as its customer-facing frontend. One of the features of the app is to allow users to be able to upload huge amounts of files. As part of their architecture, they use a single bucket in the us-east-1 region and all the data from users is uploaded to this bucket. Now, as the demand for applications has grown, more and more users are using the application, which has led to an increase in file uploads. Because the users are across the globe, the upload takes time when the users are geographically far from the us-east-1region, which leads to a degraded user experience. You need to improve the upload experience without making major code level changes.

Which is the best service that you could use to achieve this requirement?


AWS DataSync

Amazon Partner Network

``````Amazon S3 Transfer Acceleration

AWS Transfer Family

Amazon S3 Transfer Acceleration utilizes Amazon CloudFronts globally distributed edge locations to accelerate the upload of objects to an S3 bucket. This service optimizes the transport path and provides a faster and more consistent user experience for uploads, regardless of the user's location.

This means that users from around the world can upload their files with improved speed and efficiency. It doesn't require significant code changes in your application, making it a suitable solution for your scenario.

====================================================================
Your organizations operational lead has decided to build a cost effective centralized logging solution for multiple AWS accounts. The solution includes an Amazon S3 bucket in the centralized logging account. The logs must be deleted after three months to minimize cost.

What is the most operationally efficient way to accomplish this task?


Transition objects to the S3 Standard-IA storage classs 30 days after creating them and delete manually/

````Expiration actions

Transition actions

Archive objects to the S3 Glacier Flexible Retrieval storage classs for three months and delete manually.

With expiration actions, you can set up a lifecycle policy in the S3 bucket to automatically delete objects that are older than a specified number of days. In this case, you would configure the policy to delete logs after three months, which helps minimize cost by removing outdated data.


========================================================================
A public-facing weather application stores millions of weather forecast files in an Amazon S3 bucket. During bad weather, the bucket can experience tens of thousands of GET requests per second. The developer must avoid Amazon S3 reads throttling because of request rate limits. Which solutions meet the requirements? (Select TWO.)

"Adopt an Object key naming pattern that distributes the objects across multiple prefixes."

Open an AWS support ticket to increase the request-per-second quota on the Amazon S3 bucket.

Create multiple access points for the Amazon S3 bucket. Adopt a naming pattern thatdistributes traffic to different access points. Adjust the application to use the multiple accesspoints.

Adjust the Amazon S3 bucket settings for Minimum TTL, Maximum TTL, and Default TTL to control cache behavior.

"Create an Amazon CloudFront web distribution with the Amazon S3 bucket as an origin. Set the distribution's Minimum TTL, Maximum TTL, and Default TTL to control cache behavior. Adjust the web application to use the CloudFront distribution domain name."


=============================================================================================================
Your manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, which will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when you access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption.

To properly upload data to this bucket, which of the following headers must be included in your request?


x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers

x-amz-server-side-encryption-customer-key header only

x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers

x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers`````````````````````````````````````````


Incorrect
Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.


x-amz-server-side-encryption-customer-algorithm – This header specifies the encryption algorithm. The header value must be “AES256”.

x-amz-server-side-encryption-customer-key – This header provides the 256-bit, base64-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data.

x-amz-server-side-encryption-customer-key-MD5 – This header provides the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error.

Hence, the correct answer is to include the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers on the upload request.

Including the x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers in the upload request is incorrect because these headers are primarily used in Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS) and not for Server-Side Encryption with Customer-Provided Keys (SSE-C).

Including the x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers is incorrect because the x-amz-server-side-encryption header is not used in SSE-C encryption. This should be replaced with the x-amz-server-side​-encryption​-customer-algorithm header.

Including just the x-amz-server-side-encryption-customer-key header only is incorrect because you have to include the x-amz-server-side​-encryption​-customer-algorithm and x-amz-server-side-encryption-customer-key-MD5 headers as well to upload the objects to the S3 bucket with SSE-C encryption.

============================================================================================================
A recruitment agency has a large collection of resumes stored in an Amazon S3 bucket. The agency wants to perform an analysis on these files, but for privacy compliance reasons, they need to ensure that certain personally identifiable information (PII) is redacted before being processed by their internal service.

Which solution can meet the requirements in the most cost-effective way?

Implement a solution with AWS Glue to transform the data and redact PII before storing it in an S3 bucket.

Use Amazon S3 Object Lambda to redact PII before it is returned to the application.`````````````

Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda@Edge function to redact the PII as data is fetched from the S3 bucket.
Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files.


Correct
S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it’s being returned to an application. This feature is designed for use cases where data needs to be transformed on-the-fly without the need to store a transformed copy of the data. It’s useful in scenarios like filtering rows, redacting confidential data, dynamically resizing images and other similar situations where data transformation or processing is required during data retrieval.

=================================================================================================================
A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.

Which solution would reduce the load on the Fargate tasks in the most operationally efficient manner?

Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.
Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.`````````````
Enable auto-scaling on the Fargate tasks.
Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.

Incorrect
CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:

– Cache key normalization – You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.

– Header manipulation – You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a True-Client-IP header to every request.

– Status code modification and body generation – You can evaluate headers and respond back to viewers with customized content.

– URL redirects or rewrites – You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.

– Request authorization – You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.

When you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.

CloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).



    In the given scenario, by attaching the validation function to the Viewer Request event, requests can be authenticated right when they hit the CloudFront cache and before they are forwarded to your AWS Fargate service. This method not only helps in reducing unnecessary traffic to your Fargate tasks but also improves overall latency for valid requests, as they don’t have to wait behind unauthenticated requests being processed by your backend infrastructure. In addition, because CloudFront Functions operate at the edge locations of AWS’s infrastructure, they are highly scalable and can handle a very high number of requests per second.

    Hence, the correct answer is the option that says: Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.

    The option that says: Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function is incorrect. This option doesn’t eliminate the problem of unauthenticated requests reaching the backend infrastructure. All requests, regardless of authentication, will still need to traverse the network to reach the ALB and Lambda, which is not operationally efficient and could increase latency.

    The option that says: Enable auto-scaling on the Fargate tasks is incorrect.  While auto-scaling could help handle the increased load by adding more tasks as demand increases, it is a reactive measure and not a long-term solution. It doesn’t prevent unauthenticated requests from consuming resources on Fargate.

    The option that says: Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution is incorrect. The Origin Response event is triggered after the request has been processed by your origin (in this case, the ALB) and the origin has returned a response to CloudFront. Validating the JWT at this stage would not reduce the number of unauthenticated requests reaching your Fargate service. It’s too late in the request processing flow to prevent unauthenticated requests from consuming resources on your backend infrastructure. Moreover, this could also introduce errors in the CloudFront flow.

    ============================================================================================================
    A company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a number of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also getting HTTP 504 errors which is why the developer was instructed to fix this problem immediately.

Which of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select TWO.)


Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.
Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance.
'Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.'
/Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses./
Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user.
Incorrect
Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:

– After CloudFront receives a request from a viewer (viewer request)

– Before CloudFront forwards the request to the origin (origin request)

– After CloudFront receives the response from the origin (origin response)

– Before CloudFront forwards the response to the viewer



In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin, which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing.

Hence, the correct answers in this scenario are:

– Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.

– Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.

The option that says: Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with minimal cost.

The option that says: Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance is incorrect. Although setting up multiple VPCs across various regions which are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.

The option that says: Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the slow authentication process of your global users and not just the caching of the static objects.

================================================================================================================
A company is legally obligated to keep transaction records containing Personally Identifiable Information (PII) for a duration of five years. These records are stored in Amazon S3. To handle data redaction, the company has developed Lambda functions with naming conventions starting as RedactPII-[role], where represents different roles.

The company wants to provide varying levels of redaction based on each role, ensuring each user only sees the necessary data. Only a single copy of the records should be maintained.

Which combination of actions will achieve the given requirements? (Select THREE.)


/Use the GetObject API to retrieve the redacted data/

Use the GetObjectLegalHold API to retrieve the redacted data.

/Configure an S3 Object Lambda Access Point for each S3 Access Point name. Associate the RedactPII-[role] Lambda functions with the corresponding S3 Object Lambda Access Point./

Set up S3 Replication for the bucket.

Set up an S3 event notification to invoke the corresponding RedactPII-[role] function in response to GET requests.

/Create an S3 Access Point for each user role./

Incorrect
S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it’s being returned to an application. This feature is designed for use cases where data needs to be transformed on the fly without the need to store a transformed copy of the data. It’s useful in scenarios like filtering rows, redacting confidential data, dynamically resizing images, and other similar situations where data transformation or processing is required during data retrieval.


In the scenario, we have to create dedicated S3 Object Lambda Access Points, with each user having access to their own unique Access Point. Each of these Access Points is associated with the corresponding RedactPII-[role] Lambda function. On the client side, users issue standard GetObject requests to their specific S3 Object Lambda Access Point, allowing them to retrieve redacted data tailored to their role. This setup ensures that each user can only access the data associated with their role while maintaining the requirement of a single copy of the records in the S3 bucket.

=============================================================================================================
An application running in an EC2 instance is regularly fetching and processing a lot of data from an S3 bucket which resulted in a significant increase in your operating costs. You want to lower the latency of retrieving data from S3 and bring the operating costs down. To improve the system, you need to use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need.

Which is the MOST suitable service that will help you accomplish this requirement?

Athena
AWS Step Functions
S3 Select`````````````````````
Redshift Spectrum
Incorrect
With Amazon S3 Select, you can use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency of retrieving these data.

==================================================================================================================
	
A developer is building an application that uses Amazon CloudFront to distribute thousands of images stored in an S3 bucket. The developer needs a fast and cost-efficient solution that will allow him to update the images immediately without waiting for the object’s expiration date.

Which solution meets the requirements?

(view)	1	0	1	00:00:41	
 Update the images by invalidating them from the edge caches.
 Update the images by using versioned file names.```````````````````
 Disable the CloudFront distribution and re-enable it to update the images in all edge locations.
 Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes.
When you update existing files in a CloudFront distribution, AWS recommends that you include some sort of version identifier either in your file names or in your directory names to give yourself better control over your content. This identifier might be a date-time stamp, a sequential number, or some other method of distinguishing two versions of the same object.



For example, instead of naming a graphic file image.jpg, you might call it image_1.jpg. When you want to start serving a new version of the file, you'd name the new file image_2.jpg, and you'd update the links in your web application or website to point to image_2.jpg. Alternatively, you might put all graphics in an images_v1 directory and, when you want to start serving new versions of one or more graphics, you'd create a new images_v2 directory, and you'd update your links to point to that directory. With versioning, you don't have to wait for an object to expire before CloudFront begins to serve a new version of it, and you don't have to pay for object invalidation.

Hence, the correct answer is: Update the images by using versioned file names.