Which of the following statements are true regarding the concept of blue/green deployment when it comes to the development and deployment of your application?

"It allows you to shift traffic between two identical environments that are running different versions of your application.

    With blue/green deployment, you can shift traffic between two identical environments that are running different versions of your application. It allows you to easily deploy changes to your application and rollback on changes very quickly."

Reference: Blue/Green Deployments on AWS

Selected
The green environment represents the production environment.

No, the green environment does not represent the production environment.

Reference: Blue/Green Deployments on AWS

Selected
"The green environment is staged running a different version of your application.

    Yes, the green environment is staged running a different version of your application."  

Reference: Blue/Green Deployments on AWS

The blue environment is staged running a different version of your application.

"The blue environment represents the current application version serving production traffic.

    Yes, the blue environment does represent the current application version serving production traffic."

Reference: Blue/Green Deployments on AWS
---------------------------------------------------------------------------
ou are building a serverless web application that will serve both static and dynamic content. Which of the following services would you use to create your application?

EC2

ElastiCache

"API Gateway

    API Gateway is a managed service which makes APIs available to your user base in a secure way, so it is a great option for a serverless application. Reference: Serverless on AWS"

"Lambda

    Lambda lets you run code without provisioning servers, so it is a great option for a serverless application."

Reference: Serverless on AWS

Selected
RDS

RDS is not suitable for this application because it is not serverless.

Selected
"S3

    S3 can be used to serve static web content, so it is a great option for a serverless application."
---------------------------------------------------------------------------
You are working on a web application that needs somewhere to store user session state across a fleet of instances. Which of the following options could you use to deal with user session state?

Store session state locally on the EC2 instance.

Store session state in memory.

Storing session state in memory is not a good option because you cant share data at high speed across a fleet of instances.

Selected
"Store session state in DynamoDB.

    DynamoDB is not optimal, but could be used for storing session state."

Reference: Managing ASP.NET Session State with Amazon DynamoDB

"Use an ElastiCache cluster.

    ElastiCache is the best option for storing session state as it is scalable, highly available, and can be accessed by multiple web servers."
---------------------------------------------------------------------------
You are deploying an application that runs in Lambda and processes messages from a Simple Queue Service (SQS) queue. During testing, you discover that occasionally the same SQS messages are received multiple times, causing Lambda to process the same message multiple times and creating problems for the application support team. Which of the following should you do to prevent duplicate messages from being processed?

Disable the automated retry for your function to prevent it from re-processing messages that are already processed.

"Create a DynamoDB table to store the SQS message IDs of the messages that have been successfully processed. Configure the Lambda function to check the DynamoDB table to see if a message has already been successfully processed and only process messages that are not already processed.

    If you are receiving multiple messages with the same message ID, you can use DynamoDB to record the message IDs of messages that were already successfully processed and have your Lambda function check the DynamoDB table before processing each message."

Reference: How Can I Prevent an Amazon SQS Message from Invoking My Lambda Function More than Once?

Configure a dead-letter queue to prevent your function from processing messages that are already processed.

    A dead-letter queue is used to save discarded events for further processing. It will not prevent your function from processing duplicate messages received from SQS.

Selected
Configure an on-failure destination to prevent your function from processing messages that are already processed.
---------------------------------------------------------------------------
You are building a simple banking system using serverless technologies. The workflow to authorize a new bank account includes two Lambda functions: one that confirms the customer identity and another that confirms the customer address. Together, these processes should complete in under five minutes. You would like to execute these tasks independently and, in parallel, you also need the ability to handle errors and retries. After the checks are successfully completed, the customer will be approved for a new bank account and a second workflow will be invoked to create the account and inform the customer. You would like to orchestrate everything using Step Functions. Which of the following workflows would be the most suitable?

"Synchronous Express workflow

    Synchronous Express workflows start a workflow, wait until it completes, and then return the result. Synchronous Express workflows can be used to orchestrate microservices and allow you to develop applications without the need to develop additional code to handle errors and retries or execute parallel tasks.
"
Asynchronous Express workflow

Asynchronous Express workflows return confirmation that the workflow was started, but does not wait for the workflow to complete.

Selected
Standard workflow

Step Functions workflow
---------------------------------------------------------------------------
Your application needs to process a large amount of job requests. You need to ensure that they are processed in order and that each request is processed only once. How would you deploy Amazon Simple Queue Service (SQS) to achieve this end?

Convert your standard queue to a First-In-First-Out (FIFO) queue by renaming your standard queue with the .fifo suffix.

Configure First-In-First-Out (FIFO) delivery in a standard SQS queue.

Amazon SQS standard queues provide the highest scalability and throughput; however, the order that messages are processed is not guaranteed and duplicates might occur. You cant configure FIFO delivery in a standard queue.

Selected
"Use an SQS First-In-First-Out (FIFO) queue to process the jobs.

    FIFO queues offer First-In-First-Out (FIFO) delivery and exactly-once processing. The order in which messages are sent and received is strictly preserved and a message is delivered once and remains available until a consumer processes and deletes it. Duplicates are not introduced into the queue."
---------------------------------------------------------------------------
You are developing a web application that has been deployed using Lambda. Today, you updated the code and uploaded the new version of your code to the Lambda console. Your test team has begun their testing, and has reported that the application seems to still be using the original code. What could be the reason for this?

Your application is referencing the function using $LATEST.

"Your application is referencing the function using an alias that points to a previous version of the code.

    The problem could be that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function so that you can use different variations of your Lambda function in your development workflow, such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions."

Reference: Lambda Function Aliases Reference: Lambda Function Versions

Selected
Your application is referencing the function using an unqualified ARN.

After deploying the function, there is usually a delay in which the new function is initialized. During this time, the application is directed to the previous version of the function.
---------------------------------------------------------------------------
You have three separate environments in your AWS account and three corresponding stages in the API Gateway. You are using the API Gateway as a HTTP proxy to a backend endpoint. How do you direct traffic for each environment without creating separate API Gateways?

Use custom authorizers for each stage.

Update the integration response to update the backend endpoints.

Use a request transformation to transform the backend endpoint responses.

"Use stage variables and configure the stage variables in the HTTP integration request to interact with different backend endpoints.

    You can use API Gateway stage variables to access the HTTP and Lambda backends for different API deployment stages."

---------------------------------------------------------------------------



2
---------------------------------------------------------------------------





---------------------------------------------------------------------------
Which DynamoDB feature allows you to set an expiry on table items so that they can automatically be deleted to reduce storage costs?

// DynamoDB lifecycle rules

// Lifecycle rules are a feature of S3, not DynamoDB.

// Selected
// DynamoDB Streams

// DynamoDB provisioned throughput

"DynamoDB TTL

Time To Live (TTL) allows you to define when items in a table expire. This means that they can be automatically deleted from the database, enabling you to reduce storage usage by deleting data that is no longer relevant to your application."
---------------------------------------------------------------------------
You are using CloudFormation to build a number of different application environments to host development, test, UAT, pre-production, and production stacks. Your application is comprised of web servers, load balancers, application servers, and databases. Each web server, load balancer, and database needs to be configured identically across all environments. You would also like to reduce the duplication of code to avoid mistakes caused by human error. How can you achieve this with CloudFormation?

"Use a CloudFormation nested stack.

Nested stacks provide the ability to configure multiple elements within your environment while reducing duplication of code. As your infrastructure grows, common patterns can emerge in which you declare the same components in multiple templates. You can separate out these common components and create dedicated templates for them. Then, use the resource in your template to reference other templates, creating nested stacks."

// Reference: Nested Stacks to Create Reusable Templates

// Copy and paste the configuration code that you want to reuse into the CloudFormation template for each environment.

// Use environment variables.

// Use the Mappings section of the template to reference the code you want to reuse.

// The Mappings section of a CloudFormation template is used to match a key to a corresponding set of named values. For example, if you want to set values based on a Region, you can create a mapping that uses the Region name as a key and contains the values you want to specify for each specific Region. Mappings cannot be used to reduce the duplication of code when you need to provision components like web servers, load balancers, application servers, and databases that need to be configured identically across all environments.
---------------------------------------------------------------------------
You are building an application that collects all the lifecycle events of EC2 instances across multiple AWS accounts. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in your companys main AWS account for further processing. Which of the following will best meet these requirements?

// Use the resource policies of the SQS queue in the main account to give each account permissions to write to the SQS queue. Add to the EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.

// Adding an EventBridge rule that matches all EC2 instance lifecycle events will not enable EventBridge to send events to other AWS accounts.

Selected
"Configure the permissions on the main account event bus to receive events from all accounts. Create an EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.

You can easily configure EventBridge to send events to other AWS accounts. On the receiver account, edit the permissions on an event bus to allow specified AWS accounts, an organization, or all AWS accounts to send events to the receiver account. On the sender account, set up one or more rules that have the receiver accounts event bus as the target. Reference: Sending and Receiving Amazon EventBridge Events between AWS Accounts"

// Configure EC2 to deliver the lifecycle events from all accounts to the EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.

// Write a Lambda function that scans all EC2 instances in the company accounts to detect instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an EventBridge scheduled rule that invokes the Lambda function every minute.
---------------------------------------------------------------------------
Which of the following AWS services enables you to capture a time-ordered sequence of any modifications that happened to the items in your DynamoDB table over the past 24 hours?

// CloudTrail

// CloudTrail is used to record an audit trail of all the user activity in your AWS account. It cannot be used to capture a time-ordered sequence of any modifications that happened to the items in your DynamoDB table.

// Selected
// DynamoDB TTL

"DynamoDB Streams

DynamoDB Streams captures a time-ordered sequence of modifications that are made to items in a DynamoDB table. It stores the information for a maximum of 24 hours."
---------------------------------------------------------------------------
---------------------------------------------------------------------------





3
---------------------------------------------------------------------------






---------------------------------------------------------------------------
You are working for an investment bank and have been asked to help the application support team with their annual disaster recovery testing. The main production PostgreSQL database is hosted in a RDS Multi-AZ deployment, with multiple applications running on a combination of EC2 and Lambda. You have been asked to help the team demonstrate the impact that a failed Availability Zone (AZ) will have on the database. Which of the following would you suggest?

'Simulate an AZ failure by performing a reboot with forced failover on the RDS instance.

        If the Amazon RDS instance is configured for Multi-AZ, you can perform the reboot with a failover. An Amazon RDS event is created when the reboot is completed. If your DB instance is a Multi-AZ deployment, you can force a failover from one AZ to another when you reboot. When you force a failover of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone, and updates the DNS record for the DB instance to point to the standby DB instance. As a result, you need to clean up and re-establish any existing connections to your DB instance. Rebooting with failover is beneficial when you want to simulate a failure of a DB instance for testing, or restore operations to the original AZ after a failover occurs.'

Reference: Rebooting a DB Instance

Simulate an AZ failure by rebooting the underlying EC2 instance that is running the database.

Simulate an AZ failure by deleting the primary RDS instance.

Simulate an AZ failure by disconnecting your RDS instance from the network.

Simulate an AZ failure by moving your RDS instance to a different subnet.

Moving your RDS instance to a different subnet is not the AWS recommended way to simulate an AZ failure.
---------------------------------------------------------------------------
Your application is using Kinesis to ingest data from a number of environmental sensors that continuously monitor for pollution within a one-mile radius of a local primary school. An EC2 instance consumes the data from the stream using the Kinesis Client Library (KCL). You have recently increased the number of shards in your stream to six, and your project manager is now suggesting that you need to add at least six additional EC2 instances to cope with the new shards. What would be your response to this suggestion?

The number of instances should be greater than the number of shards.

You should decrease the number of shards to match the number of consumer instances.

'One worker can process any number of shards, so its fine if the number of shards exceeds the number of instances.

    Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so its fine if the number of shards exceeds the number of instances. When resharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group, and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load.'

Reference: Kinesis Data Streams Terminology and Concepts Reference: Resharding, Scaling, and Parallel Processing

Selected
You should increase the number of instances to match the number of shards.
---------------------------------------------------------------------------
Your application is using Kinesis to ingest clickstream data relating to your products from a variety of social media sites. Your company has been trending this quarter because a high-profile movie star has recently signed a contract to endorse your products. As a result, the amount of data flowing through Kinesis has increased, requiring you to increase the number of shards in your stream from four to six. The application consuming the data runs on a single EC2 instance in an Auto Scaling group in us-east-1a. How many consumer instances will you now need in total to cope with the increased number of shards?

'One instance in us-east-1a and one instance in us-east-1b.

Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one Kinesis Client Library (KCL) worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so its fine if the number of shards exceeds the number of instances. When resharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load. Having the instances in different Availability Zones helps with availability within the Region.'

Reference: Kinesis Data Streams Terminology and Concepts Reference: Resharding, Scaling, and Parallel Processing

Selected
Six instances in us-east-1a and six instances in us-east-1b.

Six instances in us-east-1a and one instance in us-east-1b.

Three instances in us-east-1a and three instances in us-east-1b.
---------------------------------------------------------------------------

You are developing a website which allows customers to purchase tickets to popular sporting events. Your application uses S3 for static web hosting, Lambda for business logic, RDS to store transaction data, and DynamoDB for product and stock information. After the customer has paid for their purchase, a message is sent to an SQS queue to trigger a confirmation email to be sent out to the customer including an e-ticket for their chosen event. You want to send out the email as soon as the payment has been processed; however, during testing, you discover that the confirmation emails are being processed a few seconds before the stock control database has finished updating. This sometimes results in selling the same ticket twice. How can you quickly fix this without re-engineering the application?

Use a First-In-First-Out (FIFO) queue to ensure the messages are always processed in the correct order.

'Modify the SQS queue to become a delay queue by setting the delivery delay to 5 seconds. This will allow the database to update before the confirmation emails are processed.

Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is zero seconds. The maximum is 15 minutes.'

Reference: SQS Delay Queues

Selected
Use Kinesis to stream the SQS messages, adding a delay of a few seconds.

Refactor the application to append a message timer to events as they are sent to the SQS queue. Set the message timers DelaySeconds parameter to 5. This will allow the database to update before the confirmation emails are processed.
---------------------------------------------------------------------------
You are working on a flight booking application that runs on a number of EC2 instances. Recently, one of your servers crashed, which meant all of your users lost their sessions and had to log in again. Many of your customers have complained that they had to start their session again from the beginning because your application does not store session state anywhere. Which of the following could you use to persist session state and prevent this from happening?

Elastic Block Storage

'DynamoDB

Many applications store session state data in memory. However, this approach doesnt scale well. After the application grows beyond a single web server, the session state must be shared between servers. Amazon DynamoDB provides an effective and scalable solution for sharing session state across web servers.'

Reference: Managing ASP.NET Session State with Amazon DynamoDB

Selected
S3 Glacier

S3
---------------------------------------------------------------------------
---------------------------------------------------------------------------





4
---------------------------------------------------------------------------






---------------------------------------------------------------------------
An application connects to an external third-party service with API keys being managed by AWS Secrets Manager. The development team uses CodeBuild for source code compilation activities in their CI/CD process. Where should the reference to the third-party service API keys be specified?

CloudFormation template

'Buildspec file

    CodeBuild uses the Buildspec file as a specification of build commands and settings. secrets-manager syntax can be used to retrieve API keys stored in AWS Secrets Manager.'



'CodeBuild environment variable

    A valid way to use CodeBuild environment variables securely is to use secrets-manager syntax to retrieve API keys stored in AWS Secrets Manager. If secrets-manager syntax is not used, items would be in plain text.'

Selected
AppSpec file

The AppSpec file is used by CodeDeploy to specify and manage deployments.
---------------------------------------------------------------------------
You are working on two CloudFormation templates. The first, named mynetwork.yml, creates a new VPC with a public subnet and security group that allows traffic from port 443 from the internet. The second template, named myinstances.yml, will launch a number of EC2 instances into the VPC and public subnet created by the first template, and it will also associate the security group. How can you share the VPC, public subnet, and security group ID so that they can be used in your second template?

'Use the Export field in the Output section of mynetwork.yml.

The Export section can be used to export stack outputs so that they can be used by other templates.'

Selected
Use the Import field in the Input section of myinstances.yml.

Use the Fn:OutputValue function in myinstances.yml.

'Use the Fn::ImportValue function in myinstances.yml.

To import values from another CloudFormation template, you can use the Fn::ImportValue function.'
---------------------------------------------------------------------------
A developer is creating a CloudFormation template to deploy an application stack. The administrator password for the application needs to be configured at CloudFormation runtime. What CloudFormation section should be used for this requirement?

Metadata

Mappings

'Parameters

CloudFormation template parameters section is used to pass input values to the template. The template parameters can be set to custom values when the template is used to create a new stack.'

Reference: Template Anatomy Reference: Parameters

Selected
Resources
---------------------------------------------------------------------------
A business-critical application is deployed using CloudFormation. The team would like to prevent accidental deletion of the stack. How can this be achieved most efficiently?

Set the DeletionProtection to True in the CloudFormation template.

Set the DeletionPolicy to Retain in the CloudFormation template.

'Set stack termination protection to Enable.

    Termination protection stack option can be enabled to prevent accidental deletion of an entire CloudFormation stack.'



Selected
Create IAM policy with Effect of Deny for cloudformation:DeleteStack action.