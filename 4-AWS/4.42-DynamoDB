Which statement about Amazon DynamoDB partitions is true?
O DynamoDB stores data in partitions and chooses the partition based on the range attribute.
O If a table has a simple primary key (partition key only), DynamoDB stores and retrieves each item based on its hash attribute.
O If a table has a composite primary key, DynamoDB will sort the items based on the sort key before selecting the partition for the item.
o A developer writes a hash function to tell DynamoDB how to partition the items.

The correct answer is: If a table has a simple primary key (partition key only),DynamoDB stores and retrieves each item based on its hash attribute.

With a composite primary key, DynamoDB stores all the items that have the same partition key value physically close together. It will then order them by sort key value in the partition.


How far back in time can an Amazon DynamoDB table be restored by using point-in-time recovery?
C) The last week
o The last 10 days
@ The last 35 days-------------------------------------------------------
C) The last calendar month
-------------------------------------------------------------------------
Low-Latency NoSQL database 
serverless, integrates well with lambda, autoscales
performance is good with SSD storage 
resiliance is good, spread across 3 distinct data centers
supports both document and key-value data models.
supported formats are JSON, HTML, XML
item = row 
attribute = column
PARTITION KEY is also called HASH  .
        A GOOD PARTITION KEY IS SHIP CLASS in STARSHIP TABLE because ITs a GROUPING OF THINGS - and as long as you have a unique value with combination of partition and sort key, it is ok
        3 ships have same ship class, etc but 
SORT KEY is also called RANGE .
DynamoDB is a no SQL key/value and Document Store database
key/value = {Title: "S01E019 DS9 Duet"}
Document Store: nested data structure
{
    Series: "DS9",
    Episodes: [{
        Season: 1,
        Episodes: 19,
        Title: "Duet"
    }]
}
DynamoDB is AWS Flagship database:
Fully managed
Multi-region
Multi-master
Durable
Built-in security
Backup and restore
in-memory caching 

rows = items 
columns = attributes 
keys = identifying names of your data 
values = the actual data itself
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Amazon DocumentDB (with MongoDB compatibility)

Amazon DocumentDB is a fully managed document database from AWS. A document database is a type of NoSQL database you can use to store and query rich documents in your application. These types of databases work well for the following use cases: content management systems, profile management, and web and mobile applications. Amazon DocumentDB has API compatibility with MongoDB. This means you can use popular open-source libraries to interact with Amazon DocumentDB, or you can migrate existing databases to Amazon DocumentDB with minimal hassle.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Amazon Keyspaces (for Apache Cassandra)

Amazon Keyspaces is a scalable, highly available, and managed Apache Cassandra compatible database service. Apache Cassandra is a popular option for high-scale applications that need top-tier performance. Amazon Keyspaces is a good option for high-volume applications with straightforward access patterns. With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra Query Language (CQL) code, Apache 2.0 licensed drivers, and tools that you use today.
Amazon Keyspaces (for Apache Cassandra) implements the Apache Cassandra Query Language (CQL) API, so you can use CQL and Cassandra drivers that you already use. Updating your application is as easy as updating your Cassandra driver or cqlsh configuration to point to the Amazon Keyspaces service endpoint.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
DynamoDB SECURITY.
USE CLOUDTRAIL .
THROTTLING
TRANSACTIONS (at no additional cost)
DynamoDB TTL 
PARTITIONS:
PARTITION KEY:
LAB
INDEXES - for efficient Sorting
SCAN vs QUERY API Calls
DynamoDB API CALLS (programatically)
DynamoDB Provisioned Throughput
DYNAMODB GLOBAL TABLES 
DAX:
STREAMS
EXPONENTIAL BACKOFF
USE CASE:
MONITORING:
Contributor Insights for DynamoDB: 
TROUBLESHOOTING:
THROTTLING:
PROBLEMS SOLUTION:
AUTO SCALING:
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
DynamoDB SECURITY.

DynamoDB provides a number of security features to consider as you develop and implement your own security policies. They include the following:
bullet
DynamoDB provides a highly durable storage infrastructure designed for mission-critical and primary data storage. Data is redundantly stored on multiple devices across multiple facilities in a DynamoDB Region.  
bullet
All user data stored in DynamoDB is fully encrypted at rest. DynamoDB encryption at rest provides enhanced security by encrypting all your data at rest using encryption keys stored in AWS Key Management Service (AWS KMS).
bullet
IAM administrators control who can be authenticated and authorized to use DynamoDB resources. You can use IAM to manage access permissions and implement security policies.
bullet

As a managed service, DynamoDB is protected by the AWS global network security procedures.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
USE CLOUDTRAIL .

When activity occurs in DynamoDB, that activity is recorded in CloudTrail ongoing record of events in DynamoDB and in your AWS files to an Amazon Simple Storage Service (Amazon S3)

If you are using an AWS managed key for encryption at rest, usage of the key is recorded in
AWS CloudTrail. CloudTrail can tell you who made the request, the services used, actions
performed, parameters for the action, and response elements returned.
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
THROTTLING:
–
Throttling is the action of limiting the number of requests that a client can submit to a given operation in a given amount of time. Throttling prevents your application from consuming too many capacity units. When a request is throttled, it fails with an HTTP 400 Bad Request error and a ProvisionedThroughputExceededException.

DYNAMODB READ CONSISTANCY:
data is saved to 3 locations AZs(data centers) at once on SSDs

it is possible for data to be inconsistant when reading from a copy which has yet to be updated
solution:---------
CONSISTENCY MODELS:

->Eventually Consistent-default
(consistancy across all copies of data = 1sec)
best for read performance
all copies of data eventually become generally consistant within 1sec
Reads are fast but no guarentee of consistancy

->Strongly Consistent
Reads are slower but with guarentee of consistancy (high latency)
use it when consistant data is crucial for your business logic
best for read consistency
==========================================================================
TRANSACTIONS (at no additional cost)

DDB offers ALL or NOTHING changes to multiple items both WITHIN & ACROSS tables
->DynamoDB Transactions -> supports ACID Transactions
Atomic (all or nothing transaction,if my money is deducted,urs is toppd up)
Consistant (consistant with data validation rules)
Isolated (transactions must happen in isolation,independent of one another)
Durable (they dont disappear if a system crashes)
if any of the steps fail, rollback db as if nothing initiated 
e.g. 
`````````````Transaction start`````````````````````
Create new payee 
Verify email in correct format  X (wrong format so it rolls back)
remove $100 from our account 
`````````````Transaction end`````````````````````
all this happens in DynamoDB using 2 functions:
            TransactWriteItems
            TransactGetItems
DDB performs TWO underlying READS or WRITES of evry item in the transaction
            one to Prepare the Transaction 
            one to Commit the Transaction
These underlying R/W operations are visible in your CloudWatch Metrics
U can also do conditional check with DDB transactions to check if item Exist/Specific Attribute can also be checked
            ConditionCheck

---------------------------------------------------------------------------
DynamoDB TTL 
DDB doesnt have a DateTime datatype
expressed as unix/posix/Epoch time, sec elapsed since Jan 1st, 1970, 12am
1544023618...
defines an expiry time for your data
helps reduce cost and makes db fast
e.g. session data, event logs, old data, tmp data

when the current time is greater than the TTL,
the item is going to be expired and it gets marked for deletion.
And then within the next 48 hours, it's actually going to be deleted.
So that's actually quite a long window between being marked for deletion and
actually getting deleted. So in order to cater for that,
you can actually filter out any expired items from your queries and scans.
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

PARTITIONS:
A partition is hwen you slice your table up into smaller chunks of data (a partition) it speeds up reads for very large tables by logically grouping similar data together
it makes Table FASTER
DynamoDB automatically creates partitions for you as your Data grows
How it works?
you start off with single partition
DynamoDB adds partition when:
    ----- exceed 10GB of data
    ----- exceed RCUs / WCUs for single partition 
          each partition has maximum of 3000 RCUs and 1000 WCUs

 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
PRIMARY KEYS:
allows us to query the data 
can be set only once - CANNOT BE CHANGED LATER
dynamoDB stores and retrieves data based on a primary key
2 types -
Partition Key - unique attribute, e.g. customer id, email id, car reg. no.
->value of the partition key is input to an internal hash function,
    which determines the partition or physical location on which,
    the data is stored. (nobody knows inner workings of hash algorithm)
->if u using partition key as ur prmary key, no 2 items can have same,
    partition key
Composite Key (partition key + sort key)
-> you use composite key, when your partition key is not necessarily unique within your table
e.g. a forum where users are posting multiple comments,
here, user id will not be a unique identifier to retrieve data.
so you user user id + a sort key(timestamp) to save data 
it gives you unique combination required for a primary key in table
-> all items with same partition key are stored together and are then sorted according to the sort key value.

 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

LAB

create a user with access keys
launch an instance 
login
install git 
run command 
aws dynamodb create-table --table-name ProductCatalog --attribute-definitions \
AttributeName=Id,AttributeType=N --key-schema \
AttributeName=Id,KeyType=HASH \ (*hash=primaryKey)
--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

then
3  git clone https://github.com/ACloudGuru-Resources/course-aws-certified-developer-associate.git
    4  ls
    5  cd course-aws-certified-developer-associate/
    6  ll
    7  cd Create_A_DynamoDB_Table_Demo/
    8  ll
    
or
curl -O https://raw.githubusercontent.com/ACloudGuru-Resources/course-aws-certified-developer-associate/main/Create_A_DynamoDB_Table_Demo/items.json

9  aws dynamodb batch-write-item --request-items file://items.json


[ec2-user@ip-172-31-36-129 ~]$ aws dynamodb get-item --table-name ProductCatalog --key '{"Id":{"N":"403"}}'
{
    "Item": {
        "Description": {
            "S": "Womens Cycling Helmet"
        },
        "Color": {
            "S": "Black"
        },
        "Price": {
            "S": "99"
        },
        "ProductCategory": {
            "S": "Helmet"
        },
        "Id": {
            "N": "403"
        },
        "Size": {
            "S": "Small"
        }
    }
}
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
INDEXES - for efficient Sorting

a database index is a copy of selected columns of data which is used to quickly sort
DynamoDB ACCESS CONTROL 
fine-grained Access Control with IAM 
IAM Condition parameter dynamodb:LeadingKeys 
--->allow users to access only the items where partition key value matches their user-id 
e.g. game scoreboard and points. user can only see their own profile

DynamoDB Secondary Indexes 
allow you to query based on attribute that is not the primary key
using global secondary indexes and local seconday indexes
you select the columns you want included in the index and run your searches on the index, rather than on the entire dataset

LOCAL Secondary Index
How-> same partition key as your original table, diff sort key. COMPOSITE
    -> needs both partition and sort key
-> gives you diff view of your data organized acc to alt sort key
-> cannot be added, modified or deleted after creation 
-> faster queries using index rather than main table 
-> provide strong CONSISTENCY 
-> shares provisioned throughput settings of table RCU/WCU
Limitation->can only be created when you are creating the table
          ->total size of indexed items for any one partition key value cant exceed 10GB 
          ->limited to 5 per table
          
GLOBAL Secondary Index 
AWS recommends using Global indexes
-> no size restrictions
-> have their own provisioned throughput settingslimited to 20 per table
much flexible, create anytime 
allows you to pick different partition key and diff sort key to main table
or same partition key and different sort key
SIMPLE+COMPOSITE --^
Cannot provide strong CONSISTENCY
 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

SCAN vs QUERY API Calls

QUERY:
By Default, reads as "EventuallyConsistant", even if Str Cons is in settings
A query finds items in a table based on the primary key attribute and a dinstinct value to search for
refine your query using your sort key.
Query results are always sorted by the sort key 
you can use "ProjectionExpression" parameter to refine the results
e.g. if you only want to see id and email, not all attributes
if sort key is numeric,it returns data in ascending numeric order bydefault
if you want to reverse the order, set the "ScanIndexForward" to false
by default, query are eventually Consistent
you need to explicitly set the query to be strongly consistent
SCAN:
it examines all the items in the table, by default, it returns all data attributes
you can use "ProjectionExpression" parameter to refine the results
e.g. if you only want to see id and email, not all attributes

Query is more Efficient than Scan 
A Scan Operation on a large table can use up the provisioned throughput for a large table in a single operation
SCAN is sequential by default
can be done in parrallel by logically dividing the table or index into segments and scanning each sagment in parrellel
but its best to avoid parrallel scan if your table/index is heavy

Improve performance by reducing page size , limit to 40 items 
"Avoid scans, design tables in a way that you can use Query,Get or 
BatchGetItem APIs"
A large Table can eat your ProvisionedThroughput in a single Scan

-----------------------------------------------------------------------
DynamoDB API CALLS (programatically)

CLI:            API:
create-table    CreateTable table name must be unique within each Region .
put-item        PutItem
get-item        GetItem
transact-get-item =
                synchronous op, retrieve multiple items from 1ormore table (but not indexes) in a single account and region. can contain upto 25 objects. aggregate size of items < 4 MB
batch-get-item  single operation = limit upto 16 MB of data, max 100 items
batch-write-item = limit 16 MB of data, 25 put/del req, each item max 400kb
transact-write-item = 
                synchronous write operationthat groups upto 25 action requests. these actions can target items in d/f tables, but not in d/f AWS accounts or regions, and no two actions can target the same item

update-item     UpdateItem
update-table    UpdateTable modifies provisioned throughput of a table
                                     global secondary indexes 
                                     DynamoDB Streams settings
list-table      ListTables
describe-table  DescribeTable
scan            Scan
query           Query
delete-item     DeleteItem
delete-table    DeleteTable

if a user...
run put-item cli command runs PutItem API call on the database
They will need IAM permissions to call PutItem

[cloudshell-user@ip-10-4-79-87 ~]$ aws dynamodb describe-table --table-name ProductCatalog
{
    "Table": {
        "AttributeDefinitions": [
            {
                "AttributeName": "Id",
                "AttributeType": "N"
            }
        ],
        "TableName": "ProductCatalog",
        "KeySchema": [
            {
                "AttributeName": "Id",
                "KeyType": "HASH"
            }
        ],
        "TableStatus": "ACTIVE",
        "CreationDateTime": "2023-08-16T21:44:00.344000+00:00",
        "ProvisionedThroughput": {
            "NumberOfDecreasesToday": 0,
            "ReadCapacityUnits": 5,
            "WriteCapacityUnits": 5
        },
        "TableSizeBytes": 0,
        "ItemCount": 0,
        "TableArn": "arn:aws:dynamodb:us-east-1:077794698366:table/ProductCatalog",
        "TableId": "ca15d5ff-8519-4a23-85af-55a5ce76fc1f",
        "DeletionProtectionEnabled": false
    }
}
(END)

---------------------------------------------------------------------------
DynamoDB Provisioned Throughput

5 write capacity unit ---$2.91/month
5 read capacity unit  ---^
while creating table, specify...
1 write capacity unit = 1 x 1KB write per second
1 read capacity unit = 1 x 4KB read per second (strongly consistent)
or
1 read capacity unit = 2 x 4KB read per second (eventually consistent)
default
e.g. your app needs to read 80 items per second 
    each item is 3kb in size 
        you need strongly consistent reads
How many read capacity units will you need? 
size of each item / 4kb = 3 / 4 = 0.75 = 1 x 80
3kb/4kb = 0.75 rounded to 1kb ,so need 80 units (sc) and 40 units (ec)

e.g. you need to write 100 items per sec. each item is 512byte 
how many write capacity units you require?
size of each item / 1kb = 512 / 1024 = 0.5 rounded to 1
so we need 100 capacity units
use it when....
capacity can be forecasted 
predictable application traffic 
traffic is consistent 
you will hav more control over costs

DynamoDB on-Demand Capacity 
charges apply on reading,writing and storing data
use it when...you have 
unknown workloads 
unpredictable application traffic 
spiky, short-lived peaks 
a pay-per-use model is desired
more difficult to predict the cost b/c db will scale up n down

You have an application that needs to read 25 items per second and each item is 13KB in size. Your application uses eventually consistent reads. What should you set the read throughput to?
50 RCU
25 RCU
100 RCU
10 RCU
Sorry!
Correct Answer
Each Read Capacity unit represents 1 x 4KB Strongly Consistent Read. 13KB / 4KB = 3.25 rounded up to 4 multiply by 25 = 100 
each items eats --^ 4kb so 25 items = 100RCU StronglyC
& 100/2=50RCU EventuallyC
which gives you the figure for Strongly consistent reads. Divide that by 2 for Eventually consistent. So the answer is 50 RCU.

You have a motion sensor which writes 600 items of data every minute. Each item consists of 5KB. What should you set the write throughput to?
20
40
10
50
Sorry!
One write request unit represents one write for an item up to 1 KB in size. If you write 600 items every minute, it means 10 items are written per second ( 600 / 60 ). The total number of write capacity units required depends on the item size. If your item size is 5 KB, just multiply 10 items by 5 KB, so you require 50 write capacity units to sustain one write request. Read/Write Capacity Mode.
------------------------------------------------------------------------
DYNAMODB GLOBAL TABLES 

provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution

To create Global Tables: you need to.....
1- use KMS CMK
2- Enable Streams 
3- Stream Type of New and Old Image
then add region 
then add replica to global table


Suppose that you have a large customer base that's spread across three
geographic areas: North America, Europe, and Asia. Customers must
update their profile information while they are using your application.
To address these requirements, you could create three identical
DynamoDB tables that are named CustomerProfiles in three different
AWS Regions. These three tables would be entirely separate from each
other. Changes to the data in one table would not be reflected in the
other tables. Without a managed replication solution, you could write
code to replicate data changes among these tables. However, writing
this code could be a time-consuming and labor-intensive effort.
Instead of writing your own code, you could create a global table that
consists of your three Region-specific CustomerProfiles tables.
DynamoDB would then automatically replicate data changes among
those tables. Thus, changes to CustomerProfiles data in one Region
would be propagated to the other Regions. In addition, if one of the
AWS Regions should become temporarily unavailable, your customers
could still access the same CustomerProfiles data in the other Regions.
------------------------------------------------------------------------
DynamDB Accelerator DAX:
DynamoDB Accelerator (DAX) reduces latency at scale by providing a write-through in-memory caching service. DAX provides sub-millisecond response times for eventually consistent read workloads. Read-heavy or bursty workloads benefit from increased throughput and reduced RCU provisioning, especially for repeated reads of individual keys. This technique is more efficient than a side cache such as Amazon ElastiCache because no code change is required. You need to get only a
new endpoint to use.

The DAX cluster, which consists of a primary node and optional read replicas, houses the cache, including the following:
• Item cache — Eventually consistent item data stored by primary key values
• Query cache — Result sets from queries and scans on the DynamoDB tables stored by their parameter values 

billing is per node-hour consumed, based on the EC2 instance type. For a three-node cluster, you are charged for three
nodes. There is no charge for data transfer between Amazon EC2 and DAX within the same Availability Zone. Standard Amazon EC2 data transfer charges apply across Availability Zones within the same Region. No additional charge is incurred for traffic into or out of the DAX node itself.


The DAX client directs all the application's DynamoDB API requests to the DAX cluster endpoint. The DAX cluster either processes the request directly (cache hit) or passes it through to DynamoDB (cache miss).

1. The client requests data from the cache.
2. The data is not present in the cache, and the request is sent to the database.
3. The data is returned from the database to the cache.
4. The data is sent to the client.


DAX is a fully managed clustered in-memory cache for DynamoDB
Deliver upto 10x read performance 
microsecond performance for millions of requests per second
e.g. black friday sale, big promotion day
how it works?......
DAX is a write-through caching servince. Data is written to the cache and the backend store at same time

instead of quering your db, your app can query DAX Cluster first
it item you are looking for is in the cache (cache hit). DAX returns the result
if CacheMiss -> DAX do GetItem from DB -> stores it in cache
So it gets the item out of DynamoDB, it writes into its cache,
and it also hands it back to the application.

|---------------------------------------|
|VPC                                    |
|    ____________        _____________  |
|   |Your APP   |       |cache | cache| |
|   |     ^     | <->   |cache | cache| | <-> DynamoDB Table
|   |     |     |       |             | |
|   |DAX Client |       |             | |
|   |___________|       |_____________| |
|      EC2                DAX Cluster   |
|                           4 nodes     |
|_______________________________________|
incoming requests are evenly distributed across all nodes in the cluster
one of the node serves as the primary node in the cluster 
additional nodes (if present) serve as Read Replicas
Each node runs its own INSTANCE of the DAX CACHING SOFTWARE .
app can access DAX by using endpoint for DAX cluster
    DAX CLIENT software works with Cluster  endpoint to perform INTELLIGENT LOAD BALANCING and ROUTING .

when DAX is not Suitable?......
it caters for eventually consistent reads only.
So it's not going to be suitable for applications that require strongly
consistent reads. so if you done want DAX, use ELASTICACHE .

It's also not really suitable for write-intensive applications.
So youre not going to get a benefit from using DAX because it only helps with read operations. 
Also,
applications that dont perform that many read operations are
not really going to see a benefit from configuring DAX,
and the same goes for applications that dont require microsecond response
times. Theres no point in configuring it if you dont need that low latency.


---------------------------------------------------------------------------
DynamoDB Streams (versions like)
Time Ordered Sequence of Modifications
is a time-ordered sequence, or stream,
and it records any modifications that are made to the items
in your DynamoDB table. So that's any time there's an insert, update,
or delete operation, you can send that change to lambda

For each item that is modified, the stream records appear in the same sequence as the actual modifications

Logs - encrypted at rest and stored for 24 hours
Dedicated Endpoint - Accessed using a dedicated endpoint 
By Default, Primary Key is recorded
Before and after Images can be captured 
Uses:
Audit or Archive transactions 
trigger an event based on a particular transaction 
replicate data across multiple tables 

Application--aws sdk--db api---> DynamoDB Endpoint 
Application--aws sdk--db streams api---> DynamoDB Streams Endpoint 
NEAR REAL_TIME 
Apps can take action based on contents of the stream
STREAMS is a great Event Source for Lambda



e.g. 
And in this example,
we have an invoicing and payment system,
which is recording invoice data into a DynamoDB table.
And each time a new invoice entry
is made into the DynamoDB table,
there is a Lambda function which reads the DynamoDB stream
to check for new transactions.
And then when it sees a new event,
it's sending a notification using SNS,
which in turn creates a message in an SNS queue.
And you can then have your payments application
polling that SQS queue, processing the message.
And let's say maybe it performs various tasks
like generating a payment request
or adding an item to a customer's bill, etc.
So you can see that this is a really powerful way
to generate triggers to Lambda,
to trigger your application
to take actions based on the changing contents
of your DynamoDB table.


Amazon DynamoDB Streams
A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures
information about every modification to data items in the table.
A stream consists of stream records. Each stream record represents a single data modification in the DynamoDB table that the stream belongs to. Each stream record is assigned a sequence number, which reflects the order that the record was published to the stream. When an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record. It records the primary key attributes of the items that were modified. You can configure the stream so that the stream records capture additional information, such as the before and after images of modified items.

SHARD
Stream records are organized into groups, or shards. Each shard acts as a container for multiple stream records, and it contains information that is required for accessing and iterating through these records. The stream records in a shard are automatically removed after 24 hours.
---------------------------------------------------------------------------'
EXPONENTIAL BACKOFF - when provisioned throughput exceeds 
ProvisionedThroughputExceeded Error
ProvisionedThroughputExceededException
if you are using aws sdk , it will automallically retries
aws sdks use Exponential backoff by default
means - progressivly longer waits b/w consecutive retries for imp workflow
if you are not using aws sdk, you need to configure app to do 1 or 2 things
1- you can reduce the request frequency
2-implement exponential backoff...

DB<-failed request<-50 ms<-retry<-100 ms wait<-retry<-200ms<-retry<-400ms
this keeps on going until Request is successful

if it does not successfully happen within 1 min, it means request size may be exceeding throughput of read/write capacity
after improving capacity, if it still fails 
then use DAX / Elasticache

"exp backoff is a feature of all aws sdk, not limited to dynamodb"

---------------------------------------------------------------------------
Your low latency web application needs to store its session state in a scalable way so that it can be accessed quickly. Which service do you recommend?


In memory on your EC2 instance


Elastic File Store


RDS


DynamoDB

Good work!
Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. Amazon DynamoDB provides an effective solution for sharing session state across web servers without incurring drawbacks. Reference: Managing ASP.NET Session State with Amazon DynamoDB.




----------------------------------------------------
Using the AWS Console, you are trying to scale DynamoDB past its pre-configured maximums. Which service limits can you increase by raising a ticket to AWS support?

Choose 2

13/
Local Secondary Indexes


Global Secondary Indexes per table


Provisioned throughput limits


Item Sizes

Sorry!
You can define a maximum of 5 local secondary indexes. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

Correct Answer
There is an initial quota of 20 global secondary indexes per table. To request a service quota increase, see https://aws.amazon.com/support. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

AWS places some default quotas on the throughput you can provision. These are the quotas unless you request a higher amount. To request a service quota increase, see https://aws.amazon.com/support. Reference: Service, Account, and Table Quotas in Amazon DynamoDB

------------------------------------------------------------

What is the difference between a Global Secondary Index and a Local Secondary Index

Choose 2


You can delete a Global Secondary Index at any time


You can create a Local Secondary Index at any time but you can only create a Global Secondary Index at table creation time


You can delete a Local Secondary Index at any time


You can create a Global Secondary Index at any time but you can only create a Local Secondary Index at table creation time

Sorry!
Correct Answer
Only Global secondary Indexes can be created or deleted at anytime. Local Secondary Indexes must be created when you first create the table and they cannot be modified or deleted.


===========================================================================
USE CASE:

Gaming application: Amazon DynamoDB
Companies in the gaming industry use Amazon DynamoDB in all aspects of game platforms, including game state, player data, session history, and leaderboards. Unlike Amazon RDS, DynamoDB is able to automatically scale to millions of concurrent users and requests while ensuring consistently low latency measured in single-digit milliseconds.

In this use case, player data is stored in DynamoDB for analytics to determine player behavior and usage patterns.

Use cases such as gaming, advertising tech, shopping carts, and IoT lend themselves particularly well to the key-value data model of DynamoDB.
===========================================================================
Use Case 2
Media streaming: Amazon ElastiCache
Amazon ElastiCache offers a fast, in-memory data store to power live streaming use cases. ElastiCache can store metadata for user profiles and viewing history, authentication information/tokens for millions of users, and manifest files to enable content delivery networks to stream videos to millions of mobile and desktop users at a time.



In this use case, Amazon Simple Storage Service (Amazon S3) and Amazon CloudFront are managing and serving media content. ElastiCache manages the content index and token authentication for in-memory, sub-milliseconds responses at scale.
===========================================================================
Use Case 4
Knowledge graph: Amazon Neptune
Amazon Neptune allows you to use existing information resources to build knowledge graphs and host them on a fully managed service. A knowledge graph allows you to store information in a graph model and use graph queries to enable your users to easily navigate highly connected datasets.



In this use case, comma-separated value (CSV) or Resource Description Framework (RDF) data is loaded from Amazon S3 to Neptune. The client application then uses simple SPARQL queries to build graphical visualizations.

Using a knowledge graph, you can add topical information to product catalogs, build and query complex models of regulatory rules, or model general information.
===========================================================================
Use Case 5
Profile management: Amazon DocumentDB
User profile management enables online transactions, user preferences, and user authentication. With the growth in number of users, increasingly complex user profile data, and growing user experience expectations, the demand for scalability, data flexibility, and performance has grown too. With Amazon DocumentDB’s document data model, you can manage profiles and preferences for millions of users and scale to process millions of user requests per second with millisecond latency.



Developers can persist data with Amazon DocumentDB by using the same document model format that they use in their application code. Content management, personalization, and mobile applications are typical use cases.
===========================================================================

USE CASE.
Mobile Bookstore web application

The Mobile Bookstore App is a full-stack web application that creates a storefront and backend for customers to shop for books. The app contains multiple customer experiences such as a shopping cart, product search, recommendations, and a top sellers list. For each of these use cases, the app uses a purpose-built database so the developer never has to compromise on functionality, performance, or scale.

Database components

Product catalog/shopping cart: Amazon DynamoDB offers fast, predictable performance for the key-value lookups needed in the product catalog, as well as the shopping cart and order history. In this use case, there are unique identifiers, titles, descriptions, quantities, locations, and price.

Recommendations: Amazon Neptune provides social recommendations based on what a user's friends have purchased, scaling as the storefront grows with more products, pages, and users.

Top sellers list: Amazon ElastiCache for Redis reads order information from Amazon DynamoDB Streams, creating a leaderboard of the “Top 20” purchased or rated books.

Other components

Search: Amazon Elasticsearch Service (Amazon EDS) enables full-text search for the storefront, enabling users to find products based on a variety of terms including author, title, and category.

Serverless backend: Amazon API Gateway powers the interface layer between the front end and backend and invokes serverless compute with AWS Lambda.

Serverless web application: Amazon CloudFront and Amazon Simple Storage Service (Amazon S3) provide a globally-distributed application.

https://github.com/aws-samples/aws-bookstore-demo-app
https://github.com/amazon-archives/aws-full-stack-template'

=========================================================================
MONITORING:
What types of monitoring metrics can I gather?

DynamoDB sends the following sets of metrics to CloudWatch:

Account metrics
Table metrics
Table operations metrics
Global secondary index name and table metrics


Account metrics
--------------------------------------------------------------------
AccountMaxReads
AccountMaxWrites
    Count	The maximum number of read/write capacity units that can be used by an account
AccountMaxTableLevelReads
AccountMaxTableLevelWrites
    Count	The maximum number of read/write capacity units that can be used by a table or global secondary index of an account
AccountProvisionedReadCapacityUtilization
AccountProvisionedWriteCapacityUtilization
    Percent	The percentage of provisioned read/write capacity units used by an account
MaxProvisionedTableReadCapacityUtilization
MaxProvisionedTableWriteCapacityUtilization
    Percent	The percentage of provisioned read/write capacity units used by the highest provisioned read table or global secondary index of an account
UserErrors
    Count	Requests to DynamoDB or DynamoDB Streams that generate an HTTP 400 status error code during the specified time period


Table metrics
--------------------------------------------------------------------
ConsumedReadCapacityUnits
    Count	The number of read capacity units consumed over the specified time period
ConsumedWriteCapacityUnits
    Count	The number of write capacity units consumed over the specified time period
ProvisionedReadCapacityUnits
    Count	The number of provisioned read capacity units for a table or a global secondary index
ProvisionedWriteCapacityUnits
    Count	The number of provisioned write capacity units for a table or a global secondary index


Table operations metrics
--------------------------------------------------------------------
ReturnedItemCount
    Count	The number of items returned by Query, Scan or ExecuteStatement (select) operations during the specified time period
SuccessfulRequestLatency
    Milliseconds 	The elapsed time for successful requests to DynamoDB or DynamoDB Streams during the specified time period


Global secondary index name and table metrics
--------------------------------------------------------------------



Why are the metrics on the DynamoDB console different from the CloudWatch metrics?

The graphs on the Metrics tab in the Amazon DynamoDB console are different from the graphs in the CloudWatch console. The metrics in the CloudWatch console are raw and provide more statistics options than the metrics in the DynamoDB console. The metrics in the DynamoDB console are average values over 1-minute intervals. For example, ConsumedWriteCapacityUnits is the sum of the consumed units over 1 minute, divided by the number of seconds (60) in a minute.

What are auto scaling alarms?

When you turn on auto scaling for a table, DynamoDB automatically creates several alarms that can launch auto scaling actions.

---------------------------------------------------------------------------
Contributor Insights for DynamoDB: 

CloudWatch Contributor Insights for DynamoDB is a diagnostic tool for identifying the most frequently accessed and throttled keys in your table or index at a glance.

If you set up CloudWatch Contributor Insights for DynamoDB on a table or global secondary index, you can determine the most accessed and throttled items in those resources.

DynamoDB creates the following rules on your behalf. Expand each of the following four tabs to learn more about these rules.


Most accessed items (partition key)
This rule identifies the partition keys of the most accessed items in your table or global secondary index.
CloudWatch rule name format:
        DynamoDBContributorInsights-PKC-[resource_name]-[creationtimestamp]

Most throttled keys (partition key)
        DynamoDBContributorInsights-PKT-[resource_name]-[creationtimestamp]

Most accessed keys (partition and sort keys)
        DynamoDBContributorInsights-SKC-[resource_name]-[creationtimestamp]

Most throttled keys (partition and sort keys)
        DynamoDBContributorInsights-SKT-[resource_name]-[creationtimestamp]


---------------------------------------------------------------------------
TROUBLESHOOTING:
How do I determine my problem?

To learn about the general steps to follow for problem determination, choose each of the following seven tabs.


Problem determination
+

Step 1: Identify the problem
–
Try to categorize the problem based on characteristic symptoms:
What error message is the administrator seeing on the console or in log files?
Are customers getting error messages in their browsers? 
What messages or error codes are users getting?

Step 2: Collect data
+
Step 3: Analyze the data
–
Analyze data to pinpoint one or more possible root causes. Sometimes you can manually search the relevant log files and tracing data.
For large amounts of diagnostic data, it might be helpful to use tools for analyzing data, such as the following:

Amazon Athena
CloudWatch Logs Insights
Splunk
Sumo Logic

Step 4: Review documentation
+
Step 5: Try known solutions
+
What else can I try?

If you exhaust all known issues and solutions without success, try the following:

Post your issue to AWS re:Post. AWS re:Post is a community-driven question-and-answer service to help AWS customers remove technical roadblocks and troubleshoot problems.
Engage with AWS IQ to post a request for help by an AWS Certified expert. Pay through AWS.
Subscribe to an AWS Support Plan and open a case:
Select a support plan that meets your needs. Compare AWS Support Plans.
You can submit your case, along with any data you gathered.


THROTTLING ISSUE:
Some of the common throttling issues that you might face are the following:

Your DynamoDB table has adequate provisioned capacity, but most of the requests are being throttled.
You activated AWS Application Auto Scaling for DynamoDB, but your DynamoDB table is being throttled.
Your DynamoDB table is in on-demand capacity mode, but the table is being throttled.
You have a hot partition in your table.

What DynamoDB metric should I examine if my tables experience throttling?

The metrics associated with throttling are:

OnlineIndexThrottleEvents
ReadThrottleEvents
ThrottledPutRecordCount
ThrottledRequests
WriteThrottleEvents


Common reasons why a DynamoDB table is throttled

To learn about the common causes of table throttling, expand each of the following four tabs.


Table has enough provisioned capacity, but most requests are throttled
+
        'DynamoDB reports minute-level metrics to CloudWatch. The metrics are calculated as the sum for a minute, and then are averaged. However, the DynamoDB rate limits are applied per second.
        For example, if you provisioned 60 write capacity units for your DynamoDB table, then you can perform 3600 writes in one minute. However, driving all 3600 requests in one second, with no requests for the rest of the minute, might result in throttling.
        The total number of read capacity units or write capacity units per minute might be lower than the provisioned throughput for the table. However, if all the workload falls within a couple of seconds, then the requests might be throttled.'

        Solution: Add jitter and exponential backoff to your API calls. For more information, see the Exponential Backoff and Jitter documentation.

Application Auto Scaling is set up, but your table is still throttled
+
        'Application Auto Scaling is not a suitable solution to address sudden spikes in traffic with DynamoDB tables. It only initiates a scale-up when two consecutive data points for consumed capacity units exceed the configured target utilization value in a 1-minute span.'

        Solution: If you use DynamoDB for a service that receives requests with several peak times and abrupt workload spikes, you too can benefit from switching the capacity mode from provisioned to on demand.

Your table uses the on-demand capacity mode but is still throttled
+
        For on-demand tables, DynamoDB automatically allocates more capacity as your traffic volume increases to make sure that your workload doesn't experience throttling. However, throttling can occur if the traffic volume is more than double the previous peak within a span of 30 minutes. '
         the table doesn't throttle as long as the following conditions are true:

        @ The table doesn't exceed double its previous peak traffic.
        @ The access pattern is distributed evenly across partitions to avoid issues related to a hot partition. (In general, a 'hot partition' is one that is accessed more frequently than other partitions in the table.)
        

        Solution: Apply a strategy to avoid creating hot partitions such as distributing the read and write operations as evenly as possible across your table. Refer to the following topic, You have a hot partition in your table.

You have a hot partition in your table
+
        'In DynamoDB, a partition key that doesnt have a high cardinality can result in many requests targeting only a few partitions and resulting in a hot partition. A hot partition can cause throttling if the partition limits of 3000 read capacity units (RCU) or 1000 write capacity units (WCU) (or a combination of both) per second are exceeded.

        To find the most accessed and throttled items in your table, use the Amazon CloudWatch Contributor Insights. Amazon CloudWatch Contributor Insights is a diagnostic tool that provides a summarized view of your DynamoDB tables traffic trends and helps you identify the most frequently accessed partition keys. With this tool, you can continuously monitor the graphs for your table’s item access patterns. 

        A hot partition can degrade the overall performance of your table. '

        Solution: To avoid this poor performance, distribute the read and write operations as evenly as possible across your table.



Here are two common reasons why on-demand tables might be throttled:

The traffic is more than double the previous peak
The traffic exceeds the per-partition maximum
        Each partition on the table can serve up to 3,000 read request units or 1,000 write request units, or a linear combination of both. If the traffic to a partition exceeds this limit, then the partition might be throttled. To resolve this issue:

        Use CloudWatch Contributor Insights for DynamoDB to identify the most frequently accessed and throttled keys in your table.
        Randomize the requests to the table so that the requests to the hot partition keys are distributed over time. 


PROBLEM:
You are getting a 4xx error ProvisionedThroughputExceededException for one of your tables.
For example:
ProvisionedThroughputExceededException: An error occurred (ProvisionedThroughputExceededException) when calling the PutItem operation (reached max retries: 9): The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API.

SOLUTION:
1- Determine whether the table is an on-demand table or a provisioned table. Check the read/write capacity modes in the DynamoDB console or with the AWS CLI command dynamodb describe-table. 

2- If the table is provisioned, determine the following:
A- Is auto scaling configured? Use the DynamoDB console to find this data.
B- Is the consumed capacity greater than the provisioned capacity? Use the CloudWatch console to compare the metrics. For example, in the following screenshot, observe that there is a spike in Consumed Write Capacity where it is going above Provisioned Capacity.

3- If the table is on demand:
A- If possible, determine the previous consumed capacity peak fr the table.
B- If you have the previous peak data, is the current consumed capacity double the previous peak?
C- Is CloudWatch Contributor Insights configured for the table? If so, identify the most frequently accessed and throttled keys in your table or index.

----------------------------------------------------------------------
PROBLEM:
How can I troubleshoot high latency on a DynamoDB table?
Performance issue: You observe an increase in the response time for DynamoDB requests.
CloudWatch metric SuccessfulRequestLatency.
// Keep in mind that DynamoDB latency metrics measure activity only in DynamoDB or DynamoDB Streams. The latency metrics don't take network latency or client-side activity into account.
Reduce the request timeout settings
+
// Tune the client AWS SDK parameters requestTimeOut and clientExecutionTimeout to timeout and fail much faster (for example, after 50 milliseconds). This causes the client to abandon high-latency requests after the specified time period and then send a second request that usually completes much faster than the first.
Reduce distance between client and DynamoDB endpoint
+
// If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions for which you want the table to be available. This can help to significantly reduce latency for your users.
Use caching
+
// If your traffic is read heavy, consider using a caching service such as Amazon DynamoDB Accelerator (DAX). DAX is a fully managed, highly available in-memory cache for DynamoDB that can help improve performance from milliseconds to microseconds, even at millions of requests per second.
Send constant traffic or reuse connections
+
// When you are not making requests, consider having the client send dummy traffic to a DynamoDB table. You can also reuse client connections or try connection pooling. All of these techniques keep internal caches warm, which can help keep latency low.
Use eventually consistent reads
+
// If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.

When analyzing the CloudWatch metric SuccessfulRequestLatency, it's a best practice to check the average latency. Occasional spikes in latency aren't a cause for concern. However, if average latency is high, you may need to resolve an underlying issue.
As shown in the following screenshot, initial requests can experience higher latency because caches are cold.


------------------------------------------------------------------------
PROBLEM:
Resolving HTTP 5xx errors
–
A 5xx error indicates a problem that must be resolved by AWS. This might be a transient issue, such as a network outage or backend hardware failure. To mitigate 5xx errors, do the following:

Implement a retry strategy for requests that fail with a 5xx error code. All AWS SDKs have a built-in retry mechanism with an algorithm that uses exponential backoff. You can modify the retry parameters to suit your needs. For more information, see the Error Retries and Exponential Backoff documentation.
Avoid strongly consistent reads. When there is a network delay or outage, strongly consistent reads are more likely to fail with a 500 error. For more information, see the Read Consistency documentation.
If you continue to get 5xx errors, open the AWS Health Dashboard to check if there are any operational issues with the service.

========================================================================
AUTO SCALING:
What is DynamoDB auto scaling?

DynamoDB auto scaling creates CloudWatch alarms on your behalf. When an alarm is activated, the CloudWatch alarm invokes AWS Application Auto Scaling. This, in turn, notifies DynamoDB to adjust the provisioned throughput capacity for the table.

DynamoDB auto scaling modifies provisioned throughput only when the workload stays depressed or elevated for several minutes. For example, assume that you set the minimum read capacity units (RCUs) to 100 and the target utilization to 70 percent:

DynamoDB auto scaling increases provisioned capacity when utilization exceeds 70 RCUs for at least 2 consecutive minutes.
DynamoDB auto scaling decreases provisioned capacity when utilization is 20 percent or more below the target for 15 consecutive minutes (50 RCUs).

PROBLEM:
To learn how to resolve issues with DyanamoDB auto scaling, choose each of the following three tabs.

Don't delete CloudWatch alarms
Be sure that you don't delete the CloudWatch alarms that DynamoDB creates when you turn on auto scaling for a table. If you do, DynamoDB auto scaling might not work as expected.

// If you accidentally delete the CloudWatch alarms, then turn off and turn on auto scaling for the table. When you do this, CloudWatch automatically recreates the alarms.


DynamoDB might not handle short activity spikes
// Don't rely on DynamoDB auto scaling to handle occasional short-duration activity spikes. DynamoDB auto scaling works best when there are gradual increases or decreases in traffic. The table's built-in burst capacity handles occasional activity spikes.

Set the billing mode to PAY_PER_REQUEST
// If the table traffic is frequently unpredictable, use an UpdateTable operation to set the billing mode to PAY_PER_REQUEST. This activates on-demand mode, which instantly adapts throughput to handle the workload.

=============================================================================
You have a large customer base spread across three geographic areasthe US East Coast, India and Western Europe. The high performance web application serving customers in three regions requires data with minimum latency and no downtime.

What would be the best way to fulfil this requirement?


"Use Amazon DynamoDB Global Tables."

Amazon DynamoDB with provisioned capacity mode.

Aurora DB with multi AZ enabled.

Amazon RDS with cross region replication.


To fulfill the requirement of serving a high-performance web application with minimum latency and no downtime for customers spread across three geographic regions (US East Coast, India, and Western Europe), the best option would be:

Use Amazon DynamoDB Global Tables.

Amazon DynamoDB Global Tables provide a fully managed, multi-region, and globally distributed database that automatically replicates data across regions. This ensures low-latency access for customers in different geographic areas while providing high availability and fault tolerance.

===========================================================================

Your company is reaching the end of the financial year, and the finance team is running a lot of large database queries and scans against your DynamoDB tables. The database queries and scans are taking much longer to complete than expected. How can you make them more efficient?

Filter your results based on the primary key and sort key.

Filtering the results based on the primary and sort keys will not improve the efficiency of this task.

Selected
Reduce the page size to return fewer items per results page.````````````

Reducing page size for queries and running scans in parallel are both recommended approaches for making DynamoDB operations more efficient.

Reference: Best Practices for Querying and Scanning Data

Selected
Set your queries to be eventually consistent.

Run parallel scans.`````````````````````````````

Reducing the page size for queries and running scans in parallel are both recommended approaches for making DynamoDB operations more efficient. DynamoDB uses eventually consistent reads by default, and filtering the results will not improve efficiency.

==========================================================================
You are storing user profile data for a mobile gaming application in a DynamoDB table. The product owner asks you to generate a list of all registered users that are located in the USA. As you retrieve the information from DynamoDB, you receive a ProvisionedThroughputExceededException error. What does the error ProvisionedThroughputExceededException mean in DynamoDB?
"
You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes."
The ProvisionedThroughputExceededException means that you exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes.
Reference: Class ProvisionedThroughputExceededException

The DynamoDB table is unavailable.

The DynamoDB table has exceeded the allocated space.

This is not correct because it is not possible to exceed the allocated space.

Selected
The size of the query you tried to perform is too large. Find a way to split the query into a set of smaller queries.

========================================================================
A developer is working on a new JavaScript web application that allows users to post opinions to a shared forum and contribute to discussions about the latest innovations in cloud computing. Users have the ability to upvote their favorite posts, and at the end of each day, the top 250 posts are displayed in a leader board. The application makes a large batch request directly to DynamoDB using the BatchGetItem API call to report the most popular 250 posts. When a large request is made, the DynamoDB table frequently returns a partial result, accompanied by a value for UnprocessedKeys. Which of the following actions are ways that the developer can enable the application to receive all the remaining items when the BatchGetItem response includes a value for UnprocessedKeys?

Update the application to use the AWS software development kit (AWS SDK) for JavaScript to make the API requests.`````````````````````````````

AWS SDKs automatically implement retries using exponential backoff. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.

Reference: Retry and Backoff with AWS SDKs

Retry the batch operation immediately.

Retry the operation using an exponential backoff algorithm.````````````````

If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, AWS strongly recommends that you use an exponential backoff algorithm. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed.

Reference: BatchGetItem API Call

Selected
Increase the provisioned write capacity of the DynamoDB table.

BatchGetItem is a read operation, so increasing the write capacity of the table will not help.

====================================================================
A developer uses the AWS SDK for C++ to retrieve data from an Amazon DynamoDB table. The data is sometimes retrieved using a known key, and sometimes the key is not known, resulting in multiple items being returned. The developer wants to ensure the code returns only one item when retrieving data without keys.

Which DynamoDB setting will meet these requirements?

Report Content Errors

A
Set the parallel scan operation to 1.
Incorrect. The parallel scan operation can logically divide a table into multiple segments, with multiple application workers scanning the segments in parallel. Parallel scan returns the same number of results as an ordinary scan, except the results can be returned more quickly. Parallel scan does not limit the number of items returned.

For more information about using DynamoDB parallel scans, see Parallel Scan.


B
Set the query filter expression to 1.
Incorrect. A filter expression applies additional matching criteria to limit the items returned. This expression does not provide a specific limit on the number of items returned.

For more information about using query filter expression statements in DynamoDB, see Query.

"
C
Set the scan limit parameter to 1.
Correct. Set the limit parameter to 1 to set the maximum number of items that need to be retrieved with a DynamoDB scan operation.

For more information about limiting the number of the results in a scan, see Limiting the Number of Items in the Result Set."


D
Set the query page-size value to 1.
Incorrect. The page-size parameter will determine the number of items returned in each page of a query. This method is only relevant for queries that involve the key.

=====================================================================
A developer has written several custom applications that read and write to the same Amazon DynamoDB table. Each time the data in the DynamoDB table is modified, this change should be sent to an external API.

Which combination of steps should the developer perform to accomplish this task? (Select TWO.)

Report Content Errors

A`````````````````````````````
Configure an AWS Lambda function to poll the stream and call the external API.
Correct. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.

For more information about how to use DynamoDB Streams to create an event that invokes a Lambda function, see Tutorial: Process New Items with DynamoDB Streams and Lambda.


B
Configure an event in Amazon EventBridge (Amazon CloudWatch Events) that publishes the change to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) data stream.
Incorrect. EventBridge (CloudWatch Events) is used to connect applications with a variety of data. It would not be able to detect updates to a DynamoDB table.

For more information about EventBridge (CloudWatch Events), see What Is Amazon EventBridge?


C
Create a trigger in the DynamoDB table to publish the change to an Amazon Kinesis data stream.
Incorrect. A trigger cannot be enabled on a DynamoDB table. To create a trigger, a DynamoDB stream should be enabled on the specific DynamoDB table.

For more information about how to use DynamoDB Streams to create an event that invokes an AWS Lambda function, see DynamoDB Streams and AWS Lambda Triggers.


D
Deliver the stream to an Amazon Simple Notification Service (Amazon SNS) topic and subscribe the API to the topic.
Incorrect. A DynamoDB stream cannot be used to create an Amazon SNS topic.

For more information about how to capture changes to DynamoDB tables, see Change Data Capture with Amazon DynamoDB.

For more information about how Amazon SNS works, see What is Amazon SNS?

For more information about how to create a topic in Amazon SNS, see Creating an Amazon SNS topic.


E`````````````````````````````
Enable DynamoDB Streams on the table.
Correct. You can enable DynamoDB Streams on a table to create an event that invokes an AWS Lambda function.